"""
Generalized automation script for audio-to-text PyTorch models.

This script can generate wrapper classes for various audio-to-text tasks:
- Automatic Speech Recognition (Whisper, Wav2Vec2, HuBERT, etc.)
- Audio Classification (emotion, genre, instrument detection, etc.)
- Audio Event Detection (environmental sounds, etc.)
- Speaker Identification/Diarization
- Music transcription
- And more...

The script uses LLM (Google Gemini) to analyze model cards and generate
appropriate PyTorchAbstractClass implementations with preprocessing,
prediction, and postprocessing methods.

Usage:
    from automation import audio_to_text_model_automation

    # For speech recognition models
    audio_to_text_model_automation(
        models_to_add=["openai/whisper-base"],
        task_type="automatic_speech_recognition"
    )

    # For audio classification models
    audio_to_text_model_automation(
        models_to_add=["MIT/ast-finetuned-audioset-10-10-0.4593"],
        task_type="audio_classification"
    )
"""

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def audio_to_text_model_automation(
    models_to_add=None, task_type="automatic_speech_recognition"
):
    """
    Generalized automation for audio-to-text tasks.

    Args:
        models_to_add: List of model identifiers to process
        task_type: Type of task - "automatic_speech_recognition", "audio_classification", etc.
    """
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, input_audios):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
"""

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch audio-to-text model."""

        model_architecture: str = Field(
            description="The model architecture type. Options: 'whisper' (Whisper models using generate), 'wav2vec2' (Wav2Vec2/HuBERT using CTC), 'audio_classification' (ASTForAudioClassification, Wav2Vec2ForSequenceClassification), 'hubert_classification', or 'other'.",
            default="whisper",
        )

        imports: str = Field(
            description="The complete imports section. Include transformers classes (processors and models), torch if needed, librosa for audio loading. Use proper formatting with newlines.",
            default="from transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport librosa",
        )

        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'PyTorch_Transformers_Whisper_Base', 'PyTorch_Transformers_AST_AudioClassification'.",
        )

        init_config: str = Field(
            description="The __init__ method signature parameters. Typically ', config=None' for audio models as they need config for sampling_rate.",
            default=", config=None",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method. Should include: 1) config initialization with super().__init__(config), 2) loading processor/feature_extractor from_pretrained, 3) loading model using self.load_hf_model(ModelClass, model_id) for multi-GPU support, 4) setting sampling_rate from config (default 16_000), 5) for classification: extracting labels from model.config.id2label to self.features list.",
        )

        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Should: 1) Load audio files using librosa.load() with sampling_rate, 2) Use processor to process audios with return_tensors='pt', 3) Extract appropriate features (input_features for Whisper, input_values for Wav2Vec2, or return full dict). For classification, handle both single and batch inputs.",
        )

        predict_body: str = Field(
            description="The complete body of the predict method. For Whisper/ASR: use model.generate(). For Wav2Vec2 CTC: use model(**model_input).logits. For classification: use model(**model_input).logits.",
        )

        postprocess_body: str = Field(
            description="The complete body of the postprocess method. For ASR: use processor.batch_decode() with skip_special_tokens=True. For CTC: use torch.argmax() then batch_decode(). For classification: apply softmax and convert to list.",
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch audio-to-text models. Your task is to generate a complete, working model.py file for various audio tasks: speech recognition, audio classification, audio event detection, speaker identification, etc.

You will be given a Hugging Face model identifier and the text content from its model card page.
You must generate the complete model configuration based *primarily* on the provided page context.

**IMPORTANT GUIDELINES:**

1. **Model Architecture Detection:**
   - **ASR Models:**
     - Whisper models (openai/whisper-*, distil-whisper/*): Use WhisperForConditionalGeneration + WhisperProcessor (or AutoModelForSpeechSeq2Seq + AutoProcessor), call model.generate()
     - Wav2Vec2 models (facebook/wav2vec2-*): Use Wav2Vec2ForCTC + Wav2Vec2Processor, use CTC decoding with torch.argmax()
     - HuBERT models (facebook/hubert-*): Use HubertForCTC + Wav2Vec2Processor, use CTC decoding
   - **Audio Classification:**
     - AST models (MIT/ast-*): Use ASTForAudioClassification + ASTFeatureExtractor
     - Wav2Vec2 classification: Use Wav2Vec2ForSequenceClassification + Wav2Vec2FeatureExtractor
     - HuBERT classification: Use HubertForSequenceClassification + Wav2Vec2FeatureExtractor
   - Check the model card to determine the exact architecture and task

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For Whisper: `from transformers import WhisperForConditionalGeneration, WhisperProcessor` (or Auto variants)
   - For Wav2Vec2 ASR: `from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor` + `import torch`
   - For audio classification: `from transformers import ASTForAudioClassification, ASTFeatureExtractor` (or similar) + `from torch.nn.functional import softmax`
   - Always include: `import librosa` for audio loading

3. **Init Method:**
   - Initialize config: `super().__init__(config)` (required for load_hf_model to work)
   - Load processor/feature_extractor: `ProcessorClass.from_pretrained(model_id)`
   - Load model: `self.model = self.load_hf_model(ModelClass, model_id)` (use load_hf_model for multi-GPU support)
   - Set sampling_rate from config with default: `self.sampling_rate = self.config.get('sampling_rate', 16_000)`
   - For classification: Extract labels: `self.features = list(self.model.config.id2label.values())`

4. **Preprocess Method:**
   - Load audio files: `librosa.load(audio_path, sr=self.sampling_rate)` for each audio file
   - Use processor/feature_extractor with sampling_rate and return_tensors="pt"
   - For Whisper: Extract .input_features
   - For Wav2Vec2/HuBERT: May need padding="longest" and extract .input_values or return full dict
   - For AST classification: Use feature_extractor, may return dict
   - Return model_input

5. **Predict Method:**
   - For Whisper ASR: `return self.model.generate(model_input)`
   - For Wav2Vec2/HuBERT CTC: `return self.model(**model_input).logits` or `return self.model(model_input).logits`
   - For classification: `return self.model(**model_input).logits`

6. **Postprocess Method:**
   - For Whisper ASR: `return self.processor.batch_decode(model_output, skip_special_tokens=True)`
   - For CTC models: `predicted_ids = torch.argmax(model_output, dim=-1)` then `return self.processor.batch_decode(predicted_ids)`
   - For classification: `return softmax(model_output, dim=1).tolist()`

**Reference Examples:**

Example 1: Whisper ASR Model
{{{{
    "model_architecture": "whisper",
    "imports": "from transformers import WhisperForConditionalGeneration, WhisperProcessor\\nimport librosa",
    "class_name": "PyTorch_Transformers_Whisper_Base",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"openai/whisper-base\\"\\n        self.processor = WhisperProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(WhisperForConditionalGeneration, model_id)\\n\\n        self.sampling_rate = self.config.get('sampling_rate', 16_000)",
    "preprocess_body": "for i in range(len(input_audios)):\\n            input_audios[i], _ = librosa.load(input_audios[i], sr=self.sampling_rate)\\n        model_input = self.processor(input_audios, sampling_rate=self.sampling_rate, return_tensors=\\"pt\\").input_features\\n        return model_input",
    "predict_body": "return self.model.generate(model_input)",
    "postprocess_body": "return self.processor.batch_decode(model_output, skip_special_tokens=True)"
}}}}

Example 2: Wav2Vec2 CTC ASR Model
{{{{
    "model_architecture": "wav2vec2",
    "imports": "import torch\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\nimport librosa",
    "class_name": "PyTorch_Transformers_Wav2Vec2_Base_960h",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"facebook/wav2vec2-base-960h\\"\\n        self.processor = Wav2Vec2Processor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(Wav2Vec2ForCTC, model_id)\\n\\n        self.sampling_rate = self.config.get('sampling_rate', 16_000)",
    "preprocess_body": "for i in range(len(input_audios)):\\n            input_audios[i], _ = librosa.load(input_audios[i], sr=self.sampling_rate)\\n        model_input = self.processor(input_audios, sampling_rate=self.sampling_rate, return_tensors=\\"pt\\", padding=\\"longest\\")\\n        return model_input",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "predicted_ids = torch.argmax(model_output, dim=-1)\\n        return self.processor.batch_decode(predicted_ids)"
}}}}

Example 3: Audio Classification Model (AST)
{{{{
    "model_architecture": "audio_classification",
    "imports": "from transformers import ASTForAudioClassification, ASTFeatureExtractor\\nfrom torch.nn.functional import softmax\\nimport librosa",
    "class_name": "PyTorch_Transformers_AST_AudioClassification",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"MIT/ast-finetuned-audioset-10-10-0.4593\\"\\n        self.feature_extractor = ASTFeatureExtractor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(ASTForAudioClassification, model_id)\\n\\n        self.sampling_rate = self.config.get('sampling_rate', 16_000)\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "for i in range(len(input_audios)):\\n            input_audios[i], _ = librosa.load(input_audios[i], sr=self.sampling_rate)\\n        model_input = self.feature_extractor(input_audios, sampling_rate=self.sampling_rate, return_tensors=\\"pt\\")\\n        return model_input",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "return softmax(model_output, dim=1).tolist()"
}}}}

Example 4: Wav2Vec2 Audio Classification
{{{{
    "model_architecture": "audio_classification",
    "imports": "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\\nfrom torch.nn.functional import softmax\\nimport librosa",
    "class_name": "PyTorch_Transformers_Wav2Vec2_Emotion_Classification",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"superb/wav2vec2-base-superb-er\\"\\n        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(Wav2Vec2ForSequenceClassification, model_id)\\n\\n        self.sampling_rate = self.config.get('sampling_rate', 16_000)\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "for i in range(len(input_audios)):\\n            input_audios[i], _ = librosa.load(input_audios[i], sr=self.sampling_rate)\\n        model_input = self.feature_extractor(input_audios, sampling_rate=self.sampling_rate, return_tensors=\\"pt\\", padding=True)\\n        return model_input",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "return softmax(model_output, dim=1).tolist()"
}}}}

**CRITICAL NOTES:**
- Pay attention to model architecture and task (ASR vs classification)
- ASR models use generate() or CTC decoding to produce text
- Classification models use softmax on logits to produce class probabilities
- For classification, extract id2label to self.features list
- Whisper models extract .input_features from processor output
- CTC models may need padding="longest" and extract .input_values or return full dict
- CTC models use torch.argmax for decoding
- Sampling rate default is typically 16_000 (16kHz)
- Use .get() method for config parameters with defaults
- Ensure librosa is imported for audio loading
- Make sure the code is complete and runnable

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Here is the context from the model's Hugging Face page:
---
{model_page_context}
---
Based on the examples AND the context above, generate the complete config for the model: '{model_identifier}'

**IMPORTANT:** 
- Use the exact model identifier '{model_identifier}' in the init_body when loading the model
- Detect the task type from context (speech recognition, audio classification, event detection, etc.)
- Choose appropriate model architecture and postprocessing (text output vs classification probabilities)
- Generate complete, working code for all methods
- Handle any model-specific requirements (processor type, CTC vs generate, etc.)
""",
            ),
        ]
    )

    modelCounter = 0

    # --- LLM Setup ---
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found in .env file or environment.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser
    print("--- NOTE: Using a REAL LLM (Google Gemini). This will make API calls. ---")

    # --- 4. Main Generation Loop ---
    BASE_DIR = f"mlmodelscope/pytorch_agent/models/default/{task_type}"
    ERROR_DIR = f"{BASE_DIR}/errors"
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models = []
    login_req_models = []

    for model_name in models_to_add:
        if modelCounter == 50:
            print("Added top 50 models. Achieved the goal for now.")
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue
        error_log = ""

        try:
            check_python_file_syntax_issue = lambda fileName: os.system(
                f"python -m py_compile {fileName}"
            )
            error = False
            while not os.path.exists(model_py_path) or (
                error := check_python_file_syntax_issue(model_py_path)
            ):
                if error:
                    print(
                        f"Syntax error detected in generated file '{model_py_path}'. Regenerating..."
                    )
                    error_log += f"Syntax error detected in generated file '{model_py_path}'. Regenerating...\n"

                # --- 1. Fetch Context from Hugging Face ---
                url = f"https://huggingface.co/{model_name}"
                print(f"Fetching context from {url}...")
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    print("Could not find main content on page, skipping.")
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    print("Login required, skipping page.")
                    login_req_models.append(model_name)
                    break
                context_text = main_content.get_text(separator=" ", strip=True)
                max_length = 20000
                if len(context_text) > max_length:
                    context_text = (
                        context_text[:max_length] + "\n... (content truncated)"
                    )
                print("Context fetched successfully.")

                # --- 2. Invoke LLM with Context ---
                print("Invoking LLM to generate config...")
                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )
                print("--- LLM Output (Parsed) ---")
                print(model_config_dict)
                print("----------------------------")

                # --- 3. Fill Template ---
                init_body = model_config_dict.get("init_body", "")
                init_body = init_body.replace("{hugging_face_model_id}", model_name)
                init_body = init_body.replace(
                    "MODEL_IDENTIFIER_PLACEHOLDER", model_name
                )

                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport librosa",
                    ),
                    class_name=model_config_dict.get(
                        "class_name",
                        f"PyTorch_{task_type.replace('_', ' ').title().replace(' ', '_')}_Model",
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "pass"
                    ).lstrip(" "),
                    predict_body=model_config_dict.get(
                        "predict_body", "return self.model.generate(model_input)"
                    ).lstrip(" "),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body",
                        "return self.processor.batch_decode(model_output, skip_special_tokens=True)",
                    ).lstrip(" "),
                )

                # --- 4. Create directory and write file ---
                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

                print(f"Generated file: {model_py_path}")

            else:
                print(f"Successfully generated {model_py_path}")
                print(f"--- Generated content for {model_folder_name}/model.py: ---")
                modelCounter += 1

        except Exception as e:
            print(f"Failed to generate code for {model_folder_name}: {e}")
            import traceback

            error_log += f"Exception: {e}\n"
            traceback.print_exc()
            with open(
                os.path.join(ERROR_DIR, f"{model_folder_name}_error.log"), "w"
            ) as error_file:
                error_file.write(error_log)
                error_file.write(f"\n\nFull traceback:\n")
                traceback.print_exc(file=error_file)
            failed_models.append(model_name)
            continue

    print(f"\n--- Automation complete. ---")
    print(f"Check the '{BASE_DIR}' folder for new model files.")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for failed_model in failed_models:
            f.write(f"{failed_model}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for failed_model in login_req_models:
            f.write(f"{failed_model}\n")
    if failed_models:
        print(f"Some models failed. See '{ERROR_DIR}/failed_models.log' for details.")
    if login_req_models:
        print(
            f"Some models needed login. See '{ERROR_DIR}/login_req_models.log' for details."
        )
    else:
        print("All models processed successfully.")


# Backward compatibility wrapper
def automatic_speech_recognition_model_automation(models_to_add=None):
    """Legacy function name for ASR. Use audio_to_text_model_automation instead."""
    return audio_to_text_model_automation(
        models_to_add=models_to_add, task_type="automatic_speech_recognition"
    )


if __name__ == "__main__":
    # Example usage for different task types:

    # Speech recognition models
    # audio_to_text_model_automation(
    #     models_to_add=["openai/whisper-base", "facebook/wav2vec2-base-960h"],
    #     task_type="automatic_speech_recognition"
    # )

    # Audio classification models
    audio_to_text_model_automation(
        models_to_add=[
            "MIT/ast-finetuned-audioset-10-10-0.4593",
            "superb/wav2vec2-base-superb-er",
        ],
        task_type="audio_classification",
    )
