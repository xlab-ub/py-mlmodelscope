# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import nemo.collections.asr as nemo_asr
import librosa
import torch

class PyTorch_NeMo_Parakeet_TDT_0_6b_v3(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)  # Note: multi_gpu is not directly used by NeMo's from_pretrained

        # Load the NeMo ASR model
        self.model = nemo_asr.models.ASRModel.from_pretrained("nvidia/parakeet-tdt-0.6b-v3")
        self.model.to(torch.device(device))

        self.sampling_rate = self.config.get('sampling_rate', 16_000)

    def preprocess(self, input_audios):
        # The NeMo model's transcribe method directly accepts a list of audio file paths.
        # Therefore, we bypass audio loading with librosa and pass the file paths directly.
        return input_audios

    def predict(self, model_input):
        # The model_input is a list of audio file paths from the preprocess step.
        # The transcribe method handles preprocessing, inference, and decoding.
        # It returns a tuple: (list of transcriptions, list of all hypotheses)
        return self.model.transcribe(paths=model_input)

    def postprocess(self, model_output):
        # The model_output from predict is a tuple: (list of transcriptions, list of all hypotheses)
        # We need the first element, which is the list of the best transcriptions.
        transcriptions = model_output[0]
        return transcriptions
