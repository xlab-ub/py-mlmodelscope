import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def depth_estimation_model_automation(models_to_add=None):
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, input_images):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
"""

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch depth estimation model."""

        imports: str = Field(
            description="The complete imports section. Include: torch, transformers classes (DPTImageProcessor, DPTForDepthEstimation or similar), PIL Image, numpy. Use proper formatting with newlines.",
            default="import torch\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nfrom PIL import Image\nimport numpy as np",
        )

        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'PyTorch_Transformers_DPT_Large'.",
        )

        init_config: str = Field(
            description="The __init__ method signature parameters. Typically ', config=None'.",
            default=", config=None",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method. Should include: 1) config initialization with super().__init__(config), 2) loading image_processor from_pretrained, 3) loading model using self.load_hf_model(ModelClass, model_id) for multi-GPU support, 4) initialize self.original_sizes = None for storing image sizes. Include proper indentation (8 spaces).",
        )

        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Should: 1) Open images using PIL Image.open(), 2) Store original sizes in self.original_sizes, 3) Use image_processor with return_tensors='pt'. Include proper indentation (8 spaces). The method receives input_images (list of image file paths).",
        )

        predict_body: str = Field(
            description="The complete body of the predict method. Should call model(**model_input) and extract predicted_depth. Include proper indentation (8 spaces).",
        )

        postprocess_body: str = Field(
            description="The complete body of the postprocess method. Should: 1) Resize predictions back to original sizes using torch.nn.functional.interpolate, 2) Normalize to 0-255, 3) Convert to list, 4) Reset self.original_sizes to None. Include proper indentation (8 spaces).",
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch depth estimation models. Generate a complete, working model.py file.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - DPT models: Use DPTForDepthEstimation + DPTImageProcessor
   - Other models may use AutoModelForDepthEstimation

2. **Imports:**
   - Always: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For DPT: `import torch`, `from transformers import DPTImageProcessor, DPTForDepthEstimation`, `from PIL import Image`, `import numpy as np`

3. **Init Method:**
   - Initialize config: `super().__init__(config)` (required for load_hf_model to work)
   - Load image_processor: `ImageProcessorClass.from_pretrained(model_id)`
   - Load model: `self.model = self.load_hf_model(ModelClass, model_id)` (use load_hf_model for multi-GPU support)
   - Set `self.original_sizes = None` to store original image dimensions

4. **Preprocess Method:**
   - Open images: `images = [Image.open(img_path) for img_path in input_images]`
   - Store sizes: `self.original_sizes = [image.size for image in images]`
   - Process: `return self.image_processor(images, return_tensors="pt")`

5. **Predict Method:**
   - Call model: `return self.model(**model_input).predicted_depth`

6. **Postprocess Method:**
   - Resize predictions to original sizes using interpolate
   - Normalize: `(output * 255 / np.max(output)).astype("uint8")`
   - Convert to list and reset self.original_sizes

**Reference Example:**

{{{{
    "imports": "import torch\\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\\nfrom PIL import Image\\nimport numpy as np",
    "class_name": "PyTorch_Transformers_DPT_Large",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"Intel/dpt-large\\"\\n        self.image_processor = DPTImageProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(DPTForDepthEstimation, model_id)\\n\\n        self.original_sizes = None",
    "preprocess_body": "images = [Image.open(input_image) for input_image in input_images]\\n        self.original_sizes = [image.size for image in images]\\n        return self.image_processor(images, return_tensors=\\"pt\\")",
    "predict_body": "return self.model(**model_input).predicted_depth",
    "postprocess_body": "predictions_resized = []\\n        for output, original_size in zip(model_output, self.original_sizes):\\n            prediction = torch.nn.functional.interpolate(\\n                output.unsqueeze(0).unsqueeze(0),\\n                size=original_size,\\n                mode=\\"bicubic\\",\\n                align_corners=False,\\n            )\\n            output = prediction.squeeze().cpu().numpy()\\n            formatted = (output * 255 / np.max(output)).astype(\\"uint8\\").tolist()\\n            predictions_resized.append(formatted)\\n        self.original_sizes = None\\n        return predictions_resized"
}}}}

Respond ONLY with the JSON structure.
""",
            ),
            (
                "human",
                """
Context: {model_page_context}
Generate config for: '{model_identifier}'
Use exact identifier '{model_identifier}' in code.
""",
            ),
        ]
    )

    modelCounter = 0
    load_dotenv()
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", model_kwargs={"thinkingBudget": -1}, temperature=0, convert_system_message_to_human=True)
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser

    BASE_DIR = "mlmodelscope/pytorch_agent/models/default/depth_estimation"
    ERROR_DIR = f"{BASE_DIR}/errors"
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models = []
    login_req_models = []

    for model_name in models_to_add:
        if modelCounter == 50:
            break
        model_folder_name = model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            continue

        try:
            check_syntax = lambda fn: os.system(f"python -m py_compile {fn}")
            error = False
            while not os.path.exists(model_py_path) or (error := check_syntax(model_py_path)):
                url = f"https://huggingface.co/{model_name}"
                response = requests.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                main_content = soup.find("model-card-content") or soup.find("main") or soup.find("body")
                if not main_content:
                    break
                if main_content.find("a", href=lambda h: h and h.startswith("/login?next=")):
                    login_req_models.append(model_name)
                    break

                context_text = main_content.get_text(separator=" ", strip=True)[:20000]
                model_config_dict = chain.invoke({"model_identifier": model_name, "model_page_context": context_text})
                
                init_body = model_config_dict.get("init_body", "").replace("{hugging_face_model_id}", model_name)
                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get("imports", "import torch\nfrom transformers import DPTImageProcessor, DPTForDepthEstimation\nfrom PIL import Image\nimport numpy as np"),
                    class_name=model_config_dict.get("class_name", "PyTorch_Depth_Estimation_Model"),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_body=model_config_dict.get("preprocess_body", "pass").lstrip(" "),
                    predict_body=model_config_dict.get("predict_body", "return self.model(**model_input)").lstrip(" "),
                    postprocess_body=model_config_dict.get("postprocess_body", "return model_output.tolist()").lstrip(" "),
                )

                os.makedirs(os.path.join(BASE_DIR, model_folder_name), exist_ok=True)
                with open(model_py_path, "w") as f:
                    f.write(filled_template)
            else:
                modelCounter += 1
        except Exception as e:
            import traceback
            traceback.print_exc()
            failed_models.append(model_name)

    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for fm in failed_models:
            f.write(f"{fm}\n")


if __name__ == "__main__":
    depth_estimation_model_automation(models_to_add=["Intel/dpt-large"])




