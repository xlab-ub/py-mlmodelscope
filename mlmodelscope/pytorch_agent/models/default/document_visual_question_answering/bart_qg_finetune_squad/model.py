# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Transformers_Bart_Qg_Finetune_Squad(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        model_id = "mghan3624/bart_qg_finetune_squad"

        # The transformers pipeline provides a high-level API for this extractive QA task.
        # It handles tokenization, model inference, and decoding of start/end logits.
        if multi_gpu and device == "cuda":
            # For multi-gpu, we load the model with device_map='auto' and pass it to the pipeline.
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForQuestionAnswering.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
            self.qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)
        else:
            # For single device, we can let the pipeline handle device placement.
            pipeline_device = 0 if device == "cuda" else -1
            self.qa_pipeline = pipeline("question-answering", model=model_id, device=pipeline_device)

    def preprocess(self, input_document_images_and_questions):
        # This model is a text-based QA model, not a visual one.
        # It expects a text context and a question.
        # We will assume the 'image_path' provided in the input tuple is a path to a text file containing the context.
        pipeline_inputs = []
        for text_file_path, question in input_document_images_and_questions:
            with open(text_file_path, 'r', encoding='utf-8') as f:
                context = f.read()
            pipeline_inputs.append({"question": question, "context": context})
        return pipeline_inputs

    def predict(self, model_input):
        # The pipeline takes a list of dictionaries and returns a list of results.
        return self.qa_pipeline(model_input)

    def postprocess(self, model_output):
        # The pipeline's output is a list of dictionaries, each containing the answer.
        # e.g., [{'score': 0.99, 'start': 10, 'end': 15, 'answer': 'the answer text'}]
        return [result['answer'] for result in model_output]
