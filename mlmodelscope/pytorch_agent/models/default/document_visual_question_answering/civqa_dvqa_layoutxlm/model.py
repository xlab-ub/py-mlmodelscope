# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import LayoutXLMProcessor, LayoutXLMForQuestionAnswering
from PIL import Image
import torch
import pytesseract

class PyTorch_Transformers_Sharka_CIVQA_DVQA_LayoutXLM(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        self.config.pop("_multi_gpu", False)
        self.device = device

        model_id = "Sharka/CIVQA_DVQA_LayoutXLM"
        self.processor = LayoutXLMProcessor.from_pretrained(model_id)
        self.model = LayoutXLMForQuestionAnswering.from_pretrained(model_id)
        self.model.to(self.device)

    def preprocess(self, input_document_images_and_questions):
        images = []
        questions = []
        all_words = []
        all_boxes = []

        for image_path, question in input_document_images_and_questions:
            image = Image.open(image_path).convert("RGB")
            images.append(image)
            questions.append(question)

            ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
            words = []
            boxes = []
            for i in range(len(ocr_data['text'])):
                if ocr_data['text'][i].strip():
                    x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]
                    boxes.append([x, y, x + w, y + h])
                    words.append(ocr_data['text'][i])
            all_words.append(words)
            all_boxes.append(boxes)

        encoding = self.processor(images, questions, all_words, boxes=all_boxes, return_tensors="pt", padding="max_length", truncation=True)
        
        for k, v in encoding.items():
            encoding[k] = v.to(self.device)
            
        return encoding

    def predict(self, model_input):
        outputs = self.model(**model_input)
        return {"outputs": outputs, "encoding": model_input}

    def postprocess(self, model_output):
        outputs = model_output["outputs"]
        encoding = model_output["encoding"]
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits
        
        answers = []
        for i in range(len(encoding["input_ids"])):
            input_ids = encoding["input_ids"][i]
            sequence_ids = encoding.sequence_ids(i)

            try:
                context_start = sequence_ids.index(1)
                context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)
            except ValueError:
                answers.append("")
                continue

            start_index = torch.argmax(start_logits[i][context_start : context_end + 1]) + context_start
            end_index = torch.argmax(end_logits[i][context_start : context_end + 1]) + context_start
            
            if start_index > end_index:
                answer = ""
            else:
                answer_tokens = input_ids[start_index : end_index + 1]
                answer = self.processor.tokenizer.decode(answer_tokens).strip()
            
            answers.append(answer)
            
        return answers
