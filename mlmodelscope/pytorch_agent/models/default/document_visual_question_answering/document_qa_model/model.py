# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering
from PIL import Image
from paddleocr import PaddleOCR

class PyTorch_Transformers_LayoutLMv3_lakshya_rawat_document_qa_model(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "lakshya-rawat/document-qa-model"
        self.processor = LayoutLMv3Processor.from_pretrained(model_id)
        self.model = LayoutLMv3ForQuestionAnswering.from_pretrained(model_id)
        self.model.to(device)

        # This model requires an OCR engine to extract words and boxes from the image.
        # The model card specifies PaddleOCR was used during training.
        self.ocr = PaddleOCR(use_angle_cls=True, lang='en')

    def preprocess(self, input_document_images_and_questions):
        images, questions, all_words, all_boxes = [], [], [], []
        for image_path, question in input_document_images_and_questions:
            image = Image.open(image_path).convert("RGB")
            width, height = image.size
            
            # Perform OCR to get words and bounding boxes
            ocr_result = self.ocr.ocr(str(image_path), cls=True)
            # The result is a list containing one list of results for the image
            ocr_data = ocr_result[0] if ocr_result and ocr_result[0] else []
            
            words = [line[1][0] for line in ocr_data]
            boxes = []
            for line in ocr_data:
                box = line[0]
                # Convert 4-point polygon to a 2-point (top-left, bottom-right) box
                x_coords = [p[0] for p in box]
                y_coords = [p[1] for p in box]
                x0, y0 = min(x_coords), min(y_coords)
                x1, y1 = max(x_coords), max(y_coords)
                
                # Normalize box coordinates to a 0-1000 scale, as expected by LayoutLMv3
                norm_box = [
                    int(1000 * x0 / width),
                    int(1000 * y0 / height),
                    int(1000 * x1 / width),
                    int(1000 * y1 / height)
                ]
                boxes.append(norm_box)
                
            images.append(image)
            questions.append(question)
            all_words.append(words)
            all_boxes.append(boxes)

        # Process the batch using the LayoutLMv3 processor
        encoding = self.processor(
            images, 
            questions, 
            words=all_words, 
            boxes=all_boxes, 
            return_tensors="pt", 
            padding="max_length", 
            truncation=True
        )
        return encoding

    def predict(self, model_input):
        # Move inputs to the model's device
        model_input = {k: v.to(self.model.device) for k, v in model_input.items()}
        
        # Get model outputs (start and end logits for the answer span)
        outputs = self.model(**model_input)
        
        # Pass both outputs and inputs to postprocessing for decoding
        return (outputs, model_input)

    def postprocess(self, model_output):
        outputs, model_input = model_output
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

        # Get the most likely start and end token indices
        start_indices = start_logits.argmax(-1)
        end_indices = end_logits.argmax(-1)

        answers = []
        # Iterate over the batch
        for i in range(len(start_indices)):
            input_ids = model_input["input_ids"][i]
            start_index = start_indices[i]
            end_index = end_indices[i]
            
            # Slice the answer token IDs from the input
            answer_ids = input_ids[start_index : end_index + 1]
            
            # Decode the token IDs to a string
            answer = self.processor.tokenizer.decode(answer_ids, skip_special_tokens=True)
            answers.append(answer.strip())

        return answers
