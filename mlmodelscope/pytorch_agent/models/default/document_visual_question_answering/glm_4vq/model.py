# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import torch

class PyTorch_Transformers_Nikravan_GLM_4VQ(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        self.device = device

        model_id = "nikravan/glm-4vq"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                device_map="auto"
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to(device)

    def preprocess(self, input_document_images_and_questions):
        chat_inputs = []
        for image_path, question in input_document_images_and_questions:
            image = Image.open(image_path).convert('RGB')
            chat_inputs.append({"role": "user", "image": image, "content": question})

        model_input = self.tokenizer.apply_chat_template(
            chat_inputs,
            add_generation_prompt=True,
            tokenize=True,
            return_tensors="pt",
            return_dict=True
        )
        return model_input.to(self.device)

    def predict(self, model_input):
        input_len = model_input['input_ids'].shape[1]
        gen_kwargs = {"max_length": 2500, "do_sample": True, "top_k": 1}

        with torch.no_grad():
            sequences = self.model.generate(**model_input, **gen_kwargs)

        return {"sequences": sequences, "input_len": input_len}

    def postprocess(self, model_output):
        sequences = model_output["sequences"]
        input_len = model_output["input_len"]

        # Slice the output to get only the generated text, removing the prompt
        output_sequences = sequences[:, input_len:]

        # Decode the generated tokens
        answers = self.tokenizer.batch_decode(output_sequences, skip_special_tokens=True)

        return [ans.strip() for ans in answers]
