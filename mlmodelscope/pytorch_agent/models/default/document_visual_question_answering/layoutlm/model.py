# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import pipeline
from PIL import Image

class PyTorch_Transformers_LayoutLM_DocVQA(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Note: multi_gpu is not directly supported by the pipeline

        model_id = "daniyalumerr/LayoutLM"

        # The pipeline expects device as an integer (-1 for CPU, >=0 for GPU)
        device_id = 0 if device == "cuda" else -1

        self.nlp = pipeline(
            "document-question-answering",
            model=model_id,
            device=device_id
        )

    def preprocess(self, input_document_images_and_questions):
        # The pipeline handles all preprocessing, so we just pass the inputs through
        return input_document_images_and_questions

    def predict(self, model_input):
        # The model_input is a list of (image_path, question) tuples
        results = []
        for image_path, question in model_input:
            # The pipeline takes the image path and question directly
            result = self.nlp(image_path, question=question)
            results.append(result)
        return results

    def postprocess(self, model_output):
        # The model_output is a list of dictionaries, each containing an 'answer' key
        answers = [result['answer'] for result in model_output]
        return answers
