# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import pipeline
from PIL import Image

class PyTorch_Transformers_LayoutLM_Document_QA(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Note: multi_gpu is not directly used by the pipeline constructor

        model_id = "impira/layoutlm-document-qa"
        # The pipeline requires device to be an integer (-1 for CPU, 0 for first GPU)
        device_id = 0 if device == "cuda" else -1
        self.pipeline = pipeline("document-question-answering", model=model_id, device=device_id)

    def preprocess(self, input_document_images_and_questions):
        # The pipeline handles image loading and all preprocessing, so we just pass the inputs along.
        return input_document_images_and_questions

    def predict(self, model_input):
        # The pipeline processes one image-question pair at a time.
        results = []
        for image_path, question in model_input:
            result = self.pipeline(image_path, question=question)
            results.append(result)
        return results

    def postprocess(self, model_output):
        # The pipeline returns a dictionary with the answer, we need to extract it.
        answers = [result['answer'] for result in model_output]
        return answers
