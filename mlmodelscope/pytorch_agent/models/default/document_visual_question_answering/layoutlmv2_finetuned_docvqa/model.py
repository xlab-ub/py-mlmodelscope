# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoProcessor, AutoModelForDocumentQuestionAnswering
from PIL import Image
import pytesseract
import torch

class PyTorch_Transformers_LayoutLMv2_finetuned_DocVQA(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        self.device = torch.device(device)
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "TusharGoel/LayoutLMv2-finetuned-docvqa"
        self.processor = AutoProcessor.from_pretrained(model_id)
        self.model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_id).to(self.device)

    def preprocess(self, input_document_images_and_questions):
        all_images, all_questions, all_words, all_boxes = [], [], [], []
        for image_path, question in input_document_images_and_questions:
            image = Image.open(image_path).convert("RGB")
            width, height = image.size
            
            ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
            words = []
            boxes = []
            for i in range(len(ocr_data["text"])):
                if int(ocr_data["conf"][i]) > 0:
                    word = ocr_data["text"][i].strip()
                    if word:
                        x, y, w, h = ocr_data["left"][i], ocr_data["top"][i], ocr_data["width"][i], ocr_data["height"][i]
                        actual_box = [
                            int(1000 * (x / width)),
                            int(1000 * (y / height)),
                            int(1000 * ((x + w) / width)),
                            int(1000 * ((y + h) / height))
                        ]
                        words.append(word)
                        boxes.append(actual_box)

            all_images.append(image)
            all_questions.append(question)
            all_words.append(words)
            all_boxes.append(boxes)

        encoding = self.processor(
            images=all_images,
            text=all_questions,
            words=all_words,
            boxes=all_boxes,
            return_tensors="pt",
            padding="max_length",
            truncation=True
        ).to(self.device)
        
        return {"encoding": encoding, "words_per_sample": all_words}

    def predict(self, model_input):
        encoding = model_input["encoding"]
        words_per_sample = model_input["words_per_sample"]
        with torch.no_grad():
            model_output = self.model(**encoding)
        return model_output, encoding, words_per_sample

    def postprocess(self, model_output):
        model_output, encoding, words_per_sample = model_output_and_context
        answers = []
        for i in range(len(model_output.start_logits)):
            start_logits = model_output.start_logits[i]
            end_logits = model_output.end_logits[i]
            word_ids = encoding.word_ids(i)
            words = words_per_sample[i]

            start_index = torch.argmax(start_logits)
            end_index = torch.argmax(end_logits)

            start_word_idx = word_ids[start_index]
            end_word_idx = word_ids[end_index]

            if start_word_idx is None or end_word_idx is None or start_word_idx > end_word_idx:
                answers.append("")
                continue

            answer = " ".join(words[start_word_idx : end_word_idx + 1])
            answers.append(answer)
        return answers
