# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import pipeline
import torch

class PyTorch_Transformers_Llama_3_1_PersianQA(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "zpm/Llama-3.1-PersianQA"

        if multi_gpu and device == "cuda":
            self.pipeline = pipeline("question-answering", model=model_id, device_map="auto", torch_dtype="auto")
        else:
            torch_device = 0 if device == "cuda" and torch.cuda.is_available() else -1
            self.pipeline = pipeline("question-answering", model=model_id, device=torch_device)

    def preprocess(self, input_document_images_and_questions):
        # This model expects a list of (context, question) string tuples
        return input_contexts_and_questions

    def predict(self, model_input):
        questions = [item[1] for item in model_input]
        contexts = [item[0] for item in model_input]
        return self.pipeline(question=questions, context=contexts)

    def postprocess(self, model_output):
        if not isinstance(model_output, list):
            model_output = [model_output]
        answers = [result['answer'] for result in model_output]
        return answers
