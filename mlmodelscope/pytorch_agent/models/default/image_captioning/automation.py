"""
Generalized automation script for image-to-text PyTorch models.

Supports: image captioning, visual question answering (VQA), OCR,
document understanding, visual reasoning, and more.

Usage:
    from automation import image_to_text_model_automation

    image_to_text_model_automation(
        models_to_add=["nlpconnect/vit-gpt2-image-captioning"],
        task_type="image_captioning"
    )
"""

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Optional


def image_to_text_model_automation(models_to_add=None, task_type="image_captioning"):
    """Generalized automation for image-to-text tasks."""
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, {preprocess_input}):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
"""

    # --- 3. LangChain AI Configuration ---

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch image-to-text model."""

        model_type: str = Field(
            description="Model architecture: 'vision_encoder_decoder', 'blip', 'git', 'donut' (OCR/doc understanding), 'vqa', 'clip_interrogator', etc.",
            default="vision_encoder_decoder",
        )

        imports: str = Field(
            description="Complete imports. Include transformers classes, PIL Image. For VQA: may need question inputs. For OCR: TrOCR or Donut classes.",
            default="from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nfrom PIL import Image",
        )

        class_name: str = Field(
            description="Class name in PascalCase, e.g., 'PyTorch_Transformers_ViT_GPT2_Captioning', 'PyTorch_Transformers_BLIP_VQA'.",
        )

        init_config: str = Field(default=", config=None")

        init_body: str = Field(
            description="__init__ body: call super().__init__(config), load processor/tokenizer from_pretrained, load model using self.load_hf_model(ModelClass, model_id) for multi-GPU support, set generation params (max_length, num_beams, etc.).",
        )

        preprocess_input: str = Field(
            description="Preprocess method parameter: 'input_images' for captioning/OCR, 'input_images_and_questions' for VQA, 'input_documents' for doc understanding.",
            default="input_images",
        )

        preprocess_body: str = Field(
            description="Preprocess body: load images, optionally handle questions/text, process with processor, return model_input.",
        )

        predict_body: str = Field(
            description="Predict body: call model.generate() with generation params.",
        )

        postprocess_body: str = Field(
            description="Postprocess body: decode output with tokenizer/processor.batch_decode(), strip results.",
        )

    # Enhanced system prompt with examples and guidance
    # Using f-string with quadruple braces for JSON examples (same approach as other automation scripts)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch image captioning models. Your task is to generate a complete, working model.py file for a PyTorch image captioning wrapper class.

You will be given a Hugging Face model identifier and the text content from its model card page.
You must generate the complete model configuration based *primarily* on the provided page context.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - Most image captioning models use VisionEncoderDecoderModel (encoder-decoder architecture)
   - BLIP models use BlipProcessor and BlipForConditionalGeneration
   - GIT models use GitProcessor and GitForCausalLM
   - Check the model card to determine the exact architecture

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For VisionEncoderDecoderModel: `from transformers import VisionEncoderDecoderModel, ViTImageProcessor (or AutoImageProcessor), AutoTokenizer`
   - For BLIP: `from transformers import BlipProcessor, BlipForConditionalGeneration`
   - Always include: `from PIL import Image`
   - Add any other necessary imports

3. **Class Name:**
   - Use descriptive PascalCase: e.g., 'PyTorch_Transformers_ViT_GPT2_Image_Captioning', 'PyTorch_Transformers_BLIP_Image_Captioning_Base'
   - Include the framework/library prefix and model architecture

4. **Init Method:**
   - Initialize config: `super().__init__(config)` (required for load_hf_model to work)
   - Load processor/feature_extractor: `ProcessorClass.from_pretrained(model_id)`
   - Load tokenizer: `AutoTokenizer.from_pretrained(model_id)`
   - Load model: `self.model = self.load_hf_model(ModelClass, model_id)` (use load_hf_model for multi-GPU support)
   - Set generation parameters from config with defaults:
     - For VisionEncoderDecoderModel: max_length (default 16), num_beams (default 4)
     - For BLIP: max_new_tokens (default 32), text (optional, for conditional generation)
   - Check if trust_remote_code is needed

5. **Preprocess Method:**
   - Load images: `Image.open(image_path).convert('RGB')` for each image
   - Use processor/feature_extractor with return_tensors="pt"
   - Extract pixel_values if the processor returns a dict
   - Return model_input (typically pixel_values tensor)

6. **Predict Method:**
   - Call model.generate() with model_input and generation parameters
   - For VisionEncoderDecoderModel: `self.model.generate(model_input, max_length=self.max_length, num_beams=self.num_beams)`
   - For BLIP: `self.model.generate(model_input, max_new_tokens=self.max_new_tokens)`
   - Return the generated token ids

7. **Postprocess Method:**
   - Use tokenizer.batch_decode() or processor.batch_decode() with skip_special_tokens=True
   - Strip the results: `[pred.strip() for pred in preds]`
   - Return list of caption strings

**Reference Examples:**

Example 1: VisionEncoderDecoderModel (ViT-GPT2)
{{{{
    "model_type": "vision_encoder_decoder",
    "imports": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nfrom PIL import Image",
    "class_name": "PyTorch_Transformers_ViT_GPT2_Image_Captioning",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"nlpconnect/vit-gpt2-image-captioning\\"\\n        self.feature_extractor = ViTImageProcessor.from_pretrained(model_id)\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(VisionEncoderDecoderModel, model_id)\\n        \\n        self.max_length = self.config.get('max_length', 16)\\n        self.num_beams = self.config.get('num_beams', 4)",
    "preprocess_body": "for i in range(len(input_images)):\\n            input_images[i] = Image.open(input_images[i]).convert('RGB')\\n        model_input = self.feature_extractor(input_images, return_tensors=\\"pt\\").pixel_values\\n        return model_input",
    "predict_body": "return self.model.generate(model_input, max_length=self.max_length, num_beams=self.num_beams)",
    "postprocess_body": "preds = self.tokenizer.batch_decode(model_output, skip_special_tokens=True)\\n        return [pred.strip() for pred in preds]"
}}}}

Example 2: BLIP Model
{{{{
    "model_type": "blip",
    "imports": "from transformers import BlipProcessor, BlipForConditionalGeneration\\nfrom PIL import Image",
    "class_name": "PyTorch_Transformers_BLIP_Image_Captioning_Base",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"Salesforce/blip-image-captioning-base\\"\\n        self.processor = BlipProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(BlipForConditionalGeneration, model_id)\\n        \\n        self.max_new_tokens = self.config.get('max_new_tokens', 32)\\n        self.text = self.config.get('text', None)",
    "preprocess_body": "for i in range(len(input_images)):\\n            input_images[i] = Image.open(input_images[i]).convert('RGB')\\n        model_input = self.processor(input_images, text=self.text, return_tensors=\\"pt\\").pixel_values\\n        return model_input",
    "predict_body": "return self.model.generate(model_input, max_new_tokens=self.max_new_tokens)",
    "postprocess_body": "return self.processor.batch_decode(model_output, skip_special_tokens=True)"
}}}}

Example 3: VisionEncoderDecoderModel with ViTFeatureExtractor (older API)
{{{{
    "model_type": "vision_encoder_decoder",
    "imports": "from transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nfrom PIL import Image",
    "class_name": "PyTorch_Transformers_ViT_GPT2_COCO_EN",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"ydshieh/vit-gpt2-coco-en\\"\\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(VisionEncoderDecoderModel, model_id)\\n        \\n        self.max_length = self.config.get('max_length', 16)\\n        self.num_beams = self.config.get('num_beams', 4)",
    "preprocess_body": "for i in range(len(input_images)):\\n            input_images[i] = Image.open(input_images[i]).convert('RGB')\\n        model_input = self.feature_extractor(input_images, return_tensors=\\"pt\\").pixel_values\\n        return model_input",
    "predict_body": "return self.model.generate(model_input, max_length=self.max_length, num_beams=self.num_beams)",
    "postprocess_body": "preds = self.tokenizer.batch_decode(model_output, skip_special_tokens=True)\\n        return [pred.strip() for pred in preds]"
}}}}

**CRITICAL NOTES:**
- Pay attention to the processor/feature_extractor API (ViTImageProcessor vs ViTFeatureExtractor - newer models use ImageProcessor)
- Check if the model needs trust_remote_code=True
- Determine correct generation parameters (max_length vs max_new_tokens)
- For BLIP models, check if text parameter is needed for conditional generation
- Use .get() method for config parameters with defaults: `self.config.get('key', default_value)`
- Ensure pixel_values is extracted if processor returns a dict
- Handle both tokenizer.batch_decode() and processor.batch_decode() appropriately
- Strip whitespace from decoded captions
- Use proper indentation (8 spaces for method bodies, 12 spaces for nested blocks)
- Make sure the code is complete and runnable

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Here is the context from the model's Hugging Face page:
---
{model_page_context}
---
Based on the examples AND the context above, generate the complete config for the model: '{model_identifier}'

**IMPORTANT:** 
- Use the exact model identifier '{model_identifier}' in the init_body when loading the model (e.g., from_pretrained("{model_identifier}"))
- Do NOT use placeholders or generic names - use the actual model identifier provided above
- Generate complete, working code for all methods
- Handle any model-specific requirements (processor type, generation parameters, etc.)

Make sure to:
1. Determine the correct model architecture and processor type
2. Use the exact model identifier '{model_identifier}' in your code
3. Generate complete, working code for all methods
4. Handle any model-specific requirements (generation parameters, conditional inputs, etc.)
5. Include proper error handling if needed
""",
            ),
        ]
    )

    modelCounter = 0

    # --- LLM Setup ---
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found in .env file or environment.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser
    print("--- NOTE: Using a REAL LLM (Google Gemini). This will make API calls. ---")

    # --- 4. Main Generation Loop ---
    BASE_DIR = f"mlmodelscope/pytorch_agent/models/default/{task_type}"
    ERROR_DIR = f"{BASE_DIR}/errors"
    if not os.path.exists(ERROR_DIR):
        os.makedirs(ERROR_DIR)
        print(f"Created error directory: '{ERROR_DIR}'")
    failed_models = []
    login_req_models = []
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)
        print(f"Created base directory: '{BASE_DIR}'")

    for model_name in models_to_add:
        if modelCounter == 50:
            print("Added top 50 models. Achieved the goal for now.")
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue
        error_log = ""

        try:
            check_python_file_syntax_issue = lambda fileName: os.system(
                f"python -m py_compile {fileName}"
            )
            error = False
            while not os.path.exists(model_py_path) or (
                error := check_python_file_syntax_issue(model_py_path)
            ):
                if error:
                    print(
                        f"Syntax error detected in generated file '{model_py_path}'. Regenerating..."
                    )
                    error_log += f"Syntax error detected in generated file '{model_py_path}'. Regenerating...\n"

                # --- 1. Fetch Context from Hugging Face ---
                url = f"https://huggingface.co/{model_name}"
                print(f"Fetching context from {url}...")
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    print("Could not find main content on page, skipping.")
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    print("Login required, skipping page.")
                    login_req_models.append(model_name)
                    break
                context_text = main_content.get_text(separator=" ", strip=True)
                max_length = 20000
                if len(context_text) > max_length:
                    context_text = (
                        context_text[:max_length] + "\n... (content truncated)"
                    )
                print("Context fetched successfully.")

                # --- 2. Invoke LLM with Context ---
                print("Invoking LLM to generate config...")
                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )
                print("--- LLM Output (Parsed) ---")
                print(model_config_dict)
                print("----------------------------")

                # --- 3. Fill Template ---
                # Ensure model identifier is used correctly in init_body
                # The LLM should already use the correct identifier, but this is a safety check
                init_body = model_config_dict.get("init_body", "")
                # Replace common placeholder patterns if they exist (safety fallback)
                init_body = init_body.replace("{hugging_face_model_id}", model_name)
                init_body = init_body.replace(
                    "MODEL_IDENTIFIER_PLACEHOLDER", model_name
                )

                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nfrom PIL import Image",
                    ),
                    class_name=model_config_dict.get(
                        "class_name",
                        f"PyTorch_{task_type.replace('_', ' ').title().replace(' ', '_')}_Model",
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_input=model_config_dict.get(
                        "preprocess_input", "input_images"
                    ),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "pass"
                    ).lstrip(" "),
                    predict_body=model_config_dict.get(
                        "predict_body", "return self.model.generate(model_input)"
                    ).lstrip(" "),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body",
                        "preds = self.tokenizer.batch_decode(model_output, skip_special_tokens=True)\n        return [pred.strip() for pred in preds]",
                    ).lstrip(" "),
                )

                # --- 4. Create directory and write file ---
                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

                print(f"Generated file: {model_py_path}")

            else:
                print(f"Successfully generated {model_py_path}")
                print(f"--- Generated content for {model_folder_name}/model.py: ---")
                modelCounter += 1

        except Exception as e:
            print(f"Failed to generate code for {model_folder_name}: {e}")
            import traceback

            error_log += f"Exception: {e}\n"
            traceback.print_exc()
            with open(
                os.path.join(ERROR_DIR, f"{model_folder_name}_error.log"), "w"
            ) as error_file:
                error_file.write(error_log)
                error_file.write(f"\n\nFull traceback:\n")
                traceback.print_exc(file=error_file)
            failed_models.append(model_name)
            continue

    print(f"\n--- Automation complete. ---")
    print(f"Check the '{BASE_DIR}' folder for new model files.")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for failed_model in failed_models:
            f.write(f"{failed_model}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for failed_model in login_req_models:
            f.write(f"{failed_model}\n")
    if failed_models:
        print(f"Some models failed. See '{ERROR_DIR}/failed_models.log' for details.")
    if login_req_models:
        print(
            f"Some models needed login. See '{ERROR_DIR}/login_req_models.log' for details."
        )
    else:
        print("All models processed successfully.")


# Backward compatibility wrapper
def image_captioning_model_automation(models_to_add=None):
    """Legacy function name. Use image_to_text_model_automation instead."""
    return image_to_text_model_automation(models_to_add, "image_captioning")


if __name__ == "__main__":
    # Image captioning
    # image_to_text_model_automation(
    #     models_to_add=["nlpconnect/vit-gpt2-image-captioning"],
    #     task_type="image_captioning"
    # )

    # OCR
    image_to_text_model_automation(
        models_to_add=["microsoft/trocr-base-printed"], task_type="ocr"
    )
