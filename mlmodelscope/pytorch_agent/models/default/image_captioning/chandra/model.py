# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoModelForCausalLM, AutoProcessor
from PIL import Image
import torch
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Transformers_Chandra_OCR(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "datalab-to/chandra"

        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype="auto",
                device_map="auto"
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype="auto"
            ).to(device)

        self.max_new_tokens = self.config.get('max_new_tokens', 2048)
        self.prompt_type = self.config.get('prompt_type', 'ocr_layout')
        self.prompts = {
            'ocr_layout': "You are an OCR model. You will be given an image of a document. Your task is to transcribe the document and return the content in markdown format. Preserve the layout of the original document. Do not summarize or omit any part of the document."
        }
        self.prompt = self.prompts.get(self.prompt_type, self.prompts['ocr_layout'])

    def preprocess(self, input_images):
        images = [Image.open(image_path).convert('RGB') for image_path in input_images]
        model_input = self.processor(text=self.prompt, images=images, return_tensors="pt")
        return {k: v.to(self.model.device) for k, v in model_input.items()}

    def predict(self, model_input):
        return self.model.generate(**model_input, max_new_tokens=self.max_new_tokens)

    def postprocess(self, model_output):
        decoded_outputs = self.processor.batch_decode(model_output, skip_special_tokens=True)
        results = [output.replace(self.prompt, "").strip() for output in decoded_outputs]
        return results
