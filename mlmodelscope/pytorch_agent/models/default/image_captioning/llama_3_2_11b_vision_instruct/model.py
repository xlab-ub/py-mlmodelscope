# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoProcessor, Llama3p2ForConditionalGeneration
from PIL import Image
import torch

class PyTorch_Transformers_Llama3_2_11B_Vision_Instruct(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        model_id = "unsloth/Llama-3.2-11B-Vision-Instruct"

        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = Llama3p2ForConditionalGeneration.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            )
        else:
            self.model = Llama3p2ForConditionalGeneration.from_pretrained(
                model_id,
                trust_remote_code=True
            ).to(device)

        self.prompt = self.config.get('prompt', "Describe the image in detail.")
        self.max_new_tokens = self.config.get('max_new_tokens', 128)

    def preprocess(self, input_images):
        images = [Image.open(image_path).convert('RGB') for image_path in input_images]
        
        # Llama 3.2 Vision uses a specific chat template
        text_prompts = [
            f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image_1|>\n{self.prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            for _ in images
        ]

        model_input = self.processor(text=text_prompts, images=images, return_tensors="pt")
        return model_input

    def predict(self, model_input):
        # The model input is a dictionary, so we unpack it
        return self.model.generate(**model_input, max_new_tokens=self.max_new_tokens, do_sample=False)

    def postprocess(self, model_output):
        preds = self.processor.batch_decode(model_output, skip_special_tokens=True)
        
        # The model output includes the prompt, so we need to remove it.
        # The response starts after the 'assistant' turn marker.
        clean_preds = []
        for pred in preds:
            parts = pred.split("assistant<|end_header_id|>\n\n")
            if len(parts) > 1:
                # Take the last part which is the generated text
                caption = parts[-1].strip()
                clean_preds.append(caption)
            else:
                # Fallback if the template is not found
                clean_preds.append(pred.strip())
        
        return clean_preds
