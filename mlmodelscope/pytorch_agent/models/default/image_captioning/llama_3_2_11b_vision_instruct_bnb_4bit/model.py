# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import torch

class PyTorch_Transformers_Unsloth_Llama_3_2_Vision_Instruct(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"

        self.processor = AutoProcessor.from_pretrained(model_id)

        if multi_gpu and device == "cuda":
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                trust_remote_code=True,
                device_map="auto",
                torch_dtype="auto"
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                trust_remote_code=True
            )

        self.prompt_template = self.config.get('prompt_template', "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image_1|>\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n")
        self.instruction = self.config.get('instruction', "Generate a short caption for the image.")
        self.max_new_tokens = self.config.get('max_new_tokens', 50)

    def preprocess(self, input_images):
        images = [Image.open(img_path).convert('RGB') for img_path in input_images]
        prompt = self.prompt_template.format(instruction=self.instruction)
        prompts = [prompt] * len(images)

        model_input = self.processor(text=prompts, images=images, return_tensors="pt")
        self.input_token_len = model_input['input_ids'].shape[1]

        return model_input

    def predict(self, model_input):
        return self.model.generate(**model_input, max_new_tokens=self.max_new_tokens)

    def postprocess(self, model_output):
        generated_ids = model_output[:, self.input_token_len:]
        preds = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        return [pred.strip() for pred in preds]
