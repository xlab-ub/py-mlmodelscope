# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from PIL import Image
import torch

class PyTorch_Transformers_Qwen2_5_VL_OCR(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "allenai/olmOCR-2-7B-1025-FP8"
        processor_id = "Qwen/Qwen2.5-VL-7B-Instruct"

        self.processor = AutoProcessor.from_pretrained(processor_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype="auto",
                device_map="auto",
                trust_remote_code=True
            )
        else:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
                trust_remote_code=True
            ).to(device)

        self.model.eval()

        # Generation parameters from model card, with a more suitable default for OCR
        self.max_new_tokens = self.config.get('max_new_tokens', 2048)
        self.do_sample = self.config.get('do_sample', True)
        self.temperature = self.config.get('temperature', 0.1)

        # Default prompt based on the model card's recommendation
        default_prompt = """---
primary_language: en
is_rotation_valid: True
rotation_correction: 0
is_table: False
is_diagram: False
---"""
        self.prompt_text = self.config.get('prompt_text', default_prompt)

    def preprocess(self, input_images):
        images = [Image.open(image_path).convert('RGB') for image_path in input_images]

        all_messages = []
        for _ in images:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.prompt_text},
                        {"type": "image"},
                    ],
                }
            ]
            all_messages.append(messages)

        texts = [self.processor.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in all_messages]

        model_input = self.processor(
            text=texts,
            images=images,
            padding=True,
            return_tensors="pt"
        )
        return model_input

    def predict(self, model_input):
        generated_ids = self.model.generate(
            **model_input,
            max_new_tokens=self.max_new_tokens,
            do_sample=self.do_sample,
            temperature=self.temperature
        )
        # Return both input and output ids for correct decoding in postprocess
        return {'output_ids': generated_ids, 'input_ids': model_input['input_ids']}

    def postprocess(self, model_output):
        output_ids = model_output['output_ids']
        input_ids = model_output['input_ids']

        prompt_length = input_ids.shape[1]
        new_tokens = output_ids[:, prompt_length:]

        preds = self.processor.batch_decode(new_tokens, skip_special_tokens=True)
        return [pred.strip() for pred in preds]
