# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from paddleocr import PaddleOCR

class PyTorch_PaddleOCR_PP_OCRv5_Server_Det(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        # multi_gpu is not directly supported by paddleocr's simple interface
        self.config.pop("_multi_gpu", False)

        use_gpu_flag = True if "cuda" in device else False

        # This model is a text detector. To get text output (like a caption),
        # we use the full PaddleOCR pipeline which includes a text recognizer.
        # The model card shows PP-OCRv5_server_rec is used in the pipeline example.
        self.model = PaddleOCR(
            text_detection_model_name="PaddlePaddle/PP-OCRv5_server_det",
            text_recognition_model_name="PP-OCRv5_server_rec",
            use_angle_cls=self.config.get('use_angle_cls', True),
            lang=self.config.get('lang', 'en'),
            use_gpu=use_gpu_flag
        )

    def preprocess(self, input_images):
        # The PaddleOCR library can process image file paths directly.
        # The input_images are expected to be a list of file paths.
        return input_images

    def predict(self, model_input):
        # The ocr method processes each image and returns its results.
        # We loop through the input paths to get results for each image.
        results = []
        for img_path in model_input:
            # The result for a single image is a list of detected lines,
            # which itself is wrapped in another list by the ocr method.
            # e.g., [[[[box], ('text', score)], ...]]
            result = self.model.ocr(img_path, cls=True)
            results.append(result)
        return results

    def postprocess(self, model_output):
        # model_output is a list of results, one for each input image.
        # e.g., [result_for_img1, result_for_img2, ...]
        # where result_for_img1 = [[[[box1], ('text1', score1)], [[box2], ('text2', score2)]]]
        
        captions = []
        for result_per_image in model_output:
            # Handle cases where no text is detected
            if not result_per_image or not result_per_image[0]:
                captions.append("")
                continue

            # The actual list of lines is the first element of the result list
            lines = result_per_image[0]
            
            # Extract the text part from each line's tuple (text, confidence)
            detected_texts = [line[1][0] for line in lines if line]
            
            # Join all detected text lines with a newline to form a single string
            full_text = "\n".join(detected_texts)
            captions.append(full_text.strip())
            
        return captions
