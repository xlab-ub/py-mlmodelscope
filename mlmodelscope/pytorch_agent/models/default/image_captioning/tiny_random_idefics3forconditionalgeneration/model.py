# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoProcessor, Idefics3ForConditionalGeneration
from PIL import Image

class PyTorch_Transformers_Idefics3_Image_Captioning(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "optimum-intel-internal-testing/tiny-random-Idefics3ForConditionalGeneration"

        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        
        if multi_gpu and device == "cuda":
            self.model = Idefics3ForConditionalGeneration.from_pretrained(model_id, device_map="auto", torch_dtype="auto", trust_remote_code=True)
        else:
            self.model = Idefics3ForConditionalGeneration.from_pretrained(model_id, trust_remote_code=True)

        self.prompt_text = self.config.get('prompt_text', "Generate a caption for this image.")
        self.max_new_tokens = self.config.get('max_new_tokens', 128)

    def preprocess(self, input_images):
        images = []
        prompts = []
        for image_path in input_images:
            images.append(Image.open(image_path).convert("RGB"))
            # Idefics3 uses a specific chat format for prompts
            prompts.append(
                [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image"},
                            {"type": "text", "text": self.prompt_text}
                        ]
                    }
                ]
            )
        
        # The processor handles both image preprocessing and text tokenization
        model_input = self.processor(prompts, images=images, return_tensors="pt")
        return model_input

    def predict(self, model_input):
        # Idefics3 models require a specific stopping condition to avoid generating past the intended output.
        # We use the token for "<end_of_utterance>" as a bad_word_id to stop generation.
        exit_condition = self.processor.tokenizer("<end_of_utterance>", add_special_tokens=False).input_ids
        bad_words_ids = [[eos_id] for eos_id in exit_condition]

        return self.model.generate(
            **model_input,
            max_new_tokens=self.max_new_tokens,
            bad_words_ids=bad_words_ids
        )

    def postprocess(self, model_output):
        decoded_outputs = self.processor.batch_decode(model_output, skip_special_tokens=True)
        captions = []
        for output in decoded_outputs:
            # The model's output includes the original prompt, so we split the string at "Assistant:"
            # to isolate the newly generated caption.
            parts = output.split("Assistant:")
            if len(parts) > 1:
                caption = parts[-1].strip()
                captions.append(caption)
            else:
                # Fallback in case the output format is unexpected
                captions.append(output.strip())
        return captions
