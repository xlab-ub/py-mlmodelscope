# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from PIL import Image
import torch

class PyTorch_Transformers_Typhoon_OCR_7B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "scb10x/typhoon-ocr-7b"
        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            ).eval()
        else:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            ).eval()

        # Generation parameters from model card
        self.max_new_tokens = self.config.get('max_new_tokens', 12000)
        self.temperature = self.config.get('temperature', 0.1)
        self.top_p = self.config.get('top_p', 0.6)
        self.repetition_penalty = self.config.get('repetition_penalty', 1.2)
        self.do_sample = self.config.get('do_sample', True)

        # Prompting parameters
        self.base_text = self.config.get("base_text", "")
        self.prompt_template = self.config.get(
            "prompt_template",
            "Below is an image of a document page along with its dimensions. "
            "Simply return the markdown representation of this document, presenting tables in markdown format as they naturally appear.\n"
            "If the document contains images, use a placeholder like dummy.png for each image.\n"
            "Your final output must be in JSON format with a single key `natural_text` containing the response.\n"
            "RAW_TEXT_START\n{base_text}\nRAW_TEXT_END"
        )

    def preprocess(self, input_images):
        images = [Image.open(p).convert('RGB') for p in input_images]
        prompt = self.prompt_template.format(base_text=self.base_text)

        # This model uses a chat-like input format. We construct it here for each image in the batch.
        texts = []
        for _ in images:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                    ]
                }
            ]
            formatted_text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            texts.append(formatted_text)

        model_input = self.processor(
            text=texts,
            images=images,
            padding=True,
            return_tensors="pt"
        )
        return model_input

    def predict(self, model_input):
        # The model generates tokens for both the prompt and the answer, so we need to slice the output.
        prompt_length = model_input["input_ids"].shape[1]

        output = self.model.generate(
            **model_input,
            max_new_tokens=self.max_new_tokens,
            temperature=self.temperature,
            top_p=self.top_p,
            repetition_penalty=self.repetition_penalty,
            do_sample=self.do_sample
        )

        # Return only the newly generated tokens
        new_tokens = output[:, prompt_length:]
        return new_tokens

    def postprocess(self, model_output):
        preds = self.processor.tokenizer.batch_decode(model_output, skip_special_tokens=True)
        return [pred.strip() for pred in preds]
