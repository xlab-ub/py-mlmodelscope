from datetime import datetime
import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Optional


def image_classification_model_automation(models_to_add=None):
    # --- 1. Model List ---

    if models_to_add is None:
        return "No models specified for addition."

    # --- 2. Model.py Template ---
    # We give the LLM more freedom to generate the complete model structure
    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        super().__init__(config)
        {init_body}

    def preprocess(self, input_images):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
"""

    # --- 3. LangChain AI Configuration ---

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch image classification model."""

        model_type: str = Field(
            description="The type of model loading method. Options: 'torchvision_hub', 'transformers', 'custom_torchvision', 'custom_implementation'. 'torchvision_hub' uses torch.hub.load, 'transformers' uses HuggingFace transformers, 'custom_torchvision' uses torchvision models directly, 'custom_implementation' requires custom model class definition.",
            default="torchvision_hub",
        )

        # Imports section
        imports: str = Field(
            description="The complete imports section as a multi-line string. Include all necessary imports like torch, torchvision, transformers, PIL, etc. Use proper formatting with newlines.",
            default="import torch\nfrom torchvision import transforms\nfrom PIL import Image",
        )

        # Class name
        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'TorchVision_ResNet50' or 'PyTorch_Transformers_ViT_Base'.",
        )

        # Init method
        init_config: str = Field(
            description="The __init__ method signature parameters. Use empty string if no config parameter, or ', config=None' if config is needed.",
            default="",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method (after super().__init__(config) call). This should include model loading, any setup, optional features file download, etc. For transformers models: use self.load_hf_model(ModelClass, model_id) instead of from_pretrained with device_map. For torchvision: use torch.hub.load as normal. Be comprehensive and handle all model-specific initialization.",
        )

        # Preprocess method
        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Handle image loading, preprocessing transforms, and return the model input tensor. The method receives input_images (list of image file paths).",
        )

        # Predict method
        predict_body: str = Field(
            description="The complete body of the predict method. Call the model with model_input and return the output.",
            default="return self.model(model_input)",
        )

        # Postprocess method
        postprocess_body: str = Field(
            description="The complete body of the postprocess method. Typically applies softmax and converts to list. Handle model output format (may need to extract .logits for transformers).",
            default="probabilities = torch.nn.functional.softmax(model_output, dim=1)\n        return probabilities.tolist()",
        )

    # Enhanced system prompt with examples and guidance
    # Using f-string with quadruple braces for JSON examples (same approach as text_to_text automation)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch image classification models. Your task is to generate a complete, working model.py file for a PyTorch image classification wrapper class.

You will be given a Hugging Face model identifier and the text content from its model card page.
You must generate the complete model configuration based *primarily* on the provided page context.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - If it's a standard torchvision model (resnet, densenet, mobilenet, etc.), use 'torchvision_hub' with torch.hub.load
   - If it's a HuggingFace transformers model (ViT, DeiT, ConvNeXT, etc.), use 'transformers' with AutoImageProcessor and AutoModelForImageClassification
   - If it requires custom implementation or special handling, use 'custom_implementation' or 'custom_torchvision'

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For torchvision: `import torch`, `from torchvision import transforms`, `from PIL import Image`
   - For transformers: `from transformers import AutoImageProcessor, AutoModelForImageClassification` (or specific classes)
   - Add any other necessary imports

3. **Class Name:**
   - Use descriptive PascalCase: e.g., 'TorchVision_ResNet50', 'PyTorch_Transformers_ViT_Base_Patch16_224'
   - Include the framework/library prefix

4. **Init Method:**
   - The template already calls `super().__init__(config)` which handles device/multi_gpu from config
   - For torchvision_hub: Use `torch.hub.load(repo, model_name, pretrained=True)` (doesn't support device_map, uses .to(device) instead)
   - **For transformers: Load processor and model using self.load_hf_model() which handles multi-GPU automatically:**
     ```
     model_id = "google/vit-base-patch16-224"
     self.processor = AutoImageProcessor.from_pretrained(model_id)
     self.model = self.load_hf_model(AutoModelForImageClassification, model_id)
     ```
     Or with trust_remote_code:
     ```
     self.model = self.load_hf_model(AutoModelForImageClassification, model_id, trust_remote_code=True)
     ```
   - **DO NOT manually extract device/multi_gpu or add device_map/torch_dtype for transformers** - use `self.load_hf_model()` instead
   - Optionally download features file (synset.txt) if using ImageNet classes
   - Set model to eval mode if necessary

5. **Preprocess Method:**
   - Open images: `Image.open(image_path).convert('RGB')`
   - Apply transforms (Resize, CenterCrop, ToTensor, Normalize)
   - For ImageNet models: typically Resize(256), CenterCrop(224), normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
   - For transformers: Use image_processor/processor with return_tensors="pt"
   - Stack images into batch tensor
   - Return model_input tensor

6. **Predict Method:**
   - Call self.model(model_input) for most cases
   - For transformers, might need: self.model(**model_input) or self.model(pixel_values=model_input)
   - Return raw model output

7. **Postprocess Method:**
   - Apply softmax: `torch.nn.functional.softmax(model_output, dim=1)` or extract logits first for transformers
   - Convert to list: `.tolist()`
   - For transformers, may need: `torch.nn.functional.softmax(model_output.logits, dim=1).tolist()`

**Reference Examples:**

Example 1: TorchVision ResNet50 (torchvision_hub)
{{{{
    "model_type": "torchvision_hub",
    "imports": "import torch\\nfrom torchvision import transforms\\nfrom PIL import Image",
    "class_name": "TorchVision_ResNet50",
    "init_config": "",
    "init_body": "self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\\n        \\n        features_file_url = \\"http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt\\"\\n        self.features = self.features_download(features_file_url)",
    "preprocess_body": "preprocessor = transforms.Compose([\\n            transforms.Resize(256),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n        ])\\n        for i in range(len(input_images)):\\n            input_images[i] = preprocessor(Image.open(input_images[i]).convert('RGB'))\\n        model_input = torch.stack(input_images)\\n        return model_input",
    "predict_body": "return self.model(model_input)",
    "postprocess_body": "probabilities = torch.nn.functional.softmax(model_output, dim=1)\\n        return probabilities.tolist()"
}}}}

Example 2: HuggingFace ViT (transformers with Multi-GPU)
{{{{
    "model_type": "transformers",
    "imports": "from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport torch",
    "class_name": "PyTorch_Transformers_ViT_Base_Patch16_224",
    "init_config": ", config=None",
    "init_body": "model_id = \\"google/vit-base-patch16-224\\"\\n        self.processor = AutoImageProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForImageClassification, model_id)\\n        self.model.eval()",
    "preprocess_body": "processed_images = [\\n            Image.open(image_path).convert('RGB')\\n            for image_path in input_images\\n        ]\\n        model_input = self.processor(processed_images, return_tensors=\\"pt\\")\\n        return model_input",
    "predict_body": "return self.model(**model_input)",
    "postprocess_body": "probabilities = torch.nn.functional.softmax(model_output.logits, dim=1)\\n        return probabilities.tolist()"
}}}}

Example 3: Custom torchvision model (custom_torchvision)
{{{{
    "model_type": "custom_torchvision",
    "imports": "import torch\\nimport torch.nn as nn\\nfrom torchvision import transforms\\nfrom PIL import Image",
    "class_name": "TorchVision_CustomModel",
    "init_config": "",
    "init_body": "from torchvision import models\\n        self.model = models.resnet50(pretrained=True)\\n        # Custom modifications if needed",
    "preprocess_body": "preprocessor = transforms.Compose([\\n            transforms.Resize(256),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n        ])\\n        for i in range(len(input_images)):\\n            input_images[i] = preprocessor(Image.open(input_images[i]).convert('RGB'))\\n        model_input = torch.stack(input_images)\\n        return model_input",
    "predict_body": "return self.model(model_input)",
    "postprocess_body": "probabilities = torch.nn.functional.softmax(model_output, dim=1)\\n        return probabilities.tolist()"
}}}}

**CRITICAL NOTES:**
- **Multi-GPU Support for Transformers**: Use `self.load_hf_model(ModelClass, model_id)` - it handles multi-GPU automatically via the parent class
- **DO NOT** manually extract device/multi_gpu or add device_map/torch_dtype for transformers models - use `self.load_hf_model()` instead
- For torchvision models: device placement is handled via `.to(device)` method (not device_map)
- Pay attention to image size requirements (224x224, 299x299, 384x384, etc.)
- Check normalization values (ImageNet standard vs custom)
- For transformers models, check if trust_remote_code is needed (pass as kwarg to load_hf_model)
- Handle different model output formats (logits attribute vs direct output)
- I will take care of indentation when filling the template, so provide code bodies without extra indentation
- Include features file download if using ImageNet classes
- Make sure the code is complete and runnable

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Here is the context from the model's Hugging Face page:
---
{model_page_context}
---
Based on the examples AND the context above, generate the complete config for the model: '{model_identifier}'

**IMPORTANT:** 
- Use the exact model identifier '{model_identifier}' in the init_body when loading the model (e.g., model_id = "{model_identifier}" or from_pretrained("{model_identifier}"))
- Do NOT use placeholders or generic names - use the actual model identifier provided above
- Generate complete, working code for all methods
- Handle any model-specific requirements (image size, normalization, etc.)

Make sure to:
1. Determine the correct model type and loading method
2. Use the exact model identifier '{model_identifier}' in your code
3. Generate complete, working code for all methods
4. Handle any model-specific requirements (image size, normalization, etc.)
5. Include proper error handling if needed
""",
            ),
        ]
    )

    modelCounter = 0

    # --- LLM Setup ---
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found in .env file or environment.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser
    print("--- NOTE: Using a REAL LLM (Google Gemini). This will make API calls. ---")

    # --- 4. Main Generation Loop ---
    BASE_DIR = "mlmodelscope/pytorch_agent/models/default/image_classification"
    ERROR_DIR = (
        "mlmodelscope/pytorch_agent/models/default/image_classification/automation/"
        + str(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    )
    if not os.path.exists(ERROR_DIR):
        os.makedirs(ERROR_DIR)
        print(f"Created error directory: '{ERROR_DIR}'")
    failed_models = []
    login_req_models = []
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)
        print(f"Created base directory: '{BASE_DIR}'")

    for model_name in models_to_add:
        if modelCounter == 50:
            print("Added top 50 models. Achieved the goal for now.")
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue
        error_log = ""

        try:
            check_syntax = lambda fileName: os.system(
                f"python -m py_compile {fileName}"
            )
            error = False
            MAX_TRIES_PER_MODEL = 5
            try_count_my_model = 0
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if try_count_my_model >= MAX_TRIES_PER_MODEL:
                    break
                try_count_my_model += 1
                if error:
                    print(
                        f"Syntax error detected in generated file '{model_py_path}'. Regenerating..."
                    )
                    error_log += f"Syntax error detected in generated file '{model_py_path}'. Regenerating...\n"

                # --- 1. Fetch Context from Hugging Face ---
                url = f"https://huggingface.co/{model_name}"
                print(f"Fetching context from {url}...")
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    print("Could not find main content on page, skipping.")
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    print("Login required, skipping page.")
                    login_req_models.append(model_name)
                    break
                context_text = main_content.get_text(separator=" ", strip=True)
                print("Context fetched successfully.")

                # --- 2. Invoke LLM with Context ---
                print("Invoking LLM to generate config...")
                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )
                print("--- LLM Output (Parsed) ---")
                print(model_config_dict)
                print("----------------------------")

                # --- 3. Fill Template ---
                # Ensure model identifier is used correctly in init_body
                # The LLM should already use the correct identifier, but this is a safety check
                init_body = model_config_dict.get("init_body", "")
                # Replace common placeholder patterns if they exist (safety fallback)
                init_body = init_body.replace("{hugging_face_model_id}", model_name)
                init_body = init_body.replace(
                    "MODEL_IDENTIFIER_PLACEHOLDER", model_name
                )
                handle_indent = lambda code: code
                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "import torch\nfrom torchvision import transforms\nfrom PIL import Image",
                    ),
                    class_name=model_config_dict.get(
                        "class_name", "PyTorch_ImageClassification_Model"
                    ),
                    init_config=model_config_dict.get("init_config", ""),
                    init_body=handle_indent(init_body),
                    preprocess_body=handle_indent(
                        model_config_dict.get("preprocess_body", "pass")
                    ),
                    predict_body=handle_indent(
                        model_config_dict.get(
                        "predict_body", "return self.model(model_input)"
                        )
                    ),
                    postprocess_body=handle_indent(
                        model_config_dict.get(
                        "postprocess_body",
                        "probabilities = torch.nn.functional.softmax(model_output, dim=1)\n        return probabilities.tolist()",
                        )
                    ),
                )

                # --- 4. Create directory and write file ---
                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

                print(f"Generated file: {model_py_path}")

            else:
                print(f"Successfully generated {model_py_path}")
                print(f"--- Generated content for {model_folder_name}/model.py: ---")
                modelCounter += 1

        except Exception as e:
            print(f"Failed to generate code for {model_folder_name}: {e}")
            import traceback

            error_log += f"Exception: {e}\n"
            traceback.print_exc()
            with open(
                os.path.join(ERROR_DIR, f"{model_folder_name}_error.log"), "w"
            ) as error_file:
                error_file.write(error_log)
                error_file.write(f"\n\nFull traceback:\n")
                traceback.print_exc(file=error_file)
            failed_models.append(model_name)
            continue

    print(f"\n--- Automation complete. ---")
    print(f"Check the '{BASE_DIR}' folder for new model files.")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for failed_model in failed_models:
            f.write(f"{failed_model}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for failed_model in login_req_models:
            f.write(f"{failed_model}\n")
    if failed_models:
        print(f"Some models failed. See '{ERROR_DIR}/failed_models.log' for details.")
    if login_req_models:
        print(
            f"Some models needed login. See '{ERROR_DIR}/login_req_models.log' for details."
        )
    else:
        print("All models processed successfully.")


if __name__ == "__main__":
    image_classification_model_automation(
        models_to_add=[
    "Falconsai/nsfw_image_detection",
    "dima806/fairface_age_image_detection",
    "timm/mobilenetv3_small_100.lamb_in1k",
    "timm/resnet50.a1_in1k",
    "timm/convnextv2_nano.fcmae_ft_in22k_in1k",
    "google/vit-base-patch16-224",
    "timm/resnet18.a1_in1k",
    "apple/mobilevit-small",
    "Freepik/nsfw_image_detector",
    "rizvandwiki/gender-classification",
    "timm/efficientnet_b0.ra_in1k",
    "microsoft/beit-base-patch16-224-pt22k-ft22k",
    "timm/resnet34.a1_in1k",
    "timm/vit_small_patch16_224.augreg_in21k_ft_in1k",
    "timm/vit_base_patch16_224.augreg2_in21k_ft_in1k",
    "timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k",
    "timm/resnet50.ram_in1k",
    "smp-hub/efficientnet-b0.imagenet",
    "amunchet/rorshark-vit-base",
    "nateraw/vit-age-classifier",
    "rizvandwiki/gender-classification-2",
    "timm/mobilenetv3_large_100.ra_in1k",
    "timm/convnext_femto.d1_in1k",
    "microsoft/swinv2-tiny-patch4-window16-256",
    "timm/convnext_tiny.in12k_ft_in1k",
    "timm/convnext_large.fb_in22k_ft_in1k",
    "timm/wide_resnet50_2.racm_in1k",
    "timm/vgg19.tv_in1k",
    "timm/rexnet_150.nav_in1k",
    "nvidia/mit-b3",
    "AdamCodd/vit-base-nsfw-detector",
    "trpakov/vit-face-expression",
    "timm/regnety_032.ra_in1k",
    "StephanAkkerman/chart-recognizer",
    "microsoft/resnet-50",
    "google/vit-hybrid-base-bit-384",
    "timm/resnet50.fb_swsl_ig1b_ft_in1k",
    "timm/deit_small_patch16_224.fb_in1k",
    "timm/vit_base_patch16_224.augreg_in21k",
    "timm/vit_base_patch32_384.augreg_in21k_ft_in1k",
    "nvidia/mit-b2",
    "timm/edgenext_small.usi_in1k",
    "haywoodsloan/ai-image-detector-dev-deploy",
    "google/vit-base-patch16-384",
    "timm/convnext_base.clip_laion2b_augreg_ft_in12k_in1k",
    "microsoft/resnet-18",
    "timm/deit_tiny_patch16_224.fb_in1k",
    "timm/convnextv2_tiny.fcmae_ft_in1k",
    "skshmjn/Pokemon-classifier-gen9-1025",
    "timm/inception_v3.tv_in1k",
    "timm/convnext_nano.in12k_ft_in1k",
    "timm/tf_mobilenetv3_large_minimal_100.in1k",
    "google/vit-large-patch16-384",
    "facebook/convnextv2-tiny-22k-224",
    "microsoft/beit-base-patch16-384",
    "timm/mobilenetv3_small_075.lamb_in1k",
    "timm/convnext_base.fb_in22k_ft_in1k",
    "timm/inception_v4.tf_in1k",
    "timm/beitv2_base_patch16_224.in1k_ft_in22k",
    "timm/mobilenetv3_large_100.miil_in21k_ft_in1k",
    "timm/tf_efficientnetv2_s.in21k_ft_in1k",
    "timm/tf_mobilenetv3_small_minimal_100.in1k",
    "microsoft/swin-large-patch4-window7-224",
    "timm/eva02_large_patch14_448.mim_m38m_ft_in22k_in1k",
    "Ateeqq/ai-vs-human-image-detector",
    "timm/vit_base_patch8_224.augreg2_in21k_ft_in1k",
    "timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k",
    "timm/swin_tiny_patch4_window7_224.ms_in1k",
    "Qdrant/clip-ViT-B-32-vision",
    "timm/tf_efficientnetv2_m.in21k_ft_in1k",
    "timm/resnet101.tv_in1k",
    "WinKawaks/vit-small-patch16-224",
    "prithivMLmods/open-deepfake-detection",
    "timm/inception_v3.tf_adv_in1k",
    "nvidia/mit-b0",
    "timm/mobilenetv3_small_050.lamb_in1k",
    "timm/resnet18.fb_swsl_ig1b_ft_in1k",
    "timm/tf_efficientnetv2_s.in21k",
    "microsoft/swin-tiny-patch4-window7-224",
    "dima806/facial_emotions_image_detection",
    "timm/tf_efficientnet_b0.ns_jft_in1k",
    "timm/efficientnet_b3.ra2_in1k",
    "timm/vit_tiny_patch16_224.augreg_in21k",
    "timm/densenet121.ra_in1k",
    "timm/deit_base_distilled_patch16_384.fb_in1k",
    "timm/mobilenetv2_100.ra_in1k",
    "timm/convnext_xxlarge.clip_laion2b_soup_ft_in1k",
    "timm/twins_svt_large.in1k",
    "timm/swin_base_patch4_window12_384.ms_in22k_ft_in1k",
    "timm/vit_large_patch16_384.augreg_in21k_ft_in1k",
    "google/mobilenet_v2_1.0_224",
    "dima806/deepfake_vs_real_image_detection",
    "timm/resnet101.a1h_in1k",
    "google/vit-large-patch16-224",
    "timm/vit_large_patch16_224.augreg_in21k_ft_in1k",
    "timm/deit_base_distilled_patch16_224.fb_in1k",
    "microsoft/swin-base-patch4-window7-224",
    "timm/inception_resnet_v2.tf_in1k",
    "dima806/man_woman_face_image_detection",
    "LukeJacob2023/nsfw-image-detector",
    "timm/maxvit_tiny_tf_512.in1k",
    "timm/efficientnet_b4.ra2_in1k",
    "ISxOdin/vit-base-oxford-iiit-pets",
    "timm/maxvit_nano_rw_256.sw_in1k",
    "timm/convnext_tiny.in12k",
    "timm/vit_base_patch32_224.augreg_in21k",
    "optimum-intel-internal-testing/tiny-random-vit",
    "timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k",
    "abhilash88/age-gender-prediction",
    "timm/vgg16.tv_in1k",
    "timm/tf_efficientnetv2_xl.in21k",
    "timm/swin_base_patch4_window12_384.ms_in22k",
    "timm/pit_b_224.in1k",
    "timm/coat_small.in1k",
    "timm/regnety_120.sw_in12k_ft_in1k",
    "timm/mobilevit_s.cvnets_in1k",
    "timm/caformer_s36.sail_in1k",
    "timm/efficientnet_b2.ra_in1k",
    "timm/convnextv2_tiny.fcmae_ft_in22k_in1k",
    "smp-hub/resnet50.imagenet",
    "timm/vit_base_r50_s16_384.orig_in21k_ft_in1k",
    "cafeai/cafe_aesthetic",
    "timm/test_resnet.r160_in1k",
    "timm/repvgg_a2.rvgg_in1k",
    "timm/ghostnet_100.in1k",
    "timm/efficientnet_b5.sw_in12k_ft_in1k",
    "facebook/convnextv2-base-22k-224",
    "timm/dm_nfnet_f0.dm_in1k",
    "facebook/deit-tiny-patch16-224",
    "timm/efficientnet_b1.ra4_e3600_r240_in1k",
    "timm/resnet50d.ra2_in1k",
    "apple/mobilevit-x-small",
    "prithivMLmods/Deep-Fake-Detector-v2-Model",
    "timm/levit_256.fb_dist_in1k",
    "facebook/deit-base-patch16-224",
    "timm/visformer_small.in1k",
    "giacomoarienti/nsfw-classifier",
    "timm/beit_base_patch16_224.in22k_ft_in22k_in1k",
    "facebook/convnext-large-224-22k-1k",
    "timm/nfnet_l0.ra2_in1k",
    "dima806/fairface_gender_image_detection",
    "timm/maxvit_large_tf_224.in21k",
    "Marqo/nsfw-image-detection-384",
    "apple/mobilevit-xx-small",
    "timm/convnext_pico.d1_in1k",
    "timm/convnextv2_base.fcmae_ft_in22k_in1k",
    "timm/vit_small_patch16_384.augreg_in21k_ft_in1k",
    "timm/deit_base_patch16_224.fb_in1k",
    "timm/lcnet_050.ra2_in1k",
    "timm/efficientnetv2_rw_m.agc_in1k",
    "timm/cait_m48_448.fb_dist_in1k",
    "timm/vit_small_patch16_224.augreg_in21k",
    "google/efficientnet-b0",
    "facebook/convnextv2-tiny-22k-384",
    "timm/tf_efficientnet_b3.ns_jft_in1k",
    "timm/tf_efficientnetv2_b0.in1k",
    "facebook/dinov2-small-imagenet1k-1-layer",
    "smp-hub/resnet34.imagenet",
    "fxmarty/resnet-tiny-mnist",
    "WinKawaks/vit-tiny-patch16-224",
    "timm/convnext_atto.d2_in1k",
    "helper2424/resnet10",
    "timm/resnetv2_50x1_bit.goog_in21k_ft_in1k",
    "timm/vit_base_patch16_384.augreg_in21k_ft_in1k",
    "Tanneru/Facial-Emotion-Detection-FER-RAFDB-AffectNet-BEIT-Large",
    "timm/vit_base_patch32_224.augreg_in21k_ft_in1k",
    "timm/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384",
    "timm/fastvit_t8.apple_dist_in1k",
    "timm/mobilenetv4_conv_small.e2400_r224_in1k",
    "iitolstykh/mivolo_v2",
    "timm/tf_efficientnet_b5.ns_jft_in1k",
    "microsoft/resnet-152",
    "timm/vit_tiny_patch16_384.augreg_in21k_ft_in1k",
    "microsoft/swinv2-tiny-patch4-window8-256",
    "timm/hrnet_w18.ms_aug_in1k",
    "smp-hub/mit_b3.imagenet",
    "timm/convnextv2_tiny.fcmae_ft_in22k_in1k_384",
    "timm/resnet50.am_in1k",
    "timm/resnet18.tv_in1k",
            "timm/tiny_vit_21m_512.dist_in22k_ft_in1k",
        ]
    )
