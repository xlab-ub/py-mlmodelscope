# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import open_clip
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_OpenCLIP_ConvNext_Large_D_320(PyTorchAbstractClass):
    def __init__(self, config=None):
        model_id = "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"
        model_name = 'convnext_large_d_320'
        
        # create_model_and_transforms will load the model and the corresponding preprocessing
        self.model, _, self.image_preprocessor = open_clip.create_model_and_transforms(
            model_name, 
            pretrained=f'hf-hub:{model_id}'
        )
        self.tokenizer = open_clip.get_tokenizer(model_name)
        self.model.eval()

        # Download ImageNet class labels for zero-shot classification
        features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
        self.features = self.features_download(features_file_url)
        
        # Pre-compute text features for the classes using a standard prompt template
        text_descriptions = [f"a photo of a {label}" for label in self.features]
        text_tokens = self.tokenizer(text_descriptions)
        
        with torch.no_grad():
            self.text_features = self.model.encode_text(text_tokens)
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)

    def preprocess(self, input_images):
        processed_images = []
        for image_path in input_images:
            with Image.open(image_path).convert("RGB") as img:
                processed_img = self.image_preprocessor(img)
                processed_images.append(processed_img)
        
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        with torch.no_grad():
            image_features = self.model.encode_image(model_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        return image_features

    def postprocess(self, model_output):
        # model_output contains the normalized image features from predict()
        # self.text_features contains the pre-computed, normalized text features
        
        # Calculate cosine similarity to get logits
        logits_per_image = model_output @ self.text_features.T
        
        # Apply softmax to convert logits to probabilities
        probabilities = torch.nn.functional.softmax(logits_per_image, dim=-1)
        
        return probabilities.tolist()
