# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import open_clip
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_OpenCLIP_ConvNext_Large_D_320(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        self.device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Note: open_clip doesn't have built-in multi-gpu like device_map

        # The Hugging Face model identifier 'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup' 
        # corresponds to the following in the open_clip library:
        model_name = 'convnext_large_d_320'
        pretrained = 'laion2b_s29b_b131k_ft_soup'

        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            model_name,
            pretrained=pretrained
        )
        self.model.to(self.device)
        self.model.eval()

        # For zero-shot classification, labels must be provided.
        # They can be passed in the config dict, e.g., config={'labels': ['a photo of a cat', 'a photo of a dog']}
        # As a fallback, we use ImageNet classes.
        self.labels = self.config.get('labels')
        if not self.labels:
            features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
            self.labels = self.features_download(features_file_url)

        # Pre-tokenize and encode text labels for efficiency
        text_tokens = open_clip.tokenize(self.labels).to(self.device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            self.text_features = self.model.encode_text(text_tokens)
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)

    def preprocess(self, input_images):
        processed_images = [
            self.preprocess(Image.open(image_path).convert('RGB'))
            for image_path in input_images
        ]
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        model_input = model_input.to(self.device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            image_features = self.model.encode_image(model_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            # Calculate cosine similarity between image and pre-computed text features
            # The 100.0 is a learned temperature parameter from CLIP
            similarity = (100.0 * image_features @ self.text_features.T)
        return similarity

    def postprocess(self, model_output):
        # The output of predict is the similarity scores (logits)
        probabilities = model_output.softmax(dim=-1)
        return probabilities.cpu().tolist()
