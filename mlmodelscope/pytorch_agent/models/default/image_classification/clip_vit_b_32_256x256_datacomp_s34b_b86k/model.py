# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import open_clip

class PyTorch_OpenCLIP_ViT_B_32_256x256(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)  # Note: multi_gpu is not explicitly handled by open_clip in a simple way like device_map

        model_id = 'hf-hub:laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K'
        
        self.model, _, self.preprocess_fn = open_clip.create_model_and_transforms(model_id)
        self.tokenizer = open_clip.get_tokenizer(model_id)
        
        self.model.to(device)
        self.model.eval()

    def preprocess(self, input_images):
        images = [
            self.preprocess_fn(Image.open(image_path).convert("RGB"))
            for image_path in input_images
        ]
        model_input = torch.stack(images)
        return model_input

    def predict(self, model_input):
        candidate_labels = kwargs.get('candidate_labels')
        if not candidate_labels or not isinstance(candidate_labels, list):
            raise ValueError("A list of 'candidate_labels' must be provided in kwargs for zero-shot classification.")

        device = next(self.model.parameters()).device
        image_tensor = model_input.to(device)
        text_tokens = self.tokenizer(candidate_labels).to(device)

        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):
            image_features = self.model.encode_image(image_tensor)
            text_features = self.model.encode_text(text_tokens)
            
            image_features /= image_features.norm(dim=-1, keepdim=True)
            text_features /= text_features.norm(dim=-1, keepdim=True)

            logits_per_image = 100.0 * image_features @ text_features.T
        
        return logits_per_image

    def postprocess(self, model_output):
        probabilities = torch.nn.functional.softmax(model_output, dim=1)
        return probabilities.tolist()
