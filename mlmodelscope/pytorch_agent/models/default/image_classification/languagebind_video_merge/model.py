# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
from languagebind import LanguageBindImage, LanguageBindImageTokenizer, LanguageBindImageProcessor
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_LanguageBind_Image_ZeroShot(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)  # Not used for loading, but kept for consistency

        # The model card indicates using 'LanguageBind/LanguageBind_Image' for image tasks
        model_id = "LanguageBind/LanguageBind_Image"

        self.model = LanguageBindImage.from_pretrained(model_id, cache_dir=self.config.get("cache_dir"))
        self.tokenizer = LanguageBindImageTokenizer.from_pretrained(model_id, cache_dir=self.config.get("cache_dir"))
        self.processor = LanguageBindImageProcessor(self.model.config, self.tokenizer)

        self.model.to(device)
        self.model.eval()
        self.device = device

    def preprocess(self, input_images):
        if not context or "labels" not in context:
            raise ValueError("Zero-shot classification requires a 'labels' key in the context dictionary.")

        labels = context["labels"]
        pil_images = [Image.open(image_path).convert("RGB") for image_path in input_images]

        # The processor handles both images and text labels for zero-shot classification
        model_input = self.processor(pil_images, labels, return_tensors="pt")

        # Move tensors to the correct device
        model_input = {k: v.to(self.device) for k, v in model_input.items()}

        return model_input

    def predict(self, model_input):
        with torch.no_grad():
            model_output = self.model(**model_input)
        return model_output

    def postprocess(self, model_output):
        # The model output contains image_embeds and text_embeds
        # image_embeds: [batch_size, embed_dim]
        # text_embeds: [num_labels, embed_dim]

        # Normalize embeddings to compute cosine similarity
        image_embeds = model_output.image_embeds / model_output.image_embeds.norm(dim=-1, keepdim=True)
        text_embeds = model_output.text_embeds / model_output.text_embeds.norm(dim=-1, keepdim=True)

        # Calculate the cosine similarity (dot product of normalized embeddings)
        # The result is logits of shape [batch_size, num_labels]
        logits_per_image = image_embeds @ text_embeds.T

        # Apply softmax to get probabilities
        probabilities = torch.nn.functional.softmax(logits_per_image, dim=1)

        return probabilities.cpu().tolist()
