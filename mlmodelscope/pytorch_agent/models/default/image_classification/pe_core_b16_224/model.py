# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

# This model requires installing a custom library first:
# git clone https://github.com/facebookresearch/perception_models.git
# cd perception_models
# pip install -e .
try:
    import core.vision_encoder.pe as pe
    import core.vision_encoder.transforms as custom_transforms
except ImportError:
    raise ImportError("The 'perception_models' library is not installed. Please clone it from https://github.com/facebookresearch/perception_models and install it using 'pip install -e .' before using this model.")

class PyTorch_Custom_PE_Core_B16_224(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "PE-Core-B16-224"
        self.model = pe.CLIP.from_config(model_id, pretrained=True)
        self.model.to(device)
        self.model.eval()

        self.preprocessor = custom_transforms.get_image_transform(self.model.image_size)

        # This is a zero-shot model. We'll prepare text features for ImageNet classes
        # to fit it into a standard classification workflow.
        features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
        self.features = self.features_download(features_file_url)
        
        tokenizer = custom_transforms.get_text_tokenizer(self.model.context_length)
        text_descriptions = [f"a photo of a {label}" for label in self.features]
        text_tokens = tokenizer(text_descriptions).to(device)

        # Pre-compute text features for all classes
        with torch.no_grad():
            # The model forward pass can take text-only input
            _, self.text_features, _ = self.model(text=text_tokens)
        
        self.device = device

    def preprocess(self, input_images):
        processed_images = []
        for image_path in input_images:
            image = Image.open(image_path).convert('RGB')
            processed_image = self.preprocessor(image)
            processed_images.append(processed_image)
        
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        model_input = model_input.to(self.device)
        with torch.no_grad():
            # Get image features and the logit scale from the model
            image_features, _, logit_scale = self.model(image=model_input)
            
            # Compute similarity between image features and pre-computed text features
            # This gives us the logits for each class
            logits = logit_scale * image_features @ self.text_features.T
            
        return logits

    def postprocess(self, model_output):
        probabilities = torch.nn.functional.softmax(model_output, dim=1)
        return probabilities.tolist()
