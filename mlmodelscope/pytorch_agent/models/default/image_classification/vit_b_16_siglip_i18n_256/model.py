# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import timm
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Timm_ViT_B_16_SigLIP_256(PyTorchAbstractClass):
    def __init__(self):
        model_id = 'hf-hub:timm/ViT-B-16-SigLIP-i18n-256'
        # This model is loaded as a feature extractor (num_classes=0) as shown in the timm usage example.
        self.model = timm.create_model(model_id, pretrained=True, num_classes=0)
        self.model.eval()
        
        # Get model-specific transforms
        data_config = timm.data.resolve_model_data_config(self.model)
        self.preprocessor = timm.data.create_transform(**data_config, is_training=False)

    def preprocess(self, input_images):
        processed_images = [
            self.preprocessor(Image.open(image_path).convert('RGB'))
            for image_path in input_images
        ]
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        return self.model(model_input)

    def postprocess(self, model_output):
        # The model outputs feature embeddings, not classification logits.
        # Applying softmax is not applicable. Returning the raw features as a list.
        return model_output.tolist()
