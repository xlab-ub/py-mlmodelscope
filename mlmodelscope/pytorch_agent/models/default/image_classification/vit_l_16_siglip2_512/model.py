# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
try:
    from open_clip import create_model_from_pretrained, get_tokenizer
except ImportError:
    raise ImportError("Please install open_clip_torch and timm: pip install open_clip_torch 'timm>=1.0.15'")

class Timm_ViT_L_16_SigLIP2_512(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)  # Note: multi_gpu not directly supported by open_clip loader

        model_id = 'hf-hub:timm/ViT-L-16-SigLIP2-512'
        self.model, self.preprocessor = create_model_from_pretrained(model_id)
        self.model.to(device)
        self.model.eval()

        self.tokenizer = get_tokenizer(model_id)

        # This is a zero-shot model, requiring text labels for classification.
        # We use ImageNet labels by default if no custom labels are provided.
        self.labels = self.config.get("labels", None)
        if self.labels is None:
            features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
            self.labels = self.features_download(features_file_url)

        # Pre-tokenize and encode the text labels to create classification heads
        with torch.no_grad():
            text_tokens = self.tokenizer(self.labels, context_length=self.model.context_length).to(device)
            self.text_features = self.model.encode_text(text_tokens, normalize=True)

    def preprocess(self, input_images):
        processed_images = [
            self.preprocessor(Image.open(image_path).convert('RGB'))
            for image_path in input_images
        ]
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        # The model is moved to the target device in __init__
        model_input = model_input.to(self.model.device)
        with torch.no_grad():
            # This model encodes the image into a feature vector
            image_features = self.model.encode_image(model_input, normalize=True)
        return image_features

    def postprocess(self, model_output):
        # model_output contains the image features from the predict step
        # self.text_features are the pre-computed label embeddings from the __init__ step
        with torch.no_grad():
            # Calculate similarity and apply the SigLIP logic (sigmoid activation)
            # to get probabilities for each class label.
            probabilities = torch.sigmoid(model_output @ self.text_features.T * self.model.logit_scale.exp() + self.model.logit_bias)
        return probabilities.tolist()
