# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import os
import json
try:
    from open_clip import create_model_from_pretrained, get_tokenizer
except ImportError:
    raise ImportError("Please install open_clip_torch: pip install open_clip_torch")

class PyTorch_OpenClip_ViT_SO400M_14_SigLIP2_378(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Note: multi_gpu is not directly supported by this custom loader

        model_id = 'hf-hub:timm/ViT-SO400M-14-SigLIP2-378'
        self.model, self.preprocessor = create_model_from_pretrained(model_id)
        self.tokenizer = get_tokenizer(model_id)

        self.model.to(device)
        self.model.eval()

        # Zero-shot classification requires a set of labels.
        # We'll load them from a file specified in config or default to ImageNet classes.
        labels_path = self.config.get("labels_path")
        if labels_path and os.path.exists(labels_path):
            with open(labels_path, 'r') as f:
                if labels_path.endswith('.json'):
                    self.labels = json.load(f)
                else: # Assumes .txt file with one label per line
                    self.labels = [line.strip() for line in f.readlines()]
        else:
            # Default to ImageNet classes if no labels are provided
            features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
            self.labels = self.features_download(features_file_url)

        # Pre-tokenize and encode the text labels for efficiency
        with torch.no_grad(), torch.cuda.amp.autocast() if device == 'cuda' else torch.cpu.amp.autocast():
            tokenized_text = self.tokenizer(self.labels, context_length=self.model.context_length).to(device)
            self.text_features = self.model.encode_text(tokenized_text, normalize=True)

    def preprocess(self, input_images):
        processed_images = [
            self.preprocessor(Image.open(image_path).convert('RGB'))
            for image_path in input_images
        ]
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        with torch.no_grad(), torch.cuda.amp.autocast() if model_input.is_cuda else torch.cpu.amp.autocast():
            image_features = self.model.encode_image(model_input, normalize=True)
            # text_features are pre-computed in __init__ for efficiency
            # Calculate probabilities using the SigLIP formula from the model card
            text_probs = torch.sigmoid(image_features @ self.text_features.T * self.model.logit_scale.exp() + self.model.logit_bias)
        return text_probs

    def postprocess(self, model_output):
        # The model output is already a tensor of probabilities
        return model_output.tolist()
