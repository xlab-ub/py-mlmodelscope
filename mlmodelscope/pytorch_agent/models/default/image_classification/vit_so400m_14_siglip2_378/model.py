# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
import open_clip

class PyTorch_OpenClip_ViT_SO400M_14_SigLIP2_378(PyTorchAbstractClass):
    def __init__(self, config=None):
        super().__init__(config)
        model_id = 'hf-hub:timm/ViT-SO400M-14-SigLIP2-378'

        # Load model and preprocessing function from open_clip
        self.model, self.preprocess_transform = open_clip.create_model_from_pretrained(model_id)
        self.tokenizer = open_clip.get_tokenizer(model_id)
        self.model.eval()

        # Get labels from config for zero-shot classification
        self.labels = self.config.get("labels")
        if not self.labels:
            # Using default ImageNet classes as a fallback if no labels are provided
            features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
            imagenet_classes = self.features_download(features_file_url)
            self.labels = [f"a photo of a {label}" for label in imagenet_classes]

        # Pre-tokenize and encode text features for efficiency
        # This is crucial for zero-shot classification performance
        text_tokens = self.tokenizer(self.labels, context_length=self.model.context_length)
        with torch.no_grad():
            self.text_features = self.model.encode_text(text_tokens, normalize=True)

        # Move text features and model to the appropriate device
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            self.text_features = self.text_features.cuda()

    def preprocess(self, input_images):
        processed_images = [
            self.preprocess_transform(Image.open(image_path).convert('RGB'))
            for image_path in input_images
        ]
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        with torch.no_grad():
            # Move input to the same device as the model
            if torch.cuda.is_available():
                model_input = model_input.cuda()

            # Encode the input images
            image_features = self.model.encode_image(model_input, normalize=True)
            
            # Calculate sigmoid probabilities using pre-computed text features
            # This follows the SigLIP methodology from the model card
            text_probs = torch.sigmoid(image_features @ self.text_features.T * self.model.logit_scale.exp() + self.model.logit_bias)
            
        return text_probs

    def postprocess(self, model_output):
        # The output from predict is already a tensor of probabilities for each label
        return model_output.cpu().tolist()
