# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
from PIL import Image
from open_clip import create_model_from_pretrained, get_tokenizer

class PyTorch_OpenClip_ViT_SO400M_16_SigLIP2_384(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Note: open_clip loading doesn't have a simple multi-gpu map

        model_id = 'hf-hub:timm/ViT-SO400M-16-SigLIP2-384'

        self.model, self.preprocess_fn = create_model_from_pretrained(model_id)
        self.tokenizer = get_tokenizer(model_id)

        self.model.to(device)
        self.model.eval()

        # For zero-shot classification, labels must be provided in the config
        self.labels = self.config.get("labels")
        if not self.labels or not isinstance(self.labels, list):
            # Using ImageNet classes as a fallback if no labels are provided
            features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
            self.labels = self.features_download(features_file_url)

        # Pre-tokenize and encode text features for efficiency
        self.text_tokens = self.tokenizer(self.labels, context_length=self.model.context_length).to(device)
        with torch.no_grad():
            self.text_features = self.model.encode_text(self.text_tokens, normalize=True)

    def preprocess(self, input_images):
        processed_images = []
        for image_path in input_images:
            image = Image.open(image_path).convert('RGB')
            processed_image = self.preprocess_fn(image)
            processed_images.append(processed_image)
        
        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        model_input = model_input.to(self.model.device)
        with torch.no_grad():
            image_features = self.model.encode_image(model_input, normalize=True)
            # Calculate similarity and apply sigmoid as per SigLIP
            text_probs = torch.sigmoid(image_features @ self.text_features.T * self.model.logit_scale.exp() + self.model.logit_bias)
        return text_probs

    def postprocess(self, model_output):
        return model_output.cpu().tolist()
