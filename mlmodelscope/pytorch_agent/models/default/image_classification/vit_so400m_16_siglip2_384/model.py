# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
from open_clip import create_model_from_pretrained, get_tokenizer

class PyTorch_OpenClip_ViT_SO400M_16_SigLIP2_384(PyTorchAbstractClass):
    def __init__(self, config=None):
        super().__init__(config)
        model_id = 'hf-hub:timm/ViT-SO400M-16-SigLIP2-384'
        tokenizer_id = 'hf-hub:timm/ViT-SO400M-16-SigLIP2-384'

        self.model, self.preprocessor = create_model_from_pretrained(model_id)
        self.tokenizer = get_tokenizer(tokenizer_id)
        self.model.eval()

        # Download ImageNet classes to use as text labels for zero-shot classification
        features_file_url = "http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt"
        self.features = self.features_download(features_file_url)

        # Pre-tokenize and encode text features for efficiency
        with torch.no_grad():
            text_tokens = self.tokenizer(self.features, context_length=self.model.context_length)
            # Store text features on CPU initially, move to GPU in predict if needed
            self.text_features = self.model.encode_text(text_tokens, normalize=True).cpu()

    def preprocess(self, input_images):
        processed_images = []
        for image_path in input_images:
            image = Image.open(image_path).convert("RGB")
            processed_image = self.preprocessor(image)
            processed_images.append(processed_image)

        model_input = torch.stack(processed_images)
        return model_input

    def predict(self, model_input):
        with torch.no_grad():
            image_features = self.model.encode_image(model_input, normalize=True)
            
            # Ensure text features are on the same device as image features
            text_features_device = self.text_features.to(image_features.device)
            
            # Calculate logits using pre-computed text features
            # This follows the SigLIP formula: z_i @ z_t.T * exp(t) + b
            logits = image_features @ text_features_device.T * self.model.logit_scale.exp() + self.model.logit_bias
        return logits

    def postprocess(self, model_output):
        # SigLIP models use a sigmoid activation function, not softmax, as they are trained with a sigmoid loss.
        # This allows for multi-label classification predictions.
        probabilities = torch.sigmoid(model_output)
        return probabilities.tolist()
