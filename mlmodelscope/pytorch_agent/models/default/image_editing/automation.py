"""
Generalized automation script for image-to-image PyTorch models.

This script can generate wrapper classes for various image-to-image tasks:
- Image Editing (InstructPix2Pix, SD Img2Img, etc.)
- Mask Generation (SAM, SegFormer, Mask2Former, etc.)
- Image Enhancement (Super-resolution, denoising, etc.)
- Semantic/Instance Segmentation
- Style Transfer
- And more...

The script uses LLM (Google Gemini) to analyze model cards and generate
appropriate PyTorchAbstractClass implementations with preprocessing,
prediction, and postprocessing methods.

Usage:
    from automation import image_to_image_model_automation

    # For mask generation models
    image_to_image_model_automation(
        models_to_add=["facebook/sam-vit-base"],
        task_type="mask_generation"
    )

    # For image editing models
    image_to_image_model_automation(
        models_to_add=["timbrooks/instruct-pix2pix"],
        task_type="image_editing"
    )
"""

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def image_to_image_model_automation(models_to_add=None, task_type="image_editing"):
    """
    Generalized automation for image-to-image tasks.

    Args:
        models_to_add: List of model identifiers to process
        task_type: Type of task - "image_editing", "mask_generation", "image_enhancement", etc.
    """
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, {preprocess_input}):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
{optional_methods}
"""

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch image-to-image model."""

        model_type: str = Field(
            description="The type of model. Options: 'diffusers_pipeline' (SD img2img, etc.), 'diffusers_components' (InstructPix2Pix components), 'transformers_pipeline' (HF pipelines), 'transformers_model' (direct transformer models), 'sam' (Segment Anything), 'custom'.",
            default="transformers_model",
        )

        imports: str = Field(
            description="The complete imports section. Include all necessary imports: torch, PIL, transformers/diffusers classes, numpy if needed. Use proper formatting with newlines.",
            default="import torch\nfrom PIL import Image",
        )

        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'PyTorch_Transformers_SAM', 'PyTorch_Diffusers_InstructPix2Pix'.",
        )

        init_config: str = Field(
            description="The __init__ method signature parameters. Typically ', config=None'.",
            default=", config=None",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method. Should include: 1) config initialization with super().__init__(config), 2) loading processor/pipeline from_pretrained, 3) loading model using self.load_hf_model(ModelClass, model_id) for multi-GPU support (for transformers models), 4) setting any task-specific parameters.",
        )

        preprocess_input: str = Field(
            description="The parameter name(s) for preprocess method. Examples: 'input_images' for images only, 'input_images_and_prompts' for image editing with text, 'input_images_and_points' for SAM-like models.",
            default="input_images",
        )

        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Should handle the specific input format for this model type (images only, images+prompts, images+points, etc.) and prepare model inputs.",
        )

        predict_body: str = Field(
            description="The complete body of the predict method. Should call the model/pipeline appropriately and return raw outputs.",
        )

        postprocess_body: str = Field(
            description="The complete body of the postprocess method. Should convert model output to the expected format: for masks - binary/segmentation maps as lists, for images - uint8 numpy arrays as lists.",
        )

        optional_methods: str = Field(
            description="Optional methods like to(device) and eval() if needed. For diffusion models, include both. For simple transformers models, can be empty. Format as complete method definitions with proper indentation.",
            default="",
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch image-to-image models. Your task is to generate a complete, working model.py file for various tasks: image editing, mask generation, segmentation, image enhancement, etc.

You will be given a Hugging Face model identifier and the text content from its model card page.
You must generate the complete model configuration based *primarily* on the provided page context.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - Mask Generation/Segmentation (SAM, semantic segmentation): Use 'transformers_model' or 'transformers_pipeline'
   - Image Editing (InstructPix2Pix): Use 'diffusers_components' with VAE, UNet, etc.
   - Image-to-Image Diffusion: Use 'diffusers_pipeline' with StableDiffusionImg2ImgPipeline
   - Custom implementations: Use 'custom'

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For SAM: `from transformers import SamModel, SamProcessor` (or AutoModel)
   - For segmentation: `from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation`
   - For diffusers: `from diffusers import ...` (pipelines or components)
   - PIL Image: `from PIL import Image`
   - Numpy if needed: `import numpy as np`

3. **Preprocess Input Parameter:**
   - For mask generation (no prompts): `input_images`
   - For image editing with text: `input_images_and_prompts`
   - For SAM with points/boxes: `input_images` or custom format
   - Choose appropriate parameter name

4. **Init Method:**
   - Initialize config: `super().__init__(config)` (required for load_hf_model to work, for transformers models)
   - For transformers models: Load processor with `from_pretrained`, load model with `self.load_hf_model(ModelClass, model_id)` for multi-GPU support
   - For diffusers components: Load VAE, text_encoder, unet, scheduler, etc. with `from_pretrained` (diffusers doesn't use load_hf_model)
   - Set task-specific parameters

5. **Preprocess Method:**
   - Handle appropriate input format
   - Load images with PIL
   - Use processor/image_processor to prepare inputs
   - Return processed inputs ready for model

6. **Predict Method:**
   - Call model appropriately
   - For transformers: `self.model(**model_input)` or `self.model(pixel_values=...)`
   - For diffusers components: implement inference loop
   - For pipelines: `self.pipeline(**model_input)`

7. **Postprocess Method:**
   - For masks/segmentation: Extract masks, resize if needed, convert to binary/labels, return as lists
   - For images: Decode, normalize to 0-255, convert to uint8, return as lists
   - Handle different output formats appropriately

8. **Optional Methods:**
   - For diffusers components: Include to(device) and eval() methods
   - For simple transformers models: Can leave empty
   - Format as complete method definitions with proper indentation

**Reference Examples:**

Example 1: Mask Generation / Semantic Segmentation (transformers_model)
{{{{
    "model_type": "transformers_model",
    "imports": "import torch\\nfrom transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\\nfrom PIL import Image\\nimport numpy as np",
    "class_name": "PyTorch_Transformers_SegFormer",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"nvidia/segformer-b0-finetuned-ade-512-512\\"\\n        self.processor = AutoImageProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForSemanticSegmentation, model_id)",
    "preprocess_input": "input_images",
    "preprocess_body": "images = [Image.open(img_path).convert('RGB') for img_path in input_images]\\n        return self.processor(images, return_tensors=\\"pt\\")",
    "predict_body": "return self.model(**model_input)",
    "postprocess_body": "logits = model_output.logits\\n        # Resize to original size if needed\\n        masks = torch.nn.functional.interpolate(\\n            logits,\\n            size=(512, 512),\\n            mode=\\"bilinear\\",\\n            align_corners=False\\n        )\\n        predicted_masks = masks.argmax(dim=1)\\n        return [mask.cpu().numpy().astype(\\"uint8\\").tolist() for mask in predicted_masks]",
    "optional_methods": ""
}}}}

Example 2: SAM (Segment Anything Model)
{{{{
    "model_type": "sam",
    "imports": "import torch\\nfrom transformers import SamModel, SamProcessor\\nfrom PIL import Image\\nimport numpy as np",
    "class_name": "PyTorch_Transformers_SAM",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"facebook/sam-vit-base\\"\\n        self.processor = SamProcessor.from_pretrained(model_id)\\n        self.model = self.load_hf_model(SamModel, model_id)",
    "preprocess_input": "input_images",
    "preprocess_body": "# For automatic mask generation\\n        images = [Image.open(img_path).convert('RGB') for img_path in input_images]\\n        return self.processor(images, return_tensors=\\"pt\\")",
    "predict_body": "return self.model(**model_input)",
    "postprocess_body": "masks = model_output.pred_masks.squeeze(1)\\n        masks = (masks > 0.5).float()\\n        return [mask.cpu().numpy().astype(\\"uint8\\").tolist() for mask in masks]",
    "optional_methods": ""
}}}}

Example 3: InstructPix2Pix (diffusers_components - complex)
{{{{
    "model_type": "diffusers_components",
    "imports": "import torch\\nfrom transformers import CLIPTextModel, CLIPTokenizer\\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, EulerAncestralDiscreteScheduler\\nfrom diffusers.image_processor import VaeImageProcessor\\nfrom PIL import Image",
    "class_name": "PyTorch_Diffusers_InstructPix2Pix",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"timbrooks/instruct-pix2pix\\"\\n        self.vae = AutoencoderKL.from_pretrained(model_id, subfolder=\\"vae\\")\\n        self.text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\\"text_encoder\\")\\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\\"tokenizer\\")\\n        self.unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\\"unet\\")\\n        self.scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\\"scheduler\\")\\n        self.image_processor = VaeImageProcessor(vae_scale_factor=8)\\n        self.seed = self.config.get('seed', 0)",
    "preprocess_input": "input_images_and_prompts",
    "preprocess_body": "# Process images and prompts for pix2pix\\n        # Complex implementation - see existing model",
    "predict_body": "# Run diffusion loop - see existing model",
    "postprocess_body": "images = self.vae.decode(model_output).sample\\n        images = ((images/2)+0.5).clamp(0, 1)\\n        images = (images*255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\\n        return [image.tolist() for image in images]",
    "optional_methods": "\\n    def to(self, device):\\n        self.device = device\\n        self.vae.to(device)\\n        self.text_encoder.to(device)\\n        self.unet.to(device)\\n        self.generator = torch.Generator(device=device).manual_seed(self.seed)\\n\\n    def eval(self):\\n        self.vae.eval()\\n        self.text_encoder.eval()\\n        self.unet.eval()"
}}}}

Example 4: Image-to-Image Pipeline (diffusers_pipeline)
{{{{
    "model_type": "diffusers_pipeline",
    "imports": "import torch\\nfrom diffusers import StableDiffusionImg2ImgPipeline\\nfrom PIL import Image",
    "class_name": "PyTorch_Diffusers_SD_Img2Img",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"runwayml/stable-diffusion-v1-5\\"\\n        self.pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(model_id)",
    "preprocess_input": "input_images_and_prompts",
    "preprocess_body": "images = []\\n        prompts = []\\n        for img_path, prompt in input_images_and_prompts:\\n            images.append(Image.open(img_path).convert('RGB'))\\n            prompts.append(prompt)\\n        return {{\\"images\\": images, \\"prompt\\": prompts}}",
    "predict_body": "return self.pipeline(**model_input).images",
    "postprocess_body": "# Pipeline returns PIL images\\n        import numpy as np\\n        return [np.array(img).tolist() for img in model_output]",
    "optional_methods": "\\n    def to(self, device):\\n        self.device = device\\n        self.pipeline.to(device)"
}}}}

**CRITICAL NOTES:**
- Detect the task type from model card (mask generation, segmentation, image editing, etc.)
- Choose appropriate model_type
- Set correct preprocess_input parameter name
- Handle outputs correctly (masks vs images)
- Include optional_methods only if needed (diffusers models typically need them)
- Use the exact model identifier provided in your code
- Generate complete, working code

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Context from Hugging Face page:
---
{model_page_context}
---
Generate config for model: '{model_identifier}'

**IMPORTANT:**
- Use the exact model identifier '{model_identifier}' in the init_body when loading the model
- Detect the task type from context (mask generation, segmentation, image editing, etc.)
- Choose appropriate model_type, preprocess_input, and output format
- Generate complete, working code for all methods
- Include optional_methods only if needed (e.g., for diffusers components)
""",
            ),
        ]
    )

    modelCounter = 0
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser

    BASE_DIR = f"mlmodelscope/pytorch_agent/models/default/{task_type}"
    ERROR_DIR = f"{BASE_DIR}/errors"
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models = []
    login_req_models = []

    for model_name in models_to_add:
        if modelCounter == 50:
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue

        error_log = ""
        try:
            check_syntax = lambda fn: os.system(f"python -m py_compile {fn}")
            error = False
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if error:
                    error_log += f"Syntax error, regenerating...\n"

                url = f"https://huggingface.co/{model_name}"
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    login_req_models.append(model_name)
                    break

                context_text = main_content.get_text(separator=" ", strip=True)
                if len(context_text) > 20000:
                    context_text = context_text[:20000] + "\n... (truncated)"

                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )

                init_body = (
                    model_config_dict.get("init_body", "")
                    .replace("{hugging_face_model_id}", model_name)
                    .replace("MODEL_IDENTIFIER_PLACEHOLDER", model_name)
                )

                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "import torch\nfrom PIL import Image",
                    ),
                    class_name=model_config_dict.get(
                        "class_name",
                        f"PyTorch_{task_type.replace('_', ' ').title().replace(' ', '_')}_Model",
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_input=model_config_dict.get(
                        "preprocess_input", "input_images"
                    ),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "pass"
                    ).lstrip(" "),
                    predict_body=model_config_dict.get(
                        "predict_body", "return self.model(**model_input)"
                    ).lstrip(" "),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body", "return model_output"
                    ).lstrip(" "),
                    optional_methods=model_config_dict.get("optional_methods", ""),
                )

                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

            else:
                print(f"Successfully generated {model_py_path}")
                modelCounter += 1

        except Exception as e:
            print(f"Failed: {e}")
            import traceback

            traceback.print_exc()
            failed_models.append(model_name)

    print(f"\n--- Automation complete ---")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for fm in failed_models:
            f.write(f"{fm}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for lm in login_req_models:
            f.write(f"{lm}\n")


# Backward compatibility wrapper
def image_editing_model_automation(models_to_add=None):
    """Legacy function name for image editing. Use image_to_image_model_automation instead."""
    return image_to_image_model_automation(
        models_to_add=models_to_add, task_type="image_editing"
    )


if __name__ == "__main__":
    # Example usage for different task types:

    # Image editing models
    # image_to_image_model_automation(
    #     models_to_add=["timbrooks/instruct-pix2pix"],
    #     task_type="image_editing"
    # )

    # Mask generation / segmentation models
    image_to_image_model_automation(
        models_to_add=[
            "facebook/sam-vit-base",
            "nvidia/segformer-b0-finetuned-ade-512-512",
            "facebook/mask2former-swin-large-coco-panoptic",
        ],
        task_type="mask_generation",
    )

    # Image enhancement / restoration models
    # image_to_image_model_automation(
    #     models_to_add=["microsoft/swin2sr-classical-sr-x2-64"],
    #     task_type="image_enhancement"
    # )
