# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from diffusers import AutoencoderKLQwenImage
from PIL import Image
import numpy as np

class PyTorch_Diffusers_VAE_Reconstruction(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "REPA-E/e2e-qwenimage-vae"
        dtype = torch.float16 if device == "cuda" else torch.float32

        if multi_gpu and device == "cuda":
            self.model = AutoencoderKLQwenImage.from_pretrained(model_id, device_map="auto", torch_dtype=dtype)
        else:
            self.model = AutoencoderKLQwenImage.from_pretrained(model_id, torch_dtype=dtype)

    def preprocess(self, input_images):
        images = []
        for img_path in input_images:
            image = Image.open(img_path).convert("RGB")
            image_np = np.array(image)
            # Convert to (B, C, H, W), normalize to [-1, 1]
            image_torch = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
            # QwenImage VAE expects an additional dimension for `num_frames` -> (B, C, F, H, W)
            image_torch = image_torch.unsqueeze(2)
            images.append(image_torch)
        return torch.cat(images, dim=0)

    def predict(self, model_input):
        with torch.no_grad():
            latents = self.model.encode(model_input).latent_dist.sample()
            reconstructed = self.model.decode(latents).sample
        return reconstructed

    def postprocess(self, model_output):
        # Squeeze the extra frame dimension from (B, C, F, H, W) to (B, C, H, W)
        reconstructed = model_output.squeeze(2)
        
        # Denormalize from [-1, 1] to [0, 255]
        reconstructed = ((reconstructed / 2) + 0.5).clamp(0, 1)
        reconstructed = (reconstructed * 255).round().to(torch.uint8)
        
        # Permute to (B, H, W, C) for standard image format and convert to list
        reconstructed_list = reconstructed.permute(0, 2, 3, 1).cpu().numpy()
        return [img.tolist() for img in reconstructed_list]

    def to(self, device):
        self.device = device
        self.model.to(device)

    def eval(self):
        self.model.eval()
