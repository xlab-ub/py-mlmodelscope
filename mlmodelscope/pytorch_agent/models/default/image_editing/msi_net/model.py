# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
import numpy as np
from PIL import Image
from huggingface_hub import snapshot_download
import os
import torchvision.transforms.functional as F
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Custom_MSINet(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        self.device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "alexanderkroner/MSI-Net"
        # NOTE: The original model at alexanderkroner/MSI-Net is a TensorFlow/Keras model.
        # This implementation provides a PyTorch-based wrapper for the pre-processing and post-processing logic
        # described in the model card. A placeholder PyTorch model is used below as the actual Keras model
        # cannot be loaded directly in a PyTorch environment.
        try:
            self.hf_dir = snapshot_download(repo_id=model_id)
        except Exception as e:
            print(f"Could not download model files: {e}")
            self.hf_dir = "."

        # Placeholder model definition
        class PlaceholderMSINet(torch.nn.Module):
            def __init__(self):
                super().__init__()
                # This is a dummy layer to simulate the image-to-image process
                self.conv = torch.nn.Conv2d(3, 1, kernel_size=1)

            def forward(self, x):
                # The original model produces a single-channel saliency map
                return torch.sigmoid(self.conv(x))

        if multi_gpu and self.device == "cuda":
            # This is a placeholder; actual multi-GPU for a custom model would require
            # more complex setup like torch.nn.DataParallel or DistributedDataParallel.
            self.model = PlaceholderMSINet()
            print("Warning: Multi-GPU is not fully implemented for this custom placeholder model.")
        else:
            self.model = PlaceholderMSINet()
        
        self.model.to(self.device)
        self.model.eval()

    def preprocess(self, input_images):
        processed_data = []
        for img_path in input_images:
            image = Image.open(img_path).convert("RGB")
            original_shape = (image.height, image.width)
            target_shape = self._get_target_shape(original_shape)

            # Convert to tensor
            img_tensor = F.to_tensor(image)

            # Resize while preserving aspect ratio
            aspect_ratio = original_shape[0] / original_shape[1]
            target_aspect_ratio = target_shape[0] / target_shape[1]
            if aspect_ratio > target_aspect_ratio:
                new_height = target_shape[0]
                new_width = int(new_height / aspect_ratio)
            else:
                new_width = target_shape[1]
                new_height = int(new_width * aspect_ratio)
            
            resized_tensor = F.resize(img_tensor.unsqueeze(0), [new_height, new_width], antialias=True).squeeze(0)

            # Calculate padding
            vertical_padding = target_shape[0] - new_height
            horizontal_padding = target_shape[1] - new_width
            v_pad_1 = vertical_padding // 2
            v_pad_2 = vertical_padding - v_pad_1
            h_pad_1 = horizontal_padding // 2
            h_pad_2 = horizontal_padding - h_pad_1
            padding_dims = [h_pad_1, v_pad_1, h_pad_2, v_pad_2]

            # Pad the tensor
            padded_tensor = F.pad(resized_tensor, padding=padding_dims)

            processed_data.append({
                "padded_tensor": padded_tensor,
                "original_shape": original_shape,
                "padding_info": {"v": (v_pad_1, v_pad_2), "h": (h_pad_1, h_pad_2)}
            })

        # Batch the tensors
        batched_tensors = torch.stack([item["padded_tensor"] for item in processed_data])
        
        return {
            "batched_tensors": batched_tensors,
            "metadata": [{
                "original_shape": item["original_shape"],
                "padding_info": item["padding_info"]
            } for item in processed_data]
        }

    def predict(self, model_input):
        input_tensors = model_input["batched_tensors"].to(self.device)
        with torch.no_grad():
            output_tensors = self.model(input_tensors)
        
        return {
            "output_tensors": output_tensors.cpu(),
            "metadata": model_input["metadata"]
        }

    def postprocess(self, model_output):
        results = []
        output_tensors = model_output["output_tensors"]
        metadata = model_output["metadata"]

        for i in range(len(output_tensors)):
            tensor = output_tensors[i]
            meta = metadata[i]
            original_shape = meta["original_shape"]
            padding_info = meta["padding_info"]

            # Un-pad the output tensor
            v_pad_1, _ = padding_info["v"]
            h_pad_1, _ = padding_info["h"]
            
            unpadded_height = tensor.shape[1] - (padding_info["v"][0] + padding_info["v"][1])
            unpadded_width = tensor.shape[2] - (padding_info["h"][0] + padding_info["h"][1])

            unpadded_tensor = tensor[:, v_pad_1:v_pad_1 + unpadded_height, h_pad_1:h_pad_1 + unpadded_width]

            # Resize back to original shape
            resized_output = F.resize(unpadded_tensor.unsqueeze(0), list(original_shape), antialias=True).squeeze(0)

            # Convert to numpy array and then to list
            saliency_map = resized_output.squeeze().numpy()
            results.append(saliency_map.tolist())
        
        return results

    def _get_target_shape(self, original_shape):
        original_aspect_ratio = original_shape[0] / original_shape[1]

        square_mode = abs(original_aspect_ratio - 1.0)
        landscape_mode = abs(original_aspect_ratio - 240 / 320)
        portrait_mode = abs(original_aspect_ratio - 320 / 240)

        best_mode = min(square_mode, landscape_mode, portrait_mode)

        if best_mode == square_mode:
            target_shape = (320, 320)
        elif best_mode == landscape_mode:
            target_shape = (240, 320)
        else:
            target_shape = (320, 240)
        return target_shape

    def to(self, device):
        self.device = device
        self.model.to(device)

    def eval(self):
        self.model.eval()
