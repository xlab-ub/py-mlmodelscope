# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline
from PIL import Image
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_Qwen_ControlNet_Inpainting(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Not directly used for pipelines, device placement is handled in 'to' method

        base_model_id = "Qwen/Qwen-Image"
        controlnet_model_id = "InstantX/Qwen-Image-ControlNet-Inpainting"

        # Use bfloat16 as recommended in the model card
        self.dtype = torch.bfloat16

        controlnet = QwenImageControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=self.dtype)

        self.pipeline = QwenImageControlNetInpaintPipeline.from_pretrained(
            base_model_id, 
            controlnet=controlnet, 
            torch_dtype=self.dtype
        )

        self.seed = self.config.get('seed', 42)
        self.device = "cpu"

    def preprocess(self, input_images_masks_and_prompts):
        images = []
        masks = []
        prompts = []
        # input is a list of tuples: [(image_path, mask_path, prompt), ...]
        for img_path, mask_path, prompt in input_images_masks_and_prompts:
            images.append(Image.open(img_path).convert('RGB'))
            masks.append(Image.open(mask_path).convert('RGB'))
            prompts.append(prompt)
        
        return {"control_image": images, "control_mask": masks, "prompt": prompts}

    def predict(self, model_input):
        prompt = model_input.get("prompt")
        control_image = model_input.get("control_image")
        control_mask = model_input.get("control_mask")

        if not control_image:
            raise ValueError("control_image is a required argument.")

        # Extract parameters from config or use defaults from model card
        negative_prompt = self.config.get('negative_prompt', ' ')
        num_inference_steps = self.config.get('num_inference_steps', 30)
        controlnet_conditioning_scale = self.config.get('controlnet_conditioning_scale', 1.0)
        true_cfg_scale = self.config.get('true_cfg_scale', 4.0)

        # Use the size of the first control image for all generated images
        width, height = control_image[0].size

        generator = torch.Generator(device=self.device).manual_seed(self.seed)

        output = self.pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            control_image=control_image,
            control_mask=control_mask,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            controlnet_conditioning_scale=controlnet_conditioning_scale,
            true_cfg_scale=true_cfg_scale,
            generator=generator
        )
        return output

    def postprocess(self, model_output):
        # The pipeline output has an 'images' attribute containing a list of PIL Images
        images = model_output.images
        # Convert PIL images to lists
        return [np.array(img).tolist() for img in images]

    def to(self, device):
        self.device = device
        self.pipeline.to(device)

    def eval(self):
        # Diffusers pipelines manage the eval state of their components internally
        pass

