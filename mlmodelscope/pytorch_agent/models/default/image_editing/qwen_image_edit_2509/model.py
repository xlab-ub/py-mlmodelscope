# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from diffusers import QwenImageEditPlusPipeline
from PIL import Image
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_QwenImageEdit(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "Qwen/Qwen-Image-Edit-2509"
        
        # The model card recommends bfloat16 for performance
        self.pipeline = QwenImageEditPlusPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16
        )
        self.seed = self.config.get('seed', 0)

    def preprocess(self, input_images_and_prompts):
        # input_images_and_prompts is a list of tuples: [('/path/to/img1.png', 'prompt'), ('/path/to/img2.png', 'prompt'), ...]
        # The prompt is expected to be the same for all images in a single request.
        images = [Image.open(img_path).convert("RGB") for img_path, _ in input_images_and_prompts]
        prompt = input_images_and_prompts[0][1] # Get the prompt from the first tuple

        # Default parameters from the model card example, can be overridden in config
        params = {
            "true_cfg_scale": self.config.get("true_cfg_scale", 4.0),
            "negative_prompt": self.config.get("negative_prompt", " "),
            "num_inference_steps": self.config.get("num_inference_steps", 40),
            "guidance_scale": self.config.get("guidance_scale", 1.0),
            "num_images_per_prompt": self.config.get("num_images_per_prompt", 1),
        }

        # The generator is created in the `to` method to ensure it's on the correct device
        if hasattr(self, 'generator'):
            params["generator"] = self.generator
        else:
            # Fallback for direct predict calls without calling `to` first
            params["generator"] = torch.manual_seed(self.seed)

        return {"image": images, "prompt": prompt, **params}

    def predict(self, model_input):
        # The pipeline returns an object with an 'images' attribute, which is a list of PIL Images
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        # The model output is already a list of PIL Images
        # Convert them to numpy arrays and then to lists for JSON serialization
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        self.pipeline.to(device)
        # Create a generator for reproducibility and ensure it's on the correct device
        self.generator = torch.Generator(device=device).manual_seed(self.seed)

    def eval(self):
        # Diffusers pipelines manage their own state and don't have a separate .eval() mode
        pass

