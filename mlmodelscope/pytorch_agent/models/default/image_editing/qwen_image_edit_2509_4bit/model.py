# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from diffusers import QwenImageEditPlusPipeline
from PIL import Image
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_QwenImageEdit(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "ovedrive/Qwen-Image-Edit-2509-4bit"
        # This model is quantized and works well with bfloat16 as suggested in the model card
        self.pipeline = QwenImageEditPlusPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
        
        # Extract parameters from config or use defaults from model card
        self.seed = self.config.get('seed', 0)
        self.num_inference_steps = self.config.get('num_inference_steps', 20)
        self.true_cfg_scale = self.config.get('true_cfg_scale', 4.0)
        self.negative_prompt = self.config.get('negative_prompt', ' ')

    def preprocess(self, input_images_and_prompts):
        # This pipeline can take a single image or a list of images with a single prompt.
        # We will collect all images and use the prompt from the first entry.
        images = [Image.open(img_path).convert('RGB') for img_path, _ in input_images_and_prompts]
        prompt = input_images_and_prompts[0][1] if input_images_and_prompts else ""

        # If only one image is provided, we don't pass it as a list
        image_input = images[0] if len(images) == 1 else images

        return {
            "image": image_input,
            "prompt": prompt,
            "generator": torch.manual_seed(self.seed),
            "num_inference_steps": self.num_inference_steps,
            "true_cfg_scale": self.true_cfg_scale,
            "negative_prompt": self.negative_prompt
        }

    def predict(self, model_input):
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        # The pipeline directly returns a list of PIL images
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        # For GPUs with limited VRAM, the model card suggests using enable_model_cpu_offload()
        if device != 'cpu' and torch.cuda.get_device_properties(0).total_memory < 20 * 1024**3: # Less than 20GB VRAM
            print("Enabling model CPU offload for low VRAM environment.")
            self.pipeline.enable_model_cpu_offload()
        else:
            self.pipeline.to(device)

    def eval(self):
        # Pipelines manage their own evaluation state.
        # We can disable the progress bar for cleaner logs during inference.
        self.pipeline.set_progress_bar_config(disable=True)

