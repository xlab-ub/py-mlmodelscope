# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
from diffusers import QwenImageTransformer2DModel, GGUFQuantizationConfig, QwenImageEditPipeline
from huggingface_hub import hf_hub_download
from PIL import Image
import numpy as np

class PyTorch_Diffusers_QwenImageEdit(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # multi_gpu not directly used, but good practice

        gguf_repo_id = "calcuis/qwen-image-edit-plus-gguf"
        gguf_filename = "qwen-image-edit-plus-v2-iq4_nl.gguf"
        base_pipeline_id = "Qwen/Qwen-Image-Edit-2509"

        # Download the GGUF model file
        gguf_model_path = hf_hub_download(
            repo_id=gguf_repo_id,
            filename=gguf_filename
        )

        # Load the transformer from the GGUF file
        # Using float16 for broader compatibility
        transformer = QwenImageTransformer2DModel.from_single_file(
            gguf_model_path,
            quantization_config=GGUFQuantizationConfig(compute_dtype=torch.float16),
            torch_dtype=torch.float16
        )

        # Load the pipeline with the custom transformer
        self.pipeline = QwenImageEditPipeline.from_pretrained(
            base_pipeline_id,
            transformer=transformer,
            torch_dtype=torch.float16
        )

        self.seed = self.config.get('seed', 0)
        self.generator = torch.manual_seed(self.seed)

    def preprocess(self, input_images_and_prompts):
        images = []
        prompts = []
        for img_path, prompt in input_images_and_prompts:
            images.append(Image.open(img_path).convert("RGB"))
            prompts.append(prompt)

        # Get inference parameters from config or use defaults from the model card
        params = {
            "image": images,
            "prompt": prompts,
            "generator": self.generator,
            "true_cfg_scale": self.config.get("true_cfg_scale", 2.5),
            "negative_prompt": self.config.get("negative_prompt", " "),
            "num_inference_steps": self.config.get("num_inference_steps", 20)
        }
        return params

    def predict(self, model_input):
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        # The pipeline returns a list of PIL images
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        self.pipeline.to(device)
        self.generator = torch.Generator(device=device).manual_seed(self.seed)

    def eval(self):
        # Pipelines manage their own model modes, so this is often a no-op
        pass

