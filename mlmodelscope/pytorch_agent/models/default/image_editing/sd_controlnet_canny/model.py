# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import numpy as np
import cv2

class PyTorch_Diffusers_ControlNetCanny(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        # multi_gpu is not directly used by this pipeline setup but is good practice to extract
        multi_gpu = self.config.pop("_multi_gpu", False)

        controlnet_model_id = "lllyasviel/sd-controlnet-canny"
        sd_model_id = "runwayml/stable-diffusion-v1-5"

        # Use float16 for memory efficiency on CUDA devices
        torch_dtype = torch.float16 if device == "cuda" else torch.float32

        controlnet = ControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=torch_dtype)
        self.pipeline = StableDiffusionControlNetPipeline.from_pretrained(
            sd_model_id,
            controlnet=controlnet,
            torch_dtype=torch_dtype
        )
        self.pipeline.scheduler = UniPCMultistepScheduler.from_config(self.pipeline.scheduler.config)

        # Set default inference parameters from config or use sane defaults
        self.num_inference_steps = self.config.get('num_inference_steps', 20)
        self.canny_low_threshold = self.config.get('canny_low_threshold', 100)
        self.canny_high_threshold = self.config.get('canny_high_threshold', 200)

    def preprocess(self, input_images_and_prompts):
        images = []
        prompts = []
        for img_path, prompt in input_images_and_prompts:
            # Load the user-provided image
            input_image = Image.open(img_path).convert("RGB")
            np_image = np.array(input_image)

            # Apply Canny edge detection to create the control image
            canny_image = cv2.Canny(np_image, self.canny_low_threshold, self.canny_high_threshold)
            # Convert single-channel Canny output to a 3-channel image
            canny_image = canny_image[:, :, None]
            canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)
            control_image = Image.fromarray(canny_image)

            images.append(control_image)
            prompts.append(prompt)
        
        # The pipeline expects the control image under the 'image' key
        return {"image": images, "prompt": prompts, "num_inference_steps": self.num_inference_steps}

    def predict(self, model_input):
        # The pipeline returns an object with a .images attribute containing a list of PIL Images
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        # Convert the output PIL images to a list of numpy arrays (as lists)
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        if device == "cuda":
            # enable_model_cpu_offload is a memory-saving technique for GPUs with limited VRAM
            try:
                self.pipeline.enable_model_cpu_offload()
            except Exception as e:
                print(f"Could not enable model CPU offload. Using default .to(device). Error: {e}")
                self.pipeline.to(device)
            
            # xformers provides a memory-efficient attention implementation for faster inference
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
            except ImportError:
                print("xformers is not installed. For faster inference, install it with 'pip install xformers'.")
        else:
            self.pipeline.to(device)

    def eval(self):
        # The pipeline's `to` method or `enable_model_cpu_offload` handles setting components to eval mode.
        pass

