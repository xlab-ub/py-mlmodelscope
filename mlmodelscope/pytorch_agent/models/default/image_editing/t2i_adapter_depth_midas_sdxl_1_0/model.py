# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL
from controlnet_aux.midas import MidasDetector
from PIL import Image
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_T2IAdapter_SDXL_Depth(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        torch_dtype = torch.float16 if "cuda" in device else torch.float32

        # 1. Load the auxiliary model for preprocessing
        self.midas_depth = MidasDetector.from_pretrained(
            "valhalla/t2iadapter-aux-models",
            filename="dpt_large_384.pt",
            model_type="dpt_large"
        )

        # 2. Load the T2I Adapter
        adapter = T2IAdapter.from_pretrained(
            "TencentARC/t2i-adapter-depth-midas-sdxl-1.0",
            torch_dtype=torch_dtype,
            variant="fp16"
        )

        # 3. Load other components for the pipeline
        model_id = 'stabilityai/stable-diffusion-xl-base-1.0'
        scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder="scheduler")
        vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch_dtype)

        # 4. Initialize the main pipeline
        self.pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(
            model_id,
            vae=vae,
            adapter=adapter,
            scheduler=scheduler,
            torch_dtype=torch_dtype,
            variant="fp16",
        )

        if "cuda" in device:
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
            except ImportError:
                print("xformers is not installed. For memory efficient attention, please install xformers.")

        self.device = None

    def preprocess(self, input_images_and_prompts):
        control_images = []
        prompts = []
        for img_path, prompt in input_images_and_prompts:
            init_image = Image.open(img_path).convert("RGB")
            
            detect_resolution = self.config.get("detect_resolution", 512)
            image_resolution = self.config.get("image_resolution", 1024)
            
            processed_image = self.midas_depth(
                init_image,
                detect_resolution=detect_resolution,
                image_resolution=image_resolution
            )
            
            control_images.append(processed_image)
            prompts.append(prompt)

        pipeline_kwargs = {
            "prompt": prompts,
            "image": control_images,
            "negative_prompt": self.config.get("negative_prompt", "anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured"),
            "num_inference_steps": self.config.get("num_inference_steps", 30),
            "adapter_conditioning_scale": self.config.get("adapter_conditioning_scale", 1.0),
            "guidance_scale": self.config.get("guidance_scale", 7.5)
        }
        return pipeline_kwargs

    def predict(self, model_input):
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        self.pipeline.to(device)
        self.midas_depth.to(device)

    def eval(self):
        # Pipelines manage their own state, but the preprocessor model should be in eval mode
        self.midas_depth.model.eval()
