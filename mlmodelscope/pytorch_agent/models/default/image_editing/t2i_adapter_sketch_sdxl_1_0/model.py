# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL
from controlnet_aux.pidi import PidiNetDetector
from PIL import Image
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_T2I_Adapter_SDXL_Sketch(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        adapter_id = "TencentARC/t2i-adapter-sketch-sdxl-1.0"
        base_model_id = 'stabilityai/stable-diffusion-xl-base-1.0'
        vae_id = "madebyollin/sdxl-vae-fp16-fix"
        annotator_id = "lllyasviel/Annotators"

        torch_dtype = torch.float16 if device != 'cpu' else torch.float32

        self.adapter = T2IAdapter.from_pretrained(adapter_id, torch_dtype=torch_dtype)
        self.scheduler = EulerAncestralDiscreteScheduler.from_pretrained(base_model_id, subfolder="scheduler")
        self.vae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=torch_dtype)
        self.pidinet = PidiNetDetector.from_pretrained(annotator_id)

        self.pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(
            base_model_id,
            vae=self.vae,
            adapter=self.adapter,
            scheduler=self.scheduler,
            torch_dtype=torch_dtype,
            variant="fp16",
        )

        self.generator_seed = self.config.get('seed', 0)
        self.num_inference_steps = self.config.get('num_inference_steps', 30)
        self.adapter_conditioning_scale = self.config.get('adapter_conditioning_scale', 0.9)
        self.guidance_scale = self.config.get('guidance_scale', 7.5)

    def preprocess(self, input_images_and_prompts):
        images = []
        prompts = []
        negative_prompts = []

        for item in input_images_and_prompts:
            img_path = item[0]
            prompt = item[1]
            # Optional negative prompt
            neg_prompt = item[2] if len(item) > 2 else "extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured"

            image = Image.open(img_path).convert("RGB")
            processed_image = self.pidinet(
                image, detect_resolution=1024, image_resolution=1024, apply_filter=True
            )
            images.append(processed_image)
            prompts.append(prompt)
            negative_prompts.append(neg_prompt)

        # The pipeline expects a single prompt or a list of prompts matching the number of images
        # For simplicity, we handle batching by processing one at a time if prompts differ, 
        # but here we assume a batch can be formed.
        return {
            "image": images,
            "prompt": prompts,
            "negative_prompt": negative_prompts,
            "num_inference_steps": self.num_inference_steps,
            "adapter_conditioning_scale": self.adapter_conditioning_scale,
            "guidance_scale": self.guidance_scale,
            "generator": self.generator
        }

    def predict(self, model_input):
        return self.pipeline(**model_input).images

    def postprocess(self, model_output):
        # The pipeline returns a list of PIL images
        return [np.array(img).tolist() for img in model_output]

    def to(self, device):
        self.device = device
        self.pipeline.to(device)
        self.pidinet.to(device)
        self.generator = torch.Generator(device=device).manual_seed(self.generator_seed)
        if device == 'cuda':
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
            except ImportError:
                print("xformers not installed, continuing without it")

    def eval(self):
        pass # Pipeline handles eval mode internally

