from datetime import datetime

"""
Generalized automation script for text-to-image generation PyTorch models.

Supports: text-to-image (Stable Diffusion), unconditional generation (DDPM),
text-to-video, controlnet, and other generative models.

Usage:
    from automation import text_to_image_model_automation

    text_to_image_model_automation(
        models_to_add=["CompVis/stable-diffusion-v1-4"],
        task_type="text_to_image"
    )
"""

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def text_to_image_model_automation(models_to_add=None, task_type="text_to_image"):
    """Generalized automation for text-to-image generation tasks."""
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, input_prompts):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}

    def to(self, device):
        {to_body}

    def eval(self):
        {eval_body}
"""

    class ModelConfig(BaseModel):
        """Configuration for text-to-image models."""

        model_type: str = Field(
            description="Model type: 'stable_diffusion', 'ddpm', 'controlnet', 'video_generation', 'pipeline'.",
            default="stable_diffusion",
        )
        imports: str = Field(
            default="import torch\nfrom diffusers import StableDiffusionPipeline"
        )
        class_name: str = Field(description="Class name in PascalCase.")
        init_config: str = Field(default=", config=None")
        init_body: str = Field(
            description="Complete __init__ body with model loading and config."
        )
        preprocess_body: str = Field(
            description="Preprocessing prompts, handling batch inputs."
        )
        predict_body: str = Field(
            description="Generation with diffusion loop or pipeline call."
        )
        postprocess_body: str = Field(
            description="Convert output to uint8 image lists."
        )
        to_body: str = Field(
            default="self.device = device\\n        self.pipeline.to(device)"
        )
        eval_body: str = Field(
            default="pass  # Pipeline models are already in eval mode"
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
You are an expert in PyTorch text-to-image models (Stable Diffusion, DDPM, etc.).

Generate complete model configurations for: text-to-image, text-to-video, unconditional generation, controlnet.

**Key Guidelines:**
- **Multi-GPU Support**: ALWAYS extract device and multi_gpu from config:
  ```
  device = self.config.pop("_device", "cpu")
  multi_gpu = self.config.pop("_multi_gpu", False)
  ```
- Stable Diffusion: Use pipeline with prompt inputs. For multi-GPU: pipeline.to(device) or use device_map
- DDPM: Use scheduler + UNet2DModel, initialize noise from prompts (batch size)
- ControlNet: Additional conditioning inputs
- Text-to-video: Similar to text-to-image with video pipeline
- For pipelines: Extract device/multi_gpu but pipelines use .to(device) method
- For component-based models: Can use device_map="auto" with individual components

**Examples:**

Stable Diffusion Pipeline (with Multi-GPU support):
{{{{
    "model_type": "stable_diffusion",
    "imports": "import torch\\nfrom diffusers import StableDiffusionPipeline",
    "class_name": "PyTorch_Diffusers_StableDiffusion_V1_4",
    "init_body": "self.config = config if config else dict()\\n        device = self.config.pop(\\"_device\\", \\"cpu\\")\\n        multi_gpu = self.config.pop(\\"_multi_gpu\\", False)\\n\\n        if multi_gpu and device == \\"cuda\\":\\n            self.pipeline = StableDiffusionPipeline.from_pretrained(\\"CompVis/stable-diffusion-v1-4\\", torch_dtype=torch.float16)\\n        else:\\n            self.pipeline = StableDiffusionPipeline.from_pretrained(\\"CompVis/stable-diffusion-v1-4\\")\\n        self.num_inference_steps = self.config.get('num_inference_steps', 50)",
    "preprocess_body": "return input_prompts",
    "predict_body": "return self.pipeline(model_input, num_inference_steps=self.num_inference_steps).images",
    "postprocess_body": "import numpy as np\\n        return [np.array(img).tolist() for img in model_output]",
    "to_body": "self.device = device\\n        self.pipeline.to(device)",
    "eval_body": "pass"
}}}}

DDPM Unconditional (with Multi-GPU support):
{{{{
    "model_type": "ddpm",
    "imports": "import torch\\nfrom diffusers import DDPMScheduler, UNet2DModel",
    "class_name": "PyTorch_Diffusers_DDPM_Cat",
    "init_body": "self.config = config if config else dict()\\n        device = self.config.pop(\\"_device\\", \\"cpu\\")\\n        multi_gpu = self.config.pop(\\"_multi_gpu\\", False)\\n\\n        self.scheduler = DDPMScheduler.from_pretrained(\\"google/ddpm-cat-256\\")\\n        if multi_gpu and device == \\"cuda\\":\\n            self.model = UNet2DModel.from_pretrained(\\"google/ddpm-cat-256\\", torch_dtype=torch.float16)\\n        else:\\n            self.model = UNet2DModel.from_pretrained(\\"google/ddpm-cat-256\\")\\n        self.num_inference_steps = 50",
    "preprocess_body": "batch_size = len(input_prompts)\\n        return torch.randn((batch_size, 3, 256, 256), device=self.device)",
    "predict_body": "sample = model_input\\n        self.scheduler.set_timesteps(self.num_inference_steps)\\n        for t in self.scheduler.timesteps:\\n            with torch.no_grad():\\n                residual = self.model(sample, t).sample\\n            sample = self.scheduler.step(residual, t, sample).prev_sample\\n        return sample",
    "postprocess_body": "images = (model_output / 2 + 0.5).clamp(0, 1) * 255\\n        return images.to(torch.uint8).permute(0,2,3,1).cpu().numpy().tolist()",
    "to_body": "self.device = device\\n        self.model.to(device)",
    "eval_body": "self.model.eval()"
}}}}

Respond with JSON matching ModelConfig schema.
""",
            ),
            (
                "human",
                """
Context: {model_page_context}
Model: '{model_identifier}'

Detect task type (text-to-image, unconditional, controlnet, video) and generate complete config.
""",
            ),
        ]
    )

    modelCounter = 0
    load_dotenv()
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser

    BASE_DIR = f"mlmodelscope/pytorch_agent/models/default/{task_type}"
    ERROR_DIR = f"{BASE_DIR}/automation/" + str(
        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models, login_req_models = [], []

    for model_name in models_to_add:
        if modelCounter == 50:
            break
        print(f"\n--- Processing: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Exists, skipping: {model_folder_name}")
            continue

        try:
            check_syntax = lambda fn: os.system(f"python -m py_compile {fn}")
            error = False
            MAX_TRIES_PER_MODEL = 5
            try_count_my_model = 0
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if try_count_my_model >= MAX_TRIES_PER_MODEL:
                    break
                try_count_my_model += 1
                url = f"https://huggingface.co/{model_name}"
                response = requests.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    break
                if main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                ):
                    login_req_models.append(model_name)
                    break

                context_text = main_content.get_text(separator=" ", strip=True)[:20000]
                model_config_dict = chain.invoke(
                    {"model_identifier": model_name, "model_page_context": context_text}
                )

                init_body = model_config_dict.get("init_body", "").replace(
                    "{hugging_face_model_id}", model_name
                )
                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "import torch\nfrom diffusers import StableDiffusionPipeline",
                    ),
                    class_name=model_config_dict.get(
                        "class_name",
                        f"PyTorch_{task_type.title().replace('_','')}_Model",
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "return input_prompts"
                    ).lstrip(),
                    predict_body=model_config_dict.get(
                        "predict_body", "return self.pipeline(model_input).images"
                    ).lstrip(),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body",
                        "import numpy as np; return [np.array(i).tolist() for i in model_output]",
                    ).lstrip(),
                    to_body=model_config_dict.get(
                        "to_body", "self.device = device; self.pipeline.to(device)"
                    ).lstrip(),
                    eval_body=model_config_dict.get("eval_body", "pass").lstrip(),
                )

                os.makedirs(os.path.join(BASE_DIR, model_folder_name), exist_ok=True)
                with open(model_py_path, "w") as f:
                    f.write(filled_template)
            else:
                print(f"Success: {model_py_path}")
                modelCounter += 1
        except Exception as e:
            print(f"Failed: {e}")
            import traceback

            traceback.print_exc()
            failed_models.append(model_name)

    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        f.write("\n".join(failed_models))
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        f.write("\n".join(login_req_models))


def image_generation_model_automation(models_to_add=None):
    """Legacy wrapper."""
    return text_to_image_model_automation(models_to_add, "text_to_image")


if __name__ == "__main__":
    text_to_image_model_automation(
        models_to_add=["google/ddpm-cat-256"], task_type="unconditional_generation"
    )
