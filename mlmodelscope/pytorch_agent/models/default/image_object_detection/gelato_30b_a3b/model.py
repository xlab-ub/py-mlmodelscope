# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration
from PIL import Image
import re

class PyTorch_Transformers_Gelato30BA3B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        self.device = device

        model_id = "mlfoundations/Gelato-30B-A3B"
        self.processor = AutoProcessor.from_pretrained(model_id)

        if multi_gpu and self.device == "cuda":
            self.model = Qwen3VLMoeForConditionalGeneration.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
        else:
            self.model = Qwen3VLMoeForConditionalGeneration.from_pretrained(model_id)
            self.model.to(self.device)

    def preprocess(self, input_images):
        # This model performs grounding based on a text prompt and an image.
        # The prompt is hardcoded here based on the model card's example.
        # It processes one image at a time.
        img = Image.open(input_images[0]).convert("RGB")
        self.image_width, self.image_height = img.size # Store for postprocessing

        PROMPT = "You are an expert UI element locator. Given a GUI image and a user's element description, provide the coordinates of the specified element as a single (x,y) point. For elements with area, return the center point. Output the coordinate pair exactly: (x,y)"
        INSTRUCTION = "Reload the cache." # Hardcoded instruction from example

        messages = [
            { "role" : "user" , "content" : [
                    { "type" : "text" , "text" : PROMPT + "\n\n" },
                    { "type" : "image" , "image" : img},
                    { "type" : "text" , "text" : "\n" + INSTRUCTION},
                ],
            }
        ]

        model_input = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.device)

        # Store input_ids to correctly decode the generated output later
        self.input_ids = model_input.input_ids

        return model_input

    def predict(self, model_input):
        # Use generate for this text-based model, with a limit on new tokens
        return self.model.generate(**model_input, max_new_tokens=32)

    def postprocess(self, model_output):
        # Trim the input prompt from the generated token IDs
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(self.input_ids, model_output)
        ]

        # Decode the remaining tokens to get the model's text output
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        # Parse the coordinates "(x, y)" from the generated text
        raw_string = output_text[0]
        try:
            matches = re.findall(r"\((\d+),\s*(\d+)\)", raw_string)
            norm_x, norm_y = tuple(map(int, matches[0]))
        except (IndexError, ValueError):
            # Default to (0,0) if parsing fails
            norm_x, norm_y = 0, 0

        # Convert normalized [0, 1000] coordinates to absolute pixel coordinates
        abs_x = int((norm_x * self.image_width) / 1000)
        abs_y = int((norm_y * self.image_height) / 1000)

        # Format the output to match the standard object detection format
        # The model provides a single point, which we represent as a 1x1 bounding box.
        # Confidence score and label are placeholders.
        box = [float(abs_x), float(abs_y), float(abs_x), float(abs_y)]
        score = 1.0
        label = 1 # Placeholder label for "located element"

        # The expected return format is a list per image in the batch.
        # Since we process one image, we return a list containing one list of results.
        return [[score]], [[label]], [[box]]
