# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
from PIL import Image

class PyTorch_Transformers_MMGroundingDinoLargeO365v2Oiv6Goldg(PyTorchAbstractClass):
    def __init__(self):
        super().__init__(config)
        model_id = "openmmlab-community/mm_grounding_dino_large_o365v2_oiv6_goldg"
        self.processor = AutoProcessor.from_pretrained(model_id)
        self.model = self.load_hf_model(AutoModelForZeroShotObjectDetection, model_id)

    def preprocess(self, input_images):
        images = [Image.open(image_path).convert("RGB") for image_path in input_images]
        self.original_sizes = [img.size[::-1] for img in images]  # (height, width)
        text_labels = self.config.get("text_labels")
        if not text_labels:
            raise ValueError("text_labels must be provided in the config for this zero-shot model.")
        model_input = self.processor(images=images, text=text_labels, return_tensors="pt")
        return model_input

    def predict(self, model_input):
        return self.model(**model_input)

    def postprocess(self, model_output):
        threshold = self.config.get("threshold", 0.4)
        results = self.processor.post_process_grounded_object_detection(
            model_output,
            threshold=threshold,
            target_sizes=self.original_sizes
        )
        # The guideline expects a tuple of lists, where each list contains the results for the first image.
        # This implies a batch size of 1.
        result = results[0]
        scores = [result["scores"].tolist()]
        labels = [result["labels"]]  # List of string labels
        boxes = [result["boxes"].tolist()]
        return scores, labels, boxes
