# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoImageProcessor, AutoModelForVision2Seq, AutoTokenizer
from PIL import Image
import torch


class PyTorch_Transformers_Seg_Zero_7B(PyTorchAbstractClass):
    def __init__(self, config):
        super().__init__(config)
        model_id = "Ricky06666/Seg-Zero-7B"
        # This model requires a specific image processor and a tokenizer for text.
        self.processor = AutoImageProcessor.from_pretrained(model_id, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        
        # The error indicates the model config is not compatible with AutoModelForCausalLM.
        # It's a Vision2Seq model, so we must use AutoModelForVision2Seq.
        self.model = self.load_hf_model(AutoModelForVision2Seq, model_id, trust_remote_code=True)

    def preprocess(self, input_images):
        # This model requires both an image and a text prompt. Since the calling
        # framework only provides an image path, we use a generic, hardcoded prompt.
        # This implementation assumes a batch size of 1.
        image_path = input_images[0]
        prompt = "Generate the segmentation mask for all objects."

        # Use the tokenizer's special formatting method for correct input creation.
        model_input = self.tokenizer.from_list_format([
            {'image': image_path},
            {'text': prompt},
        ])
        return model_input

    def predict(self, model_input):
        # Vision-language models like this generate sequences of tokens.
        # We use the `generate` method for inference.
        model_input = {k: v.to(self.model.device) for k, v in model_input.items()}
        
        with torch.inference_mode():
            output_ids = self.model.generate(
                **model_input,
                do_sample=False,
                max_new_tokens=1024,
                use_cache=True,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
        return output_ids

    def postprocess(self, model_output):
        # The output from `generate` is a tensor of token IDs.
        # We decode this tensor back into a string.
        response = self.tokenizer.decode(model_output[0], skip_special_tokens=False)
        
        # The custom tokenizer has a helper function to extract the mask from the response.
        mask = self.tokenizer.get_mask(response)
        
        # The final output should be a JSON-serializable list.
        if mask is not None:
            return mask.tolist()
        else:
            return []