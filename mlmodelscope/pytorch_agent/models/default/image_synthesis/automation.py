import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def image_synthesis_model_automation(models_to_add=None):
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, input_prompts):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}

    def to(self, device):
        {to_body}

    def eval(self):
        {eval_body}
"""

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch image synthesis model."""

        imports: str = Field(
            description="The complete imports section. Include torch, diffusers (StableDiffusionPipeline or components), transformers for CLIP. Use proper formatting with newlines.",
            default="import torch\nfrom diffusers import StableDiffusionPipeline",
        )

        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'PyTorch_Diffusers_Stable_Diffusion_v1_5'.",
        )

        init_config: str = Field(
            description="The __init__ method signature parameters. Typically ', config=None'.",
            default=", config=None",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method. Should include: 1) config initialization, 2) loading pipeline or individual components (VAE, text_encoder, tokenizer, unet, scheduler), 3) setting generation parameters from config (height, width, num_inference_steps, guidance_scale, seed). Include proper indentation (8 spaces).",
        )

        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Should: 1) Tokenize prompts, 2) Get text embeddings, 3) Prepare unconditional embeddings if using guidance, 4) Initialize latents with noise. Include proper indentation (8 spaces).",
        )

        predict_body: str = Field(
            description="The complete body of the predict method. Should run diffusion loop with scheduler or call pipeline. Include proper indentation (8 spaces).",
        )

        postprocess_body: str = Field(
            description="The complete body of the postprocess method. Should: 1) Decode latents with VAE, 2) Normalize images, 3) Convert to uint8, 4) Return as list. Include proper indentation (8 spaces).",
        )

        to_body: str = Field(
            description="The complete body of the to(device) method. Should move all components (VAE, text_encoder, unet) to device and initialize generator. Include proper indentation (8 spaces).",
            default="self.device = device\\n        self.vae.to(device)\\n        self.text_encoder.to(device)\\n        self.unet.to(device)\\n        self.generator = torch.Generator(device=device).manual_seed(self.seed)",
        )

        eval_body: str = Field(
            description="The complete body of the eval() method. Should set all components to eval mode. Include proper indentation (8 spaces).",
            default="self.vae.eval()\\n        self.text_encoder.eval()\\n        self.unet.eval()",
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch image synthesis models (Stable Diffusion). Your task is to generate a complete, working model.py file.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - Stable Diffusion models: Use diffusers with VAE, CLIP text encoder, UNet, scheduler
   - Can use pipeline or individual components

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For Stable Diffusion: `import torch`, diffusers components, transformers CLIP classes

3. **Init Method:**
   - Initialize config
   - Load VAE, text_encoder, tokenizer, unet, scheduler from_pretrained
   - Set generation parameters: height, width, num_inference_steps, guidance_scale, seed

4. **Preprocess Method:**
   - Tokenize prompts with tokenizer
   - Get text embeddings from text_encoder
   - Prepare unconditional embeddings for CFG
   - Concatenate embeddings
   - Initialize latent noise
   - Set scheduler timesteps

5. **Predict Method:**
   - Run diffusion loop
   - For each timestep: denoise latents with unet
   - Apply classifier-free guidance
   - Step scheduler

6. **Postprocess Method:**
   - Scale latents
   - Decode with VAE
   - Normalize to [0, 1]
   - Convert to uint8
   - Return as list

7. **to() and eval() Methods:**
   - Move all components to device
   - Initialize generator on device
   - Set components to eval mode

**Reference Example:**

{{{{
    "imports": "import torch\\nfrom transformers import CLIPTextModel, CLIPTokenizer\\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler",
    "class_name": "PyTorch_Transformers_Stable_Diffusion_v1_5",
    "init_config": ", config=None",
    "init_body": "self.config = config if config else dict()\\n        model_id = \\"runwayml/stable-diffusion-v1-5\\"\\n        self.vae = AutoencoderKL.from_pretrained(model_id, subfolder=\\"vae\\", use_safetensors=True)\\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\\"tokenizer\\")\\n        self.text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\\"text_encoder\\", use_safetensors=True)\\n        self.unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\\"unet\\", use_safetensors=True)\\n        self.scheduler = PNDMScheduler.from_pretrained(model_id, subfolder=\\"scheduler\\")\\n\\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\\n        self.height = self.config.get('height', 512)\\n        self.width = self.config.get('width', 512)\\n        self.num_inference_steps = self.config.get('num_inference_steps', 25)\\n        self.guidance_scale = self.config.get('guidance_scale', 7.5)\\n        self.seed = self.config.get('seed', 0)",
    "preprocess_body": "batch_size = len(input_prompts)\\n        text_inputs = self.tokenizer(input_prompts, padding=\\"max_length\\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\\"pt\\").input_ids.to(self.device)\\n        unconditional_input = self.tokenizer([\\"\\"]*batch_size, padding=\\"max_length\\", max_length=self.tokenizer.model_max_length, return_tensors=\\"pt\\").input_ids.to(self.device)\\n        with torch.no_grad():\\n            cond_embeddings = self.text_encoder(text_inputs).last_hidden_state\\n            uncond_embeddings = self.text_encoder(unconditional_input).last_hidden_state\\n        model_input = torch.cat([uncond_embeddings, cond_embeddings], dim=0)\\n        self.latents = torch.randn((batch_size, self.unet.config.in_channels, self.height//8, self.width//8), generator=self.generator, device=self.device) * self.scheduler.init_noise_sigma\\n        self.scheduler.set_timesteps(self.num_inference_steps, device=self.device)\\n        return model_input",
    "predict_body": "for t in self.scheduler.timesteps:\\n            latent_model_input = torch.cat([self.latents]*2)\\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, timestep=t)\\n            with torch.no_grad():\\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=model_input).sample\\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\\n            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\\n            self.latents = self.scheduler.step(noise_pred, t, self.latents).prev_sample\\n        return self.latents",
    "postprocess_body": "image_latents = model_output * (1/0.18215)\\n        with torch.no_grad():\\n            images = self.vae.decode(image_latents).sample\\n        images = ((images/2)+0.5).clamp(0, 1)\\n        if images.ndim == 3:\\n            images.unsqueeze_(0)\\n        images = (images*255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\\n        return [image.tolist() for image in images]",
    "to_body": "self.device = device\\n        self.vae.to(device)\\n        self.text_encoder.to(device)\\n        self.unet.to(device)\\n        self.generator = torch.Generator(device=device).manual_seed(self.seed)",
    "eval_body": "self.vae.eval()\\n        self.text_encoder.eval()\\n        self.unet.eval()"
}}}}

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Context from Hugging Face page:
---
{model_page_context}
---
Generate config for model: '{model_identifier}'

Use the exact model identifier '{model_identifier}' in the init_body.
""",
            ),
        ]
    )

    modelCounter = 0
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser

    BASE_DIR = "mlmodelscope/pytorch_agent/models/default/image_synthesis"
    ERROR_DIR = f"{BASE_DIR}/errors"
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models = []
    login_req_models = []

    for model_name in models_to_add:
        if modelCounter == 50:
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue

        error_log = ""
        try:
            check_syntax = lambda fn: os.system(f"python -m py_compile {fn}")
            error = False
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if error:
                    error_log += f"Syntax error, regenerating...\n"

                url = f"https://huggingface.co/{model_name}"
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    login_req_models.append(model_name)
                    break

                context_text = main_content.get_text(separator=" ", strip=True)
                if len(context_text) > 20000:
                    context_text = context_text[:20000] + "\n... (truncated)"

                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )

                init_body = model_config_dict.get("init_body", "").replace(
                    "{hugging_face_model_id}", model_name
                )

                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "import torch\nfrom diffusers import StableDiffusionPipeline",
                    ),
                    class_name=model_config_dict.get(
                        "class_name", "PyTorch_Image_Synthesis_Model"
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "pass"
                    ).lstrip(" "),
                    predict_body=model_config_dict.get(
                        "predict_body", "return model_input"
                    ).lstrip(" "),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body", "return model_output"
                    ).lstrip(" "),
                    to_body=model_config_dict.get(
                        "to_body",
                        "self.device = device\n        self.vae.to(device)\n        self.text_encoder.to(device)\n        self.unet.to(device)\n        self.generator = torch.Generator(device=device).manual_seed(self.seed)",
                    ).lstrip(" "),
                    eval_body=model_config_dict.get(
                        "eval_body",
                        "self.vae.eval()\n        self.text_encoder.eval()\n        self.unet.eval()",
                    ).lstrip(" "),
                )

                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

            else:
                print(f"Successfully generated {model_py_path}")
                modelCounter += 1

        except Exception as e:
            print(f"Failed: {e}")
            import traceback

            traceback.print_exc()
            failed_models.append(model_name)

    print(f"\n--- Automation complete ---")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for fm in failed_models:
            f.write(f"{fm}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for lm in login_req_models:
            f.write(f"{lm}\n")


if __name__ == "__main__":
    image_synthesis_model_automation(
        models_to_add=["runwayml/stable-diffusion-v1-5"]
    )

