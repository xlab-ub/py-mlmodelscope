# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
import os
import sys
import subprocess
import numpy as np
import soundfile as sf
from huggingface_hub import snapshot_download

class PyTorch_HuggingFace_F5_TTS_Vietnamese_ViVoice(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        self.device = device

        model_id = "hynt/F5-TTS-Vietnamese-ViVoice"
        model_path = snapshot_download(repo_id=model_id)

        repo_url = "https://github.com/nguyenthienhy/F5-TTS-Vietnamese"
        repo_dir = "F5-TTS-Vietnamese_repo"
        if not os.path.exists(repo_dir):
            subprocess.run(["git", "clone", repo_url, repo_dir], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        sys.path.insert(0, os.path.abspath(repo_dir))

        from f5_tts.models import load_model
        from f5_tts.utils.vocoder import load_vocoder, load_ref_audio
        from f5_tts.infer import infer as f5_infer

        self.f5_infer = f5_infer

        ckpt_file = os.path.join(model_path, "model_last.pt")
        vocab_file = os.path.join(model_path, "vocab.txt")

        self.model = load_model("F5TTS_Base", ckpt_file, vocab_file, self.device)
        self.vocoder = load_vocoder("vocos", self.device)

        dummy_ref_path = "dummy_ref.wav"
        sampling_rate = self.vocoder.sr
        duration = 2
        silence = np.zeros(int(duration * sampling_rate), dtype=np.float32)
        sf.write(dummy_ref_path, silence, sampling_rate)
        
        self.ref_mel = load_ref_audio(dummy_ref_path, self.vocoder, self.device)

        self.features = {"sampling_rate": self.vocoder.sr}

    def preprocess(self, input_texts):
        return input_texts

    def predict(self, model_input):
        speed = self.config.get('speed', 1.0)
        generated_audio = []
        for text in model_input:
            wav = self.f5_infer(text, self.ref_mel, self.model, self.vocoder, speed)
            generated_audio.append(wav)
        return generated_audio

    def postprocess(self, model_output):
        return [wav.tolist() for wav in model_output]
