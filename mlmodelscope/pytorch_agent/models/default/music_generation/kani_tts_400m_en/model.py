# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from kani_tts import KaniTTS
import torch

class PyTorch_KaniTTS_400m_en(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        # KaniTTS takes a 'device' argument in its constructor.
        # We pass the determined device and any other generation parameters from the config.
        model_kwargs = self.config.copy()
        model_kwargs['device'] = device

        self.model = KaniTTS('nineninesix/kani-tts-400m-en', **model_kwargs)
        self.features = {"sampling_rate": self.model.sample_rate}

    def preprocess(self, input_texts):
        # The model takes raw text, so we just pass it through in a dictionary.
        return {"texts": input_texts}

    def predict(self, model_input):
        input_texts = model_input["texts"]
        # The model call can also take a speaker_id, which we get from the config.
        speaker_id = self.config.get("speaker_id")
        
        audios = []
        for text in input_texts:
            # The model returns a tuple (audio_waveform, text_prompt)
            audio, _ = self.model(text, speaker_id=speaker_id)
            audios.append(audio)
        
        return audios

    def postprocess(self, model_output):
        # model_output is a list of audio waveforms (numpy arrays)
        # Convert each waveform to a list for JSON serialization.
        return [audio.tolist() for audio in model_output]
