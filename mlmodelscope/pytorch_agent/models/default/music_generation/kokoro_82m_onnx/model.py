# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from onnxruntime import InferenceSession
from huggingface_hub import hf_hub_download
import numpy as np
import os
import sys
import importlib.util

class PyTorch_ONNX_Kokoro_82M_ONNX(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = 'onnx-community/Kokoro-82M-ONNX'

        model_path = hf_hub_download(repo_id=model_id, filename="onnx/model.onnx")
        kokoro_py_path = hf_hub_download(repo_id=model_id, filename="kokoro.py")
        voice_file = self.config.get("voice_file", "voices/af.bin")
        voice_path = hf_hub_download(repo_id=model_id, filename=voice_file)

        spec = importlib.util.spec_from_file_location("kokoro", kokoro_py_path)
        kokoro_module = importlib.util.module_from_spec(spec)
        sys.modules["kokoro"] = kokoro_module
        spec.loader.exec_module(kokoro_module)
        self.phonemize = kokoro_module.phonemize
        self.tokenize = kokoro_module.tokenize

        self.voices = np.fromfile(voice_path, dtype=np.float32).reshape(-1, 1, 256)

        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == 'cuda' else ['CPUExecutionProvider']
        self.sess = InferenceSession(model_path, providers=providers)

        self.features = {"sampling_rate": 24000}

    def preprocess(self, input_texts):
        processed_inputs = []
        for text in input_texts:
            phonemes = self.phonemize(text)
            tokens = self.tokenize(phonemes)

            if len(tokens) >= self.voices.shape[0]:
                tokens = tokens[:self.voices.shape[0] - 1]

            ref_s = self.voices[len(tokens)]

            input_ids = np.array([[0, *tokens, 0]], dtype=np.int64)
            speed = np.ones(1, dtype=np.float32)

            processed_inputs.append({
                'input_ids': input_ids,
                'style': ref_s,
                'speed': speed
            })
        return processed_inputs

    def predict(self, model_input):
        outputs = []
        for single_input in model_input:
            audio_output = self.sess.run(None, single_input)[0]
            outputs.append(audio_output)
        return outputs

    def postprocess(self, model_output):
        results = []
        for audio_array in model_output:
            results.append(audio_array.squeeze().tolist())
        return results
