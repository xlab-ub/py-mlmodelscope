# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import outetts
import torch
import numpy as np

class PyTorch_OuteAI_Llama_OuteTTS_1_0_1B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        # multi_gpu is not directly used as the 'outetts' library with the HF backend handles device mapping.
        multi_gpu = self.config.pop("_multi_gpu", False)

        # Use float16 for CUDA for better performance, as recommended for many modern transformers.
        dtype = torch.float16 if device == "cuda" else torch.float32

        # The OuteTTS model is used via its dedicated 'outetts' library.
        model_config = outetts.ModelConfig.auto_config(
            model="OuteAI/Llama-OuteTTS-1.0-1B",
            backend=outetts.Backend.HF,
            options=outetts.HuggingFaceOptions(
                torch_dtype=dtype
            )
        )
        self.interface = outetts.Interface(config=model_config)

        # The model requires a speaker reference. We load a default one during initialization.
        self.default_speaker = self.interface.load_default_speaker("EN-FEMALE-1-NEUTRAL")

        # Configure the sampler based on the model card's recommendations.
        self.sampler_config = outetts.SamplerConfig(
            temperature=self.config.get("temperature", 0.4),
            repetition_penalty=self.config.get("repetition_penalty", 1.1),
            repetition_range=self.config.get("repetition_range", 64),
            top_k=self.config.get("top_k", 40),
            top_p=self.config.get("top_p", 0.9),
            min_p=self.config.get("min_p", 0.05)
        )

        # The audio encoder (DAC) used by this model has a sampling rate of 24000.
        self.features = {"sampling_rate": 24000}

    def preprocess(self, input_texts):
        # The 'outetts' library takes raw text strings as input.
        # We pass the input texts in a dictionary to the predict method.
        return {"texts": input_texts}

    def predict(self, model_input):
        # model_input is a dictionary like {"texts": ["text1", "text2"]}
        texts = model_input["texts"]
        outputs = []
        # The library processes one text at a time, so we iterate through the batch.
        for text in texts:
            generation_config = outetts.GenerationConfig(
                text=text,
                generation_type=outetts.GenerationType.CHUNKED,
                speaker=self.default_speaker,
                sampler_config=self.sampler_config
            )
            output = self.interface.generate(config=generation_config)
            outputs.append(output)
        return outputs

    def postprocess(self, model_output):
        # model_output is a list of outetts.GenerationOutput objects.
        # Each object contains the generated audio in a .waveform attribute as a numpy array.
        return [output.waveform.tolist() for output in model_output]
