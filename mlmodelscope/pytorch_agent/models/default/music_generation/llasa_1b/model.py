# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from xcodec2.modeling_xcodec2 import XCodec2Model

class PyTorch_Transformers_Llasa_1B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        self.device = device
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "HKUSTAudio/Llasa-1B"
        codec_id = "HKUSTAudio/xcodec2"

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"

        self.codec_model = XCodec2Model.from_pretrained(codec_id).to(self.device)
        self.codec_model.eval()

        if multi_gpu and device == "cuda":
            self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_id).to(self.device)
        self.model.eval()

        self.max_length = self.config.get('max_length', 2048)
        self.do_sample = self.config.get('do_sample', True)
        self.top_p = self.config.get('top_p', 1.0)
        self.temperature = self.config.get('temperature', 0.8)
        self.speech_end_id = self.tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')

        self.features = {"sampling_rate": 16000}

    def preprocess(self, input_texts):
        chats = []
        for text in input_texts:
            formatted_text = f"<|TEXT_UNDERSTANDING_START|> {text} <|TEXT_UNDERSTANDING_END|>"
            chat = [
                {"role": "user", "content": "Convert the text to speech:" + formatted_text},
                {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>"}
            ]
            chats.append(chat)
        
        formatted_prompts = self.tokenizer.apply_chat_template(chats, tokenize=False, add_generation_prompt=True)
        model_input = self.tokenizer(formatted_prompts, return_tensors="pt", padding=True)
        
        return model_input.to(self.model.device)

    def predict(self, model_input):
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_input,
                max_length=self.max_length,
                eos_token_id=self.speech_end_id,
                do_sample=self.do_sample,
                top_p=self.top_p,
                temperature=self.temperature
            )
        return (generated_ids, model_input["attention_mask"])

    def postprocess(self, model_output):
        generated_ids, attention_mask = model_output
        generated_ids = generated_ids.cpu()
        attention_mask = attention_mask.cpu()
        
        outputs = []
        for i in range(len(attention_mask)):
            input_len = int(attention_mask[i].sum())
            # Slice to get only the generated part, excluding the prompt and the final EOS token
            output_tokens = generated_ids[i][input_len:-1]
            
            speech_tokens_str = self.tokenizer.batch_decode(output_tokens, skip_special_tokens=True)
            
            speech_ids = []
            for token_str in speech_tokens_str:
                if token_str.startswith('<|s_') and token_str.endswith('|>'):
                    num_str = token_str[4:-2].strip()
                    try:
                        speech_ids.append(int(num_str))
                    except ValueError:
                        continue
            
            if not speech_ids:
                outputs.append([])
                continue
            
            speech_tokens_tensor = torch.tensor(speech_ids).to(self.codec_model.device).unsqueeze(0).unsqueeze(0)
            
            with torch.no_grad():
                gen_wav = self.codec_model.decode_code(speech_tokens_tensor)
            
            waveform_list = gen_wav[0, 0, :].cpu().numpy().tolist()
            outputs.append(waveform_list)
            
        return outputs
