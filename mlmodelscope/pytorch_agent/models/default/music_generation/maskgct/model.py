# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import os
import sys
import subprocess
import torch
from huggingface_hub import hf_hub_download
import safetensors
import json

class PyTorch_Amphion_MaskGCT(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False) # Not used by this model's custom loader, but extracted per guidelines.
        self.device = torch.device(device)

        repo_path = "Amphion"
        if not os.path.exists(repo_path):
            subprocess.run(["git", "clone", "https://github.com/open-mmlab/Amphion.git"], check=True)
        sys.path.insert(0, os.path.abspath(repo_path))

        from models.tts.maskgct.maskgct_utils import load_config, build_semantic_model, build_semantic_codec, build_acoustic_codec, build_t2s_model, build_s2a_model, MaskGCT_Inference_Pipeline

        model_id = "amphion/MaskGCT"
        semantic_code_ckpt = hf_hub_download(model_id, filename="semantic_codec/model.safetensors")
        codec_encoder_ckpt = hf_hub_download(model_id, filename="acoustic_codec/model.safetensors")
        codec_decoder_ckpt = hf_hub_download(model_id, filename="acoustic_codec/model_1.safetensors")
        t2s_model_ckpt = hf_hub_download(model_id, filename="t2s_model/model.safetensors")
        s2a_1layer_ckpt = hf_hub_download(model_id, filename="s2a_model/s2a_model_1layer/model.safetensors")
        s2a_full_ckpt = hf_hub_download(model_id, filename="s2a_model/s2a_model_full/model.safetensors")

        cfg_path = os.path.join(repo_path, "models/tts/maskgct/config/maskgct.json")
        cfg = load_config(cfg_path)

        semantic_model, semantic_mean, semantic_std = build_semantic_model(self.device)
        semantic_codec = build_semantic_codec(cfg.model.semantic_codec, self.device)
        safetensors.torch.load_model(semantic_codec, semantic_code_ckpt)
        codec_encoder, codec_decoder = build_acoustic_codec(cfg.model.acoustic_codec, self.device)
        safetensors.torch.load_model(codec_encoder, codec_encoder_ckpt)
        safetensors.torch.load_model(codec_decoder, codec_decoder_ckpt)
        t2s_model = build_t2s_model(cfg.model.t2s_model, self.device)
        safetensors.torch.load_model(t2s_model, t2s_model_ckpt)
        s2a_model_1layer = build_s2a_model(cfg.model.s2a_model.s2a_1layer, self.device)
        s2a_model_full = build_s2a_model(cfg.model.s2a_model.s2a_full, self.device)
        safetensors.torch.load_model(s2a_model_1layer, s2a_1layer_ckpt)
        safetensors.torch.load_model(s2a_model_full, s2a_full_ckpt)

        self.model = MaskGCT_Inference_Pipeline(semantic_model, semantic_codec, codec_encoder, codec_decoder, t2s_model, s2a_model_1layer, s2a_model_full, semantic_mean, semantic_std, self.device)

        self.features = {"sampling_rate": 24000}

        sys.path.pop(0)

    def preprocess(self, input_texts):
        # Expects a list of JSON strings, each containing keys: "prompt_wav_path", "prompt_text", "target_text"
        parsed_inputs = [json.loads(text) for text in input_texts]
        return parsed_inputs

    def predict(self, model_input):
        # model_input is a list of dictionaries from preprocess
        # This model's inference pipeline does not support batching, so we process one by one.
        results = []
        target_len = self.config.get("target_len", None)
        prompt_lang = self.config.get("prompt_lang", "en")
        target_lang = self.config.get("target_lang", "en")

        for item in model_input:
            prompt_wav_path = item["prompt_wav_path"]
            prompt_text = item["prompt_text"]
            target_text = item["target_text"]
            
            audio_output = self.model.maskgct_inference(
                prompt_wav_path,
                prompt_text,
                target_text,
                prompt_lang,
                target_lang,
                target_len=target_len
            )
            results.append(audio_output)
        return results

    def postprocess(self, model_output):
        # model_output is a list of numpy arrays from predict
        return [audio.tolist() for audio in model_output]
