# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from diffusers import StableDiffusionPipeline
import torch
import numpy as np
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Diffusers_Riffusion_Model_V1(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        # Riffusion uses a StableDiffusionPipeline. We load it with float16 for efficiency
        # and move the entire pipeline to the specified device.
        # The `device_map` argument from transformers is not applicable here.
        self.model = StableDiffusionPipeline.from_pretrained("riffusion/riffusion-model-v1", torch_dtype=torch.float16)
        self.model.to(device)

        # This model generates spectrogram images, not raw audio waveforms.
        # Therefore, there is no sampling_rate in the model's config.
        self.features = {}

    def preprocess(self, input_texts):
        # The pipeline expects a 'prompt' keyword argument.
        return {"prompt": input_texts}

    def predict(self, model_input):
        # The pipeline returns a result object containing a list of PIL images.
        return self.model(**model_input).images

    def postprocess(self, model_output):
        # The model output is a list of PIL Images representing spectrograms.
        # We convert them to numpy arrays and then to lists as per the required output format.
        # A separate step is required to convert these spectrograms into audio, which is outside this wrapper's scope.
        return [np.array(image).tolist() for image in model_output]
