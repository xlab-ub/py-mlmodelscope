"""
Generalized automation script for text-classification PyTorch models.

This script can generate wrapper classes for various text classification tasks:
- Sentiment Analysis (positive/negative, multi-class emotions)
- Emotion Detection (joy, anger, sadness, etc.)
- Toxicity Detection (toxic, severe_toxic, obscene, etc.)
- Spam Detection (spam/ham classification)
- Intent Classification (chatbot intents)
- Named Entity Recognition (NER)
- Topic Classification
- Language Detection
- And more...

The script uses LLM (Google Gemini) to analyze model cards and generate
appropriate PyTorchAbstractClass implementations with preprocessing,
prediction, and postprocessing methods.

Usage:
    from automation import text_classification_model_automation

    # For sentiment analysis models
    text_classification_model_automation(
        models_to_add=["distilbert-base-uncased-finetuned-sst-2-english"],
        task_type="sentiment_analysis"
    )

    # For toxicity detection models
    text_classification_model_automation(
        models_to_add=["unitary/toxic-bert"],
        task_type="toxicity_detection"
    )
"""

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI


def text_classification_model_automation(
    models_to_add=None, task_type="sentiment_analysis"
):
    """
    Generalized automation for text classification tasks.

    Args:
        models_to_add: List of model identifiers to process
        task_type: Type of task - "sentiment_analysis", "toxicity_detection", "intent_classification", etc.
    """
    if models_to_add is None:
        return "No models specified for addition."

    MODEL_TEMPLATE = """# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

{imports}

class {class_name}(PyTorchAbstractClass):
    def __init__(self{init_config}):
        {init_body}

    def preprocess(self, input_texts):
        {preprocess_body}

    def predict(self, model_input):
        {predict_body}

    def postprocess(self, model_output):
        {postprocess_body}
"""

    class ModelConfig(BaseModel):
        """Configuration details for a PyTorch text classification model."""

        model_type: str = Field(
            description="The type of model. Options: 'sequence_classification' (BERT, RoBERTa, DistilBERT for sentiment/toxicity/etc.), 'token_classification' (for NER), 'zero_shot_classification' (for zero-shot classification), 'custom'.",
            default="sequence_classification",
        )

        imports: str = Field(
            description="The complete imports section. Include transformers classes (tokenizer and model for sequence/token classification), torch.nn.functional for softmax. Use proper formatting with newlines.",
            default="from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.nn.functional import softmax",
        )

        class_name: str = Field(
            description="The class name for the model. Should be descriptive and follow PascalCase convention, e.g., 'PyTorch_Transformers_DistilBERT_Sentiment', 'PyTorch_Transformers_BERT_Toxicity'.",
        )

        init_config: str = Field(
            description="The __init__ method signature parameters. Typically ', config=None'.",
            default=", config=None",
        )

        init_body: str = Field(
            description="The complete body of the __init__ method. Should include: 1) config initialization with super().__init__(config), 2) loading tokenizer from_pretrained, 3) loading model using self.load_hf_model(ModelClass, model_id) for multi-GPU support, 4) extracting label names from model.config.id2label and storing in self.features list.",
        )

        preprocess_body: str = Field(
            description="The complete body of the preprocess method. Should use tokenizer with return_tensors='pt', padding=True, truncation=True. For NER, may need additional parameters.",
        )

        predict_body: str = Field(
            description="The complete body of the predict method. For sequence classification: call model(**model_input).logits. For token classification: call model(**model_input).logits. For zero-shot: use pipeline appropriately.",
        )

        postprocess_body: str = Field(
            description="The complete body of the postprocess method. For sequence classification: apply softmax and convert to list. For token classification: argmax per token then map to labels. For multilabel: apply sigmoid instead of softmax.",
        )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in PyTorch text classification models. Your task is to generate a complete, working model.py file for various text classification tasks: sentiment analysis, emotion detection, toxicity detection, spam detection, intent classification, NER, topic classification, etc.

You will be given a Hugging Face model identifier and the text content from its model card page.
You must generate the complete model configuration based *primarily* on the provided page context.

**IMPORTANT GUIDELINES:**

1. **Model Type Detection:**
   - **Sequence Classification** (most common): Sentiment, emotion, toxicity, spam, intent, topic classification
     - Use AutoModelForSequenceClassification + AutoTokenizer
     - Single or multi-label classification
   - **Token Classification**: NER (Named Entity Recognition), POS tagging
     - Use AutoModelForTokenClassification + AutoTokenizer
   - **Zero-Shot Classification**: Use pipeline or specific zero-shot models
   - Check the model card to determine the exact task

2. **Imports:**
   - Always include: `from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass`
   - For sequence classification: `from transformers import AutoTokenizer, AutoModelForSequenceClassification` + `from torch.nn.functional import softmax`
   - For token classification: `from transformers import AutoTokenizer, AutoModelForTokenClassification` + `import torch`
   - For multilabel: Use `import torch` instead of just softmax

3. **Init Method:**
   - Initialize config: `super().__init__(config)` (required for load_hf_model to work)
   - Load tokenizer: `AutoTokenizer.from_pretrained(model_id)`
   - Load model: `self.model = self.load_hf_model(AutoModelForSequenceClassification, model_id)` (use load_hf_model for multi-GPU support)
   - Extract labels: `self.features = list(self.model.config.id2label.values())`
   - Check if trust_remote_code or problem_type (multilabel) is needed

4. **Preprocess Method:**
   - Tokenize: `return self.tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True)`
   - For NER: May need `return_offsets_mapping=True` or other special parameters

5. **Predict Method:**
   - For sequence classification: `return self.model(**model_input).logits`
   - For token classification: `return self.model(**model_input).logits`

6. **Postprocess Method:**
   - For single-label classification: `return softmax(model_output, dim=1).tolist()`
   - For multi-label classification: `return torch.sigmoid(model_output).tolist()`
   - For token classification: 
     ```
     predictions = torch.argmax(model_output, dim=2)
     predicted_labels = [[self.features[p.item()] for p in prediction] for prediction in predictions]
     return predicted_labels
     ```

**Reference Examples:**

Example 1: Sentiment Analysis (Binary Classification)
{{{{
    "model_type": "sequence_classification",
    "imports": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nfrom torch.nn.functional import softmax",
    "class_name": "PyTorch_Transformers_DistilBERT_Sentiment",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"distilbert-base-uncased-finetuned-sst-2-english\\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForSequenceClassification, model_id)\\n\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "return self.tokenizer(input_texts, return_tensors=\\"pt\\", padding=True, truncation=True)",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "return softmax(model_output, dim=1).tolist()"
}}}}

Example 2: Toxicity Detection (Multi-label Classification)
{{{{
    "model_type": "sequence_classification",
    "imports": "import torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification",
    "class_name": "PyTorch_Transformers_BERT_Toxicity",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"unitary/toxic-bert\\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForSequenceClassification, model_id)\\n\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "return self.tokenizer(input_texts, return_tensors=\\"pt\\", padding=True, truncation=True)",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "return torch.sigmoid(model_output).tolist()"
}}}}

Example 3: Emotion Detection (Multi-class Classification)
{{{{
    "model_type": "sequence_classification",
    "imports": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nfrom torch.nn.functional import softmax",
    "class_name": "PyTorch_Transformers_RoBERTa_Emotion",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"j-hartmann/emotion-english-distilroberta-base\\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForSequenceClassification, model_id)\\n\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "return self.tokenizer(input_texts, return_tensors=\\"pt\\", padding=True, truncation=True)",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "return softmax(model_output, dim=1).tolist()"
}}}}

Example 4: Named Entity Recognition (Token Classification)
{{{{
    "model_type": "token_classification",
    "imports": "import torch\\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification",
    "class_name": "PyTorch_Transformers_BERT_NER",
    "init_config": ", config=None",
    "init_body": "super().__init__(config)\\n        model_id = \\"dslim/bert-base-NER\\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\\n        self.model = self.load_hf_model(AutoModelForTokenClassification, model_id)\\n\\n        self.features = list(self.model.config.id2label.values())",
    "preprocess_body": "return self.tokenizer(input_texts, return_tensors=\\"pt\\", padding=True, truncation=True)",
    "predict_body": "return self.model(**model_input).logits",
    "postprocess_body": "predictions = torch.argmax(model_output, dim=2)\\n        predicted_labels = [[self.features[p.item()] for p in prediction] for prediction in predictions]\\n        return predicted_labels"
}}}}

**CRITICAL NOTES:**
- Detect if it's single-label (softmax), multi-label (sigmoid), or token classification
- For multi-label: Use torch.sigmoid() instead of softmax
- For token classification: Use argmax per token and map to label names
- Always extract id2label to self.features list
- Check for trust_remote_code if model requires it
- Use truncation=True to handle long texts
- Make sure the code is complete and runnable

Respond ONLY with the JSON structure matching the ModelConfig schema.
""",
            ),
            (
                "human",
                """
Context from Hugging Face page:
---
{model_page_context}
---
Generate config for model: '{model_identifier}'

**IMPORTANT:**
- Use the exact model identifier '{model_identifier}' in the init_body when loading the model
- Detect the task type from context (sentiment, emotion, toxicity, NER, etc.)
- Determine if it's single-label, multi-label, or token classification
- Choose appropriate postprocessing (softmax, sigmoid, or token-level)
- Generate complete, working code for all methods
""",
            ),
        ]
    )

    modelCounter = 0
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser

    BASE_DIR = f"mlmodelscope/pytorch_agent/models/default/{task_type}"
    ERROR_DIR = f"{BASE_DIR}/errors"
    os.makedirs(ERROR_DIR, exist_ok=True)
    failed_models = []
    login_req_models = []

    for model_name in models_to_add:
        if modelCounter == 50:
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue

        error_log = ""
        try:
            check_syntax = lambda fn: os.system(f"python -m py_compile {fn}")
            error = False
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if error:
                    error_log += f"Syntax error, regenerating...\n"

                url = f"https://huggingface.co/{model_name}"
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    login_req_models.append(model_name)
                    break

                context_text = main_content.get_text(separator=" ", strip=True)
                if len(context_text) > 20000:
                    context_text = context_text[:20000] + "\n... (truncated)"

                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )

                init_body = (
                    model_config_dict.get("init_body", "")
                    .replace("{hugging_face_model_id}", model_name)
                    .replace("MODEL_IDENTIFIER_PLACEHOLDER", model_name)
                )

                filled_template = MODEL_TEMPLATE.format(
                    imports=model_config_dict.get(
                        "imports",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.nn.functional import softmax",
                    ),
                    class_name=model_config_dict.get(
                        "class_name",
                        f"PyTorch_{task_type.replace('_', ' ').title().replace(' ', '_')}_Model",
                    ),
                    init_config=model_config_dict.get("init_config", ", config=None"),
                    init_body=init_body.lstrip(" "),
                    preprocess_body=model_config_dict.get(
                        "preprocess_body", "pass"
                    ).lstrip(" "),
                    predict_body=model_config_dict.get(
                        "predict_body", "return self.model(**model_input).logits"
                    ).lstrip(" "),
                    postprocess_body=model_config_dict.get(
                        "postprocess_body",
                        "return softmax(model_output, dim=1).tolist()",
                    ).lstrip(" "),
                )

                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)

            else:
                print(f"Successfully generated {model_py_path}")
                modelCounter += 1

        except Exception as e:
            print(f"Failed: {e}")
            import traceback

            traceback.print_exc()
            failed_models.append(model_name)

    print(f"\n--- Automation complete ---")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for fm in failed_models:
            f.write(f"{fm}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for lm in login_req_models:
            f.write(f"{lm}\n")


# Backward compatibility wrapper
def sentiment_analysis_model_automation(models_to_add=None):
    """Legacy function name for sentiment analysis. Use text_classification_model_automation instead."""
    return text_classification_model_automation(
        models_to_add=models_to_add, task_type="sentiment_analysis"
    )


if __name__ == "__main__":
    # Example usage for different task types:

    # Sentiment analysis models
    # text_classification_model_automation(
    #     models_to_add=["distilbert-base-uncased-finetuned-sst-2-english"],
    #     task_type="sentiment_analysis"
    # )

    # Toxicity detection models
    # text_classification_model_automation(
    #     models_to_add=["unitary/toxic-bert"],
    #     task_type="toxicity_detection"
    # )

    # Emotion detection models
    text_classification_model_automation(
        models_to_add=["j-hartmann/emotion-english-distilroberta-base"],
        task_type="emotion_detection",
    )
