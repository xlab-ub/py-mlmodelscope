# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class PyTorch_Transformers_Bart_ZeroShot_YahooAnswers(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        model_id = "joeddav/bart-large-mnli-yahoo-answers"

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        if multi_gpu and device == "cuda":
            self.model = AutoModelForSequenceClassification.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(model_id)

        self.hypothesis_template = self.config.pop("hypothesis_template", "This text is about {}.")
        self.features = self.config.pop("candidate_labels", [])
        if not self.features:
            raise ValueError("candidate_labels must be provided in the config for zero-shot classification.")

    def preprocess(self, input_texts):
        self.num_texts = len(input_texts)
        self.num_labels = len(self.features)
        
        pairs = []
        for text in input_texts:
            for label in self.features:
                pairs.append([text, self.hypothesis_template.format(label)])
                
        return self.tokenizer(pairs, return_tensors="pt", padding=True, truncation=True)

    def predict(self, model_input):
        return self.model(**model_input).logits

    def postprocess(self, model_output):
        # model_output has shape (num_texts * num_labels, 3)
        # The NLI model labels are: 0 -> contradiction, 1 -> neutral, 2 -> entailment
        
        # Reshape logits to (num_texts, num_labels, 3)
        reshaped_logits = model_output.view(self.num_texts, self.num_labels, -1)
        
        # We are interested in the logits for contradiction (0) and entailment (2)
        entail_contradiction_logits = reshaped_logits[:, :, [0, 2]]
        
        # Apply softmax to get probabilities over contradiction and entailment
        probs = torch.nn.functional.softmax(entail_contradiction_logits, dim=-1)
        
        # The probability of the label being true is the probability of entailment (the second element)
        prob_label_is_true = probs[:, :, 1]
        
        return prob_label_is_true.tolist()
