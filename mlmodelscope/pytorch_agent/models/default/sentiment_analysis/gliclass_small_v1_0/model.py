# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class PyTorch_Transformers_GLiClass_Small_ZeroShot(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        model_id = "knowledgator/gliclass-small-v1.0"

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

        if multi_gpu and device == "cuda":
            self.model = AutoModelForSequenceClassification.from_pretrained(model_id, trust_remote_code=True, device_map="auto", torch_dtype="auto")
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(model_id, trust_remote_code=True)

        self.labels = self.config.get("labels")
        if not self.labels or not isinstance(self.labels, list) or not all(isinstance(label, str) for label in self.labels):
            raise ValueError("Zero-shot classification requires a 'labels' config parameter containing a list of strings.")
        self.features = self.labels

        # Pre-tokenize labels for efficiency, as they are fixed for the model instance
        self.tokenized_labels = self.tokenizer(self.labels, return_tensors="pt", padding=True, truncation=True)

    def preprocess(self, input_texts):
        text_inputs = self.tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True)
        
        # The custom model's forward pass is designed to handle a batch of texts against a single set of labels.
        # We combine the tokenized texts with the pre-tokenized labels into one dictionary.
        # The agent is responsible for moving the entire dictionary of tensors to the correct device before prediction.
        model_input = {**text_inputs}
        model_input['label_input_ids'] = self.tokenized_labels.input_ids
        model_input['label_attention_mask'] = self.tokenized_labels.attention_mask
        
        return model_input

    def predict(self, model_input):
        # The model's custom forward method accepts text and label inputs simultaneously
        return self.model(**model_input).logits

    def postprocess(self, model_output):
        # The model card example indicates multi-label classification, so we use a sigmoid function.
        return torch.sigmoid(model_output).tolist()
