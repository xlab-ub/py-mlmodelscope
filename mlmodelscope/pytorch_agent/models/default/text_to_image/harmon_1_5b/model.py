# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoTokenizer, AutoModel

class PyTorch_Transformers_Harmon_1_5B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)
        self.multi_gpu = multi_gpu

        model_id = "wusize/Harmon-1_5B"
        dtype = torch.bfloat16 if device == "cuda" else torch.float32

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype=dtype,
                device_map="auto"
            )
        else:
            self.model = AutoModel.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype=dtype
            )

        self.num_inference_steps = self.config.get("num_inference_steps", 64)
        self.cfg_scale = self.config.get("cfg_scale", 3.0)
        self.temperature = self.config.get("temperature", 1.0)
        self.image_size = 512

    def preprocess(self, input_prompts):
        GENERATION_TEMPLATE = "Generate an image: {text}"
        return [GENERATION_TEMPLATE.format(text=p) for p in input_prompts]

    def predict(self, model_input):
        PROMPT_TEMPLATE = dict(
            SYSTEM='<|im_start|>system\n{system}<|im_end|>\n',
            INSTRUCTION='<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n',
            SUFFIX='<|im_end|>',
            SUFFIX_AS_EOS=True,
            SEP='\n',
            STOP_WORDS=['<|im_end|>', '<|endoftext|>']
        )
        negative_prompt = self.config.get("negative_prompt", "Generate an image.")
        
        prompts = [PROMPT_TEMPLATE['INSTRUCTION'].format(input=p) for p in model_input]
        
        if self.cfg_scale != 1.0:
            prompts += [PROMPT_TEMPLATE['INSTRUCTION'].format(input=negative_prompt)] * len(model_input)

        inputs = self.tokenizer(
            prompts, add_special_tokens=True, return_tensors='pt', padding=True
        ).to(self.model.device)

        m = n = self.image_size // 16
        images = self.model.sample(
            **inputs,
            num_iter=self.num_inference_steps,
            cfg=self.cfg_scale,
            cfg_schedule="constant",
            temperature=self.temperature,
            progress=False,
            image_shape=(m, n)
        )
        return images

    def postprocess(self, model_output):
        import torch
        import numpy as np
        images = torch.clamp(127.5 * model_output + 128.0, 0, 255)
        images = images.to("cpu", dtype=torch.uint8).permute(0, 2, 3, 1).numpy()
        return [img.tolist() for img in images]

    def to(self, device):
        self.device = device
        if not self.multi_gpu or device == "cpu":
            self.model.to(device)

    def eval(self):
        self.model.eval()
