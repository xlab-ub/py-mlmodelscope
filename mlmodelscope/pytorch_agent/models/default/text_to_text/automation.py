from datetime import datetime
import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Optional


def text_to_text_model_automation(models_to_add=None):
    # --- 1. Model List ---

    if models_to_add is None:
        return "No models specified for addition."

    # --- 2. Model.py Templates ---

    # These blocks will be conditionally added to the main template
    # This is the "if 'messages' in self.config:" block
    RUNTIME_SWITCH_TEMPLATE = """
        if "messages" in self.config:
            self.messages = self.config["messages"]
            self.preprocess = self.preprocess_chat
            self.postprocess = self.postprocess_chat
"""

    # These are the full "preprocess_chat" and "postprocess_chat" methods
    CHAT_METHODS_TEMPLATE = """
    def preprocess_chat(self, input_texts):
        formatted_messages = []
        for message in self.messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            formatted_messages.append({{"role": role, "content": content}})
        formatted_messages.append({{"role": "user", "content": input_texts[0]}})

        # --- Generated Preprocess Chat ---
        {preprocess_chat_logic}
        # --- End Generated Preprocess Chat ---

        # The chat template logic above must set a variable named 'encoded'
        self.input_ids_shape = encoded["input_ids"].shape
        return encoded

    def postprocess_chat(self, model_output):
        # --- Generated Postprocess Chat ---
        {postprocess_chat_logic}
        # --- End Generated Postprocess Chat ---
"""

    # The main template is now simpler, with placeholders for the conditional blocks
    MODEL_TEMPLATE = """
# Generated by automation script
# Using the absolute import path to avoid ModuleNotFoundError
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import {tokenizer_class}, {model_class}


class {class_name}(PyTorchAbstractClass):
    def __init__(self, config=None):
        super().__init__(config)

        model_id = "{hugging_face_model_id}"

        # --- Generated Configuration ---
        {trust_remote_code_comment}
        tokenizer_args = {{'padding_side': 'left'{tokenizer_trust_remote}}}
        
        try:
            self.tokenizer = {tokenizer_class}.from_pretrained(model_id, **tokenizer_args)
            self.model = self.load_hf_model({model_class}, model_id{model_trust_remote_kwargs})
        except Exception as e:
            if model_id in e.__str__():
                self.huggingface_authenticate()
                self.tokenizer = {tokenizer_class}.from_pretrained(model_id, **tokenizer_args)
                self.model = self.load_hf_model({model_class}, model_id{model_trust_remote_kwargs})
            else:
                raise e

        {pad_token_str}
        # --- End Generated Configuration ---

        self.max_new_tokens = self.config.get("max_new_tokens", {max_new_tokens})

    
        # This block is now conditionally added
        {runtime_switch}

    def preprocess(self, input_texts):
        # This is the default/base preprocessor
        # The 'predict' method expects a dictionary or object with 'input_ids'
        return self.tokenizer(input_texts, return_tensors="pt", padding=True)

    def predict(self, model_input):
        outputs = self.model.generate(
            model_input["input_ids"],
            attention_mask=model_input.get("attention_mask"), # Use .get for safety
            max_new_tokens=self.max_new_tokens,
            pad_token_id={pad_token_id_val}
        )
        return outputs

    def postprocess(self, model_output):
        # This is the default/base postprocessor
        return self.tokenizer.batch_decode(model_output, skip_special_tokens=True)


    # The preprocess_chat and postprocess_chat methods
    # are now conditionally added here
    {chat_methods}
"""

    # --- 3. LangChain AI Configuraion ---

    # The Pydantic model now includes "is_chat_model" and makes chat logic optional
    class ModelConfig(BaseModel):
        """Configuration details for a Hugging Face text-to-text model."""

        is_chat_model: bool = Field(
            description="Set to True if this is a chat or instruct model. Set to False if it is a base text-completion model."
        )
        tokenizer_class: str = Field(
            description="The tokenizer class, e.g., 'AutoTokenizer' or 'GPT2Tokenizer'.",
            default="AutoTokenizer",
        )
        model_class: str = Field(
            description="The model class, e.g., 'AutoModelForCausalLM' or 'AutoModelForSeq2SeqLM'.",
            default="AutoModelForCausalLM",
        )
        trust_remote_code: bool = Field(
            description="Whether 'trust_remote_code=True' is required."
        )
        max_new_tokens: int = Field(
            description="The default value for 'max_new_tokens' in generation like 1024 or 128 but for this model.",
            default=1024,
        )
        pad_token_str: str = Field(
            description="The *exact* line of Python code to set the pad_token, e.g., 'self.tokenizer.pad_token = self.tokenizer.eos_token' or 'pass'.",
            default="self.tokenizer.pad_token = self.tokenizer.eos_token",
        )
        pad_token_id_val: str = Field(
            description="The *exact* value for pad_token_id in model.generate(), e.g., 'self.tokenizer.eos_token_id' or 'self.tokenizer.pad_token_id'.",
            default="self.tokenizer.eos_token_id",
        )
        preprocess_chat_logic: Optional[str] = Field(
            default=None,
            description="The *exact* line(s) of Python code for the 'preprocess_chat' method's chat template logic. MUST be null if is_chat_model is False.",
        )
        postprocess_chat_logic: Optional[str] = Field(
            default=None,
            description="The *exact* line(s) of Python code for the 'postprocess_chat' method to slice the output. MUST be null if is_chat_model is False. Example: 'decoded = self.tokenizer.decode(model_output[0][self.input_ids_shape[-1] :], skip_special_tokens=True)'",
        )

    # The system prompt is now much more explicit about the base vs. chat distinction.
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
You are an expert in Hugging Face transformers. Your task is to generate the configuration for a Python wrapper class.
You will be given an identifier and the text content from its Hugging Face model card.
You must generate the JSON config based *primarily* on the provided page context.

**CRITICAL INSTRUCTION:**
First, determine if the model is a 'base' model (for text completion) or a 'chat/instruct' model (for conversation).
1.  Set the `is_chat_model` boolean field accordingly.
2.  If `is_chat_model` is `True`, you **MUST** provide the `preprocess_chat_logic` and `postprocess_chat_logic`.
3.  If `is_chat_model` is `False`, you **MUST** set `preprocess_chat_logic` and `postprocess_chat_logic` to `null`.

**MULTI-GPU SUPPORT:**
All generated models automatically support multi-GPU via the `load_hf_model` method from the parent class:
- The parent class `PyTorchAbstractClass` automatically handles `_device` and `_multi_gpu` from config
- Simply use `self.load_hf_model(ModelClass, model_id)` - it handles multi-GPU automatically
- When `_multi_gpu=True` and `_device="cuda"`, the model loads with `device_map="auto"` and `torch_dtype="auto"`
- **DO NOT** manually extract device/multi_gpu or add device_map/torch_dtype - use `self.load_hf_model()` instead

Pay close attention to these details:
- **Trust Remote Code**: Is `trust_remote_code=True` needed? (e.g., Phi-3 needs it).
- **Model Class**: Is it `AutoModelForCausalLM` or a specific one like `GPT2LMHeadModel`?
- **Chat Logic**: If it's a chat model, what is the *exact* `apply_chat_template` logic? Does it need `add_generation_prompt=True`? What is the *exact* slicing logic for postprocessing?

Reference Examples:
---
Example 1: 'microsoft/Phi-3-mini-4k-instruct'
{{{{
    "is_chat_model": true,
    "tokenizer_class": "AutoTokenizer",
    "model_class": "AutoModelForCausalLM",
    "trust_remote_code": true,
    "max_new_tokens": 1024,
    "pad_token_str": "self.tokenizer.pad_token = self.tokenizer.eos_token",
    "pad_token_id_val": "self.tokenizer.eos_token_id",
    "preprocess_chat_logic": "encoded = self.tokenizer.apply_chat_template(formatted_messages, return_dict=True, return_tensors=\\"pt\\", add_generation_prompt=True)",
    "postprocess_chat_logic": "decoded = self.tokenizer.decode(model_output[0][self.input_ids_shape[-1] :], skip_special_tokens=True)\\nreturn [decoded]"
}}}}
---
Example 2: 'gpt2'
{{{{
    "is_chat_model": false,
    "tokenizer_class": "GPT2Tokenizer",
    "model_class": "GPT2LMHeadModel",
    "trust_remote_code": false,
    "max_new_tokens": 128,
    "pad_token_str": "self.tokenizer.pad_token = self.tokenizer.eos_token",
    "pad_token_id_val": "self.tokenizer.eos_token_id",
    "preprocess_chat_logic": null,
    "postprocess_chat_logic": null
}}}}
---
Example 3: 'Qwen/Qwen2-1.5B-Instruct'
{{{{
    "is_chat_model": true,
    "tokenizer_class": "AutoTokenizer",
    "model_class": "AutoModelForCausalLM",
    "trust_remote_code": false,
    "max_new_tokens": 1024,
    "pad_token_str": "self.tokenizer.pad_token = self.tokenizer.eos_token",
    "pad_token_id_val": "self.tokenizer.eos_token_id",
    "preprocess_chat_logic": "encoded = self.tokenizer.apply_chat_template(formatted_messages, return_dict=True, return_tensors=\\"pt\\", add_generation_prompt=True)",
    "postprocess_chat_logic": "decoded = self.tokenizer.decode(model_output[0][self.input_ids_shape[-1]:], skip_special_tokens=True)\\nreturn [decoded]"
}}}}
---
Respond ONLY with the JSON structure.
""",
            ),
            (
                "human",
                """
Here is the context from the model's Hugging Face page:
---
{model_page_context}
---
Based on the examples AND the context above, generate the config for the model: '{model_identifier}'
""",
            ),
        ]
    )
    modelCounter = 0

    # --- LLM Setup ---
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY not found in .env file or environment.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        model_kwargs={"thinkingBudget": -1},
        temperature=0,
        convert_system_message_to_human=True,
    )
    parser = JsonOutputParser(pydantic_object=ModelConfig)
    chain = prompt | llm | parser
    print("--- NOTE: Using a REAL LLM (Google Gemini). This will make API calls. ---")

    # --- 4. Main Generation Loop ---
    BASE_DIR = "mlmodelscope/pytorch_agent/models/default/text_to_text"
    ERROR_DIR = (
        "mlmodelscope/pytorch_agent/models/default/text_to_text/automation/"
        + str(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    )
    if not os.path.exists(ERROR_DIR):
        os.makedirs(ERROR_DIR)
        print(f"Created error directory: '{ERROR_DIR}'")
    failed_models = []
    login_req_models = []
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)
        print(f"Created base directory: '{BASE_DIR}'")
        # raise FileNotFoundError(f"Base directory '{BASE_DIR}' does not exist.")

    for model_name in models_to_add:
        if modelCounter == 50:
            print("Added top 50 models. Achieved the goal for now.")
            break
        print(f"\n--- Processing model: {model_name} ---")

        model_folder_name = (
            model_name.split("/")[-1].replace("-", "_").replace(".", "_").lower()
        )
        model_py_path = os.path.join(BASE_DIR, model_folder_name, "model.py")

        if os.path.exists(model_py_path):
            print(f"Model '{model_folder_name}' already exists. Skipping.")
            continue
        error_log = ""

        try:
            check_syntax = lambda fileName: os.system(
                f"python -m py_compile {fileName}"
            )
            error = False
            MAX_TRIES_PER_MODEL = 5
            try_count_my_model = 0
            while not os.path.exists(model_py_path) or (
                error := check_syntax(model_py_path)
            ):
                if try_count_my_model >= MAX_TRIES_PER_MODEL:
                    break
                try_count_my_model += 1
                if error:
                    print(
                        f"Syntax error detected in generated file '{model_py_path}'. Regenerating..."
                    )
                    error_log += (
                        f"Syntax error detected in generated file '{model_py_path}'. Regenerating...\n"
                        + error
                    )

                # --- 1. Fetch Context from Hugging Face ---
                url = f"https://huggingface.co/{model_name}"
                print(f"Fetching context from {url}...")
                response = requests.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                main_content = (
                    soup.find("model-card-content")
                    or soup.find("main")
                    or soup.find("body")
                )

                if not main_content:
                    print("Could not find main content on page, skipping.")
                    break

                login_link = main_content.find(
                    "a", href=lambda h: h and h.startswith("/login?next=")
                )
                if login_link:
                    print("Login required, skipping page.")
                    login_req_models.append(model_name)
                    break
                context_text = main_content.get_text(separator=" ", strip=True)
                max_length = 15000
                if len(context_text) > max_length:
                    context_text = (
                        context_text[:max_length] + "\n... (content truncated)"
                    )
                print("Context fetched successfully.")

                # --- 2. Invoke LLM with Context ---
                print("Invoking LLM to generate config...")
                model_config_dict = chain.invoke(
                    {
                        "model_identifier": model_name,
                        "model_page_context": context_text,
                    }
                )
                print("--- LLM Output (Parsed) ---")
                print(model_config_dict)
                print("----------------------------")

                # --- 3. Format trust_remote_code ---
                if model_config_dict.get("trust_remote_code"):
                    trust_comment = (
                        "# trust_remote_code=True is required for this model"
                    )
                    tokenizer_trust = ", 'trust_remote_code': True"
                    model_trust = "'trust_remote_code': True"
                else:
                    trust_comment = "# trust_remote_code=False"
                    tokenizer_trust = ""
                    model_trust = ""

                # --- 4. Conditionally build chat-related code ---
                print(f"Model is_chat_model: {model_config_dict.get('is_chat_model')}")
                if model_config_dict.get("is_chat_model"):

                    # It's a chat model. Fill in the chat methods.
                    preprocess_chat_str = (
                        model_config_dict.get("preprocess_chat_logic")
                        or "pass # AI did not provide logic"
                    )
                    postprocess_chat_str = (
                        model_config_dict.get("postprocess_chat_logic")
                        or "pass # AI did not provide logic"
                    )

                    chat_methods_fill = CHAT_METHODS_TEMPLATE.format(
                        preprocess_chat_logic=preprocess_chat_str.replace(
                            "\n", "\n" + "    " * 2
                        ),
                        postprocess_chat_logic=postprocess_chat_str.replace(
                            "\n", "\n" + "    " * 2
                        ),
                    )

                    runtime_switch_fill = RUNTIME_SWITCH_TEMPLATE
                    print("Identified as CHAT model. Adding chat methods.")

                else:
                    # It's a base model. Leave chat methods and switch empty.
                    chat_methods_fill = (
                        "# This is a base model. No chat methods are defined."
                    )
                    runtime_switch_fill = (
                        "# This is a base model. No runtime switch is needed."
                    )
                    print("Identified as BASE model. Omitting chat methods.")

                # --- 5. Prepare model_trust_remote_kwargs for load_hf_model ---
                if model_config_dict.get("trust_remote_code"):
                    model_trust_remote_kwargs = ", trust_remote_code=True"
                else:
                    model_trust_remote_kwargs = ""

                # --- 6. Fill Main Template ---
                filled_template = MODEL_TEMPLATE.format(
                    tokenizer_class=model_config_dict.get(
                        "tokenizer_class", "AutoTokenizer"
                    ),
                    model_class=model_config_dict.get(
                        "model_class", "AutoModelForCausalLM"
                    ),
                    class_name="PyTorch_Transformers_"
                    + "_".join(
                        part.capitalize() for part in model_folder_name.split("_")
                    ),
                    hugging_face_model_id=model_name,
                    trust_remote_code_comment=trust_comment,
                    tokenizer_trust_remote=tokenizer_trust,
                    model_trust_remote_kwargs=model_trust_remote_kwargs,
                    max_new_tokens=model_config_dict.get("max_new_tokens", 32),
                    pad_token_str=model_config_dict.get("pad_token_str", "pass"),
                    pad_token_id_val=model_config_dict.get(
                        "pad_token_id_val", "self.tokenizer.eos_token_id"
                    ),
                    runtime_switch=runtime_switch_fill,
                    chat_methods=chat_methods_fill,
                )

                # --- 6. Create directory and write file ---
                model_dir = os.path.join(BASE_DIR, model_folder_name)
                os.makedirs(model_dir, exist_ok=True)

                with open(model_py_path, "w") as f:
                    f.write(filled_template)
            else:
                print(f"Successfully generated {model_py_path}")
                print(f"--- Generated content for {model_folder_name}/model.py: ---")
                modelCounter += 1

        except Exception as e:
            print(f"Failed to generate code for {model_folder_name}: {e}")
            import traceback

            error_log += f"Exception: {e}\n"
            traceback.print_exc()  # Print full error stack
            with open(
                os.path.join(ERROR_DIR, f"{model_folder_name}_error.log"), "w"
            ) as error_file:
                error_file.write(error_log)
            failed_models.append(model_name)
            continue

    print(f"\n--- Automation complete. ---")
    print(f"Check the '{BASE_DIR}' folder for new model files.")
    with open(os.path.join(ERROR_DIR, "failed_models.log"), "w") as f:
        for failed_model in failed_models:
            f.write(f"{failed_model}\n")
    with open(os.path.join(ERROR_DIR, "login_req_models.log"), "w") as f:
        for failed_model in login_req_models:
            f.write(f"{failed_model}\n")
    if failed_models:
        print(f"Some models failed. See '{ERROR_DIR}/failed_models.log' for details.")
    if login_req_models:
        print(
            f"Some models needed login. See '{ERROR_DIR}/login_req_models.log' for details."
        )

    else:
        print("All models processed successfully.")


if __name__ == "__main__":
    text_to_text_model_automation(
        models_to_add=[
            "Qwen/Qwen3-0.6B",
            # "openai-community/gpt2",
            # "microsoft/Phi-3-mini-4k-instruct",
        ]
    )
