
# Generated by automation script
# Using the absolute import path to avoid ModuleNotFoundError
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import GPT2Tokenizer, GPT2LMHeadModel


class PyTorch_Transformers_Gpt2_Large(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else {}

        model_id = "openai-community/gpt2-large"

        # --- Generated Configuration ---
        # trust_remote_code=False
        tokenizer_args = {'padding_side': 'left'}
        model_args = {}

        self.tokenizer = GPT2Tokenizer.from_pretrained(model_id, **tokenizer_args)
        self.model = GPT2LMHeadModel.from_pretrained(model_id, **model_args)

        self.tokenizer.pad_token = self.tokenizer.eos_token
        # --- End Generated Configuration ---

        self.max_new_tokens = self.config.get("max_new_tokens", 128)

    
        # This block is now conditionally added
        # This is a base model. No runtime switch is needed.

    def preprocess(self, input_texts):
        # This is the default/base preprocessor
        # The 'predict' method expects a dictionary or object with 'input_ids'
        return self.tokenizer(input_texts, return_tensors="pt", padding=True)

    def predict(self, model_input):
        outputs = self.model.generate(
            model_input["input_ids"],
            attention_mask=model_input.get("attention_mask"), # Use .get for safety
            max_new_tokens=self.max_new_tokens,
            pad_token_id=self.tokenizer.eos_token_id
        )
        return outputs

    def postprocess(self, model_output):
        # This is the default/base postprocessor
        return self.tokenizer.batch_decode(model_output, skip_special_tokens=True)


    # The preprocess_chat and postprocess_chat methods
    # are now conditionally added here
    # This is a base model. No chat methods are defined.
