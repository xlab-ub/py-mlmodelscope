
# Generated by automation script
# Using the absolute import path to avoid ModuleNotFoundError
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoTokenizer, AutoModelForCausalLM


class PyTorch_Transformers_Meta_Llama_Llama_3_2_3b_Instruct_Fp16(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else {}

        model_id = "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16"

        # --- Generated Configuration ---
        # trust_remote_code=False
        tokenizer_args = {'padding_side': 'left'}
        model_args = {}

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_args)
        self.model = AutoModelForCausalLM.from_pretrained(model_id, **model_args)

        self.tokenizer.pad_token = self.tokenizer.eos_token
        # --- End Generated Configuration ---

        self.max_new_tokens = self.config.get("max_new_tokens", 1024)

    
        # This block is now conditionally added
        
        if "messages" in self.config:
            self.messages = self.config["messages"]
            self.preprocess = self.preprocess_chat
            self.postprocess = self.postprocess_chat


    def preprocess(self, input_texts):
        # This is the default/base preprocessor
        # The 'predict' method expects a dictionary or object with 'input_ids'
        return self.tokenizer(input_texts, return_tensors="pt", padding=True)

    def predict(self, model_input):
        outputs = self.model.generate(
            model_input["input_ids"],
            attention_mask=model_input.get("attention_mask"), # Use .get for safety
            max_new_tokens=self.max_new_tokens,
            pad_token_id=self.tokenizer.eos_token_id
        )
        return outputs

    def postprocess(self, model_output):
        # This is the default/base postprocessor
        return self.tokenizer.batch_decode(model_output, skip_special_tokens=True)


    # The preprocess_chat and postprocess_chat methods
    # are now conditionally added here
    
    def preprocess_chat(self, input_texts):
        formatted_messages = []
        for message in self.messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            formatted_messages.append({"role": role, "content": content})
        formatted_messages.append({"role": "user", "content": input_texts[0]})

        # --- Generated Preprocess Chat ---
        encoded = self.tokenizer.apply_chat_template(formatted_messages, return_dict=True, return_tensors="pt", add_generation_prompt=True)
        # --- End Generated Preprocess Chat ---

        # The chat template logic above must set a variable named 'encoded'
        self.input_ids_shape = encoded["input_ids"].shape
        return encoded

    def postprocess_chat(self, model_output):
        # --- Generated Postprocess Chat ---
        decoded = self.tokenizer.decode(model_output[0][self.input_ids_shape[-1] :], skip_special_tokens=True)
        return [decoded]
        # --- End Generated Postprocess Chat ---

