
# Generated by automation script
# Using the absolute import path to avoid ModuleNotFoundError
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import AutoTokenizer, AutoModel


class PyTorch_Transformers_Paraphrase_Mpnet_Base_V2(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else {}
        
        # Extract device and multi_gpu settings for multi-GPU support
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "sentence-transformers/paraphrase-mpnet-base-v2"

        # --- Generated Configuration ---
        # trust_remote_code=False
        tokenizer_args = {'padding_side': 'left'}
        model_args = {}
        
        # Add multi-GPU support to model_args if enabled
        if multi_gpu and device == "cuda":
            model_args['device_map'] = "auto"
            model_args['torch_dtype'] = "auto"

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_args)
        self.model = AutoModel.from_pretrained(model_id, **model_args)

        self.tokenizer.pad_token = self.tokenizer.eos_token
        # --- End Generated Configuration ---

        self.max_new_tokens = self.config.get("max_new_tokens", 128)

    
        # This block is now conditionally added
        # This is a base model. No runtime switch is needed.

    def preprocess(self, input_texts):
        # This is the default/base preprocessor
        # The 'predict' method expects a dictionary or object with 'input_ids'
        return self.tokenizer(input_texts, return_tensors="pt", padding=True)

    def predict(self, model_input):
        outputs = self.model.generate(
            model_input["input_ids"],
            attention_mask=model_input.get("attention_mask"), # Use .get for safety
            max_new_tokens=self.max_new_tokens,
            pad_token_id=self.tokenizer.eos_token_id
        )
        return outputs

    def postprocess(self, model_output):
        # This is the default/base postprocessor
        return self.tokenizer.batch_decode(model_output, skip_special_tokens=True)


    # The preprocess_chat and postprocess_chat methods
    # are now conditionally added here
    # This is a base model. No chat methods are defined.
