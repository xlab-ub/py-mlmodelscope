
# Generated by automation script
# Using the absolute import path to avoid ModuleNotFoundError
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from transformers import T5Tokenizer, T5ForConditionalGeneration


class PyTorch_Transformers_T5_Small_Booksum(PyTorchAbstractClass):
    def __init__(self, config=None):
        super().__init__(config)

        model_id = "cnicu/t5-small-booksum"

        # --- Generated Configuration ---
        # trust_remote_code=False
        tokenizer_args = {'padding_side': 'left'}
        
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(model_id, **tokenizer_args)
            self.model = self.load_hf_model(T5ForConditionalGeneration, model_id)
        except Exception as e:
            if model_id in e.__str__():
                self.huggingface_authenticate()
                self.tokenizer = T5Tokenizer.from_pretrained(model_id, **tokenizer_args)
                self.model = self.load_hf_model(T5ForConditionalGeneration, model_id)
            else:
                raise e

        None
        # --- End Generated Configuration ---

        self.max_new_tokens = self.config.get("max_new_tokens", 256)

    
        # This block is now conditionally added
        # This is a base model. No runtime switch is needed.

    def preprocess(self, input_texts):
        # This is the default/base preprocessor
        # The 'predict' method expects a dictionary or object with 'input_ids'
        return self.tokenizer(input_texts, return_tensors="pt", padding=True)

    def predict(self, model_input):
        outputs = self.model.generate(
            model_input["input_ids"],
            attention_mask=model_input.get("attention_mask"), # Use .get for safety
            max_new_tokens=self.max_new_tokens,
            pad_token_id=None
        )
        return outputs

    def postprocess(self, model_output):
        # This is the default/base postprocessor
        return self.tokenizer.batch_decode(model_output, skip_special_tokens=True)


    # The preprocess_chat and postprocess_chat methods
    # are now conditionally added here
    # This is a base model. No chat methods are defined.
