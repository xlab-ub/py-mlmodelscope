# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoModel, AutoTokenizer

class PyTorch_Transformers_InternLM_XComposer2_VL_7B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "internlm/internlm-xcomposer2-vl-7b"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                device_map="auto"
            )
        else:
            self.model = AutoModel.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                trust_remote_code=True
            ).to(device)

        self.model = self.model.eval()

    def preprocess(self, input_image_and_questions):
        queries = [f'<ImageHere>{item[1]}' for item in input_image_and_questions]
        images = [item[0] for item in input_image_and_questions]
        return list(zip(images, queries))

    def predict(self, model_input):
        responses = []
        torch.set_grad_enabled(False)
        for image_path, query in model_input:
            device = next(self.model.parameters()).device
            if device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    response, _ = self.model.chat(self.tokenizer, query=query, image=image_path, history=[], do_sample=False)
            else:
                response, _ = self.model.chat(self.tokenizer, query=query, image=image_path, history=[], do_sample=False)
            responses.append(response)
        return responses

    def postprocess(self, model_output):
        return model_output
