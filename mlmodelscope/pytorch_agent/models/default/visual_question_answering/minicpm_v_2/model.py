# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

class PyTorch_Transformers_MiniCPM_V_2(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = 'openbmb/MiniCPM-V-2'
        # Use float16 as a safe default; bfloat16 is for newer GPUs like A100/H100
        torch_dtype = torch.float16

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        
        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map="auto"
            )
        else:
            self.model = AutoModel.from_pretrained(
                model_id,
                trust_remote_code=True,
                torch_dtype=torch_dtype
            ).to(device)

        self.model.eval()

        self.generation_params = {
            'sampling': self.config.get('sampling', True),
            'temperature': self.config.get('temperature', 0.7),
            'max_new_tokens': self.config.get('max_new_tokens', 100)
        }

    def preprocess(self, input_image_and_questions):
        processed_inputs = []
        for image_path, question in input_image_and_questions:
            image = Image.open(image_path).convert('RGB')
            msgs = [{'role': 'user', 'content': question}]
            processed_inputs.append({'image': image, 'msgs': msgs})
        return processed_inputs

    def predict(self, model_input):
        results = []
        for item in model_input:
            # The model's chat method handles moving data to the correct device internally
            res, _, _ = self.model.chat(
                image=item['image'],
                msgs=item['msgs'],
                context=None, # Start a new conversation for each question
                tokenizer=self.tokenizer,
                **self.generation_params
            )
            results.append(res)
        return results

    def postprocess(self, model_output):
        # The predict method already returns a list of decoded strings
        return model_output
