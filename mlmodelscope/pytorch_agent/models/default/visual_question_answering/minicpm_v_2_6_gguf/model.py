# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
from transformers import AutoModel, AutoTokenizer
import torch
from PIL import Image

class PyTorch_Transformers_OpenBMB_MiniCPM_V_2_6(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        # The provided model 'gaianet/MiniCPM-V-2_6-GGUF' is in GGUF format, which is not directly compatible with PyTorch transformers.
        # We are using the original PyTorch base model 'openbmb/MiniCPM-V-2_6' instead.
        model_id = "openbmb/MiniCPM-V-2_6"
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        
        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto")
        else:
            self.model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16)

        self.generation_kwargs = {
            'max_new_tokens': self.config.get('max_new_tokens', 1024),
            'temperature': self.config.get('temperature', 0.7),
            'sampling': self.config.get('sampling', True)
        }

    def preprocess(self, input_image_and_questions):
        processed_inputs = []
        for image_path, question in input_image_and_questions:
            image = Image.open(image_path).convert('RGB')
            msgs = [{'role': 'user', 'content': question}]
            processed_inputs.append({'image': image, 'msgs': msgs})
        return processed_inputs

    def predict(self, model_input):
        results = []
        for item in model_input:
            res = self.model.chat(
                image=item['image'],
                msgs=item['msgs'],
                tokenizer=self.tokenizer,
                **self.generation_kwargs
            )
            results.append(res)
        return results

    def postprocess(self, model_output):
        return model_output
