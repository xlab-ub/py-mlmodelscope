# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
import warnings

class PyTorch_Transformers_MiniCPM_V_4_GGUF(PyTorchAbstractClass):
    def __init__(self, config=None):
        warnings.warn("This model does not support batching. Inputs will be processed one by one.")
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        # The requested model is a GGUF-quantized version. We use the original PyTorch model it is based on.
        # 'openbmb/MiniCPM-V-4' does not exist, so we use the closest available version, 'openbmb/MiniCPM-V-2.0'.
        model_id = "openbmb/MiniCPM-V-2.0"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        
        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto")
        else:
            self.model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16)

        self.model.eval()
        self.max_new_tokens = self.config.get('max_new_tokens', 100)

    def preprocess(self, input_image_and_questions):
        images = [Image.open(item[0]).convert('RGB') for item in input_image_and_questions]
        msgs_list = [[{'role': 'user', 'content': item[1]}] for item in input_image_and_questions]
        return list(zip(images, msgs_list))

    def predict(self, model_input):
        # This model's .chat() method does not support batching, so we process inputs sequentially in a loop.
        results = []
        for image, msgs in model_input:
            res = self.model.chat(
                image=image,
                msgs=msgs,
                tokenizer=self.tokenizer,
                generation_config={'max_new_tokens': self.max_new_tokens},
                sampling=True,
                temperature=0.7
            )
            results.append(res)
        return results

    def postprocess(self, model_output):
        # The predict method already returns a list of decoded strings.
        return model_output
