# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoTokenizer, AutoModel
from PIL import Image
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

class PyTorch_Transformers_ByteDance_Sa2VA_4B(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        model_id = "ByteDance/Sa2VA-4B"

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True,
            use_fast=False
        )

        model_kwargs = {
            "torch_dtype": torch.bfloat16,
            "low_cpu_mem_usage": True,
            "trust_remote_code": True,
            "use_flash_attn": True
        }

        if multi_gpu and device == "cuda":
            self.model = AutoModel.from_pretrained(model_id, device_map="auto", **model_kwargs)
        else:
            self.model = AutoModel.from_pretrained(model_id, **model_kwargs).to(device)
        
        self.model.eval()

    def preprocess(self, input_image_and_questions):
        processed_inputs = []
        for image_path, question in input_image_and_questions:
            image = Image.open(image_path).convert('RGB')
            prompt = f"<image>{question}"
            
            input_dict = {
                'image': image,
                'text': prompt,
                'past_text': '',
                'mask_prompts': None,
                'tokenizer': self.tokenizer,
            }
            processed_inputs.append(input_dict)
        return processed_inputs

    def predict(self, model_input):
        predictions = []
        with torch.no_grad():
            for input_dict in model_input:
                return_dict = self.model.predict_forward(**input_dict)
                predictions.append(return_dict["prediction"])
        return predictions

    def postprocess(self, model_output):
        return model_output
