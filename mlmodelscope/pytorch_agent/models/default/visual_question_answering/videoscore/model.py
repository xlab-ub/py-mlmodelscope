# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoProcessor, AutoModelForSequenceClassification
from PIL import Image

class PyTorch_Transformers_TIGER_Lab_VideoScore(PyTorchAbstractClass):
    def __init__(self, config=None):
        self.config = config if config else dict()
        device = self.config.pop("_device", "cpu")
        multi_gpu = self.config.pop("_multi_gpu", False)

        self.REGRESSION_QUERY_PROMPT = """Suppose you are an expert in judging and evaluating the quality of AI-generated videos, please watch the following frames of a given video and see the text prompt for generating the video, then give scores from 5 different dimensions: (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements (3) dynamic degree, the degree of dynamic changes (4) text-to-video alignment, the alignment between the text prompt and the video content (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge for each dimension, output a float number from 1.0 to 4.0, the higher the number is, the better the video performs in that sub-score, the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video) Here is an output example: visual quality: 3.2 temporal consistency: 2.7 dynamic degree: 4.0 text-to-video alignment: 2.3 factual consistency: 1.8 For this video, the text prompt is \"{text_prompt}\", all the frames of video are as follows:"""

        model_id = "TIGER-Lab/VideoScore"
        model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16

        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)

        if multi_gpu and device == "cuda":
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=model_dtype,
                trust_remote_code=True
            )
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_id,
                torch_dtype=model_dtype,
                trust_remote_code=True
            )

    def preprocess(self, input_image_and_questions):
        # This model processes a set of frames for a single video prompt at a time.
        # We assume the entire input_image_and_questions list corresponds to one video.
        if not input_image_and_questions:
            return {}

        images = [Image.open(item[0]).convert('RGB') for item in input_image_and_questions]
        # All questions in the list are assumed to be the same for one video
        question = input_image_and_questions[0][1]

        # Format the prompt and add one <image> token per frame.
        prompt = self.REGRESSION_QUERY_PROMPT.format(text_prompt=question)
        prompt += " " + "<image>" * len(images)

        # The processor expects a batch, so we wrap our single prompt and image list in another list.
        return self.processor(text=[prompt], images=[images], return_tensors="pt")

    def predict(self, model_input):
        # This is a classification/regression model, not a generator, so we do a forward pass.
        return self.model(**model_input)

    def postprocess(self, model_output):
        logits = model_output.logits
        results = []
        # The model outputs 5 scores for 5 aspects for each item in the batch.
        # Our preprocess method creates a batch of 1.
        for logit_tensor in logits:
            scores = [round(score.item(), 3) for score in logit_tensor]
            results.append(str(scores))
        return results
