# Generated by automation script
from mlmodelscope.pytorch_agent.models.pytorch_abc import PyTorchAbstractClass

import torch
from transformers import AutoProcessor, Qwen2ForConditionalGeneration
from PIL import Image

class PyTorch_Transformers_TIGER_Lab_VL_Rethinker_72B(PyTorchAbstractClass):
    def __init__(self, config=None):
        super().__init__(config)
        model_id = "TIGER-Lab/VL-Rethinker-72B"
        self.prompt_suffix = '"""Guidelines: Please think step by step, and **regularly perform self-questioning, self-verification, self-correction to check your ongoing reasoning**, using connectives such as "Wait a moment", "Wait, does it seem right?", etc. Remember to put your final answer within \\boxed{}. """'

        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        self.model = self.load_hf_model(
            Qwen2ForConditionalGeneration, 
            model_id, 
            torch_dtype="auto", 
            device_map="auto", 
            trust_remote_code=True
        )

        self.max_new_tokens = self.config.get('max_new_tokens', 1024)
        self.input_token_len = 0

    def preprocess(self, input_image_and_questions):
        images = [Image.open(item[0]).convert('RGB') for item in input_image_and_questions]
        questions = [item[1] for item in input_image_and_questions]

        messages_list = []
        for question in questions:
            content = [
                {"type": "image"},
                {"type": "text", "text": f"{question}\n{self.prompt_suffix}"}
            ]
            messages = [{"role": "user", "content": content}]
            messages_list.append(messages)

        prompts = [self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) for messages in messages_list]
        model_input = self.processor(text=prompts, images=images, return_tensors="pt")

        self.input_token_len = model_input["input_ids"].shape[1]
        return model_input

    def predict(self, model_input):
        return self.model.generate(**model_input, max_new_tokens=self.max_new_tokens, eos_token_id=self.processor.tokenizer.eos_token_id)

    def postprocess(self, model_output):
        generated_ids = model_output[:, self.input_token_len:]
        return self.processor.batch_decode(generated_ids, skip_special_tokens=True)
